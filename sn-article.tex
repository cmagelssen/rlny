%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Article Title}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

\author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

\affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

\affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. Authors are advised to check the author instructions for the journal they are submitting to for word limits and if structural elements like subheadings, citations, or equations are permitted.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{keyword1, Keyword2, Keyword3, Keyword4}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

Developing expertise demands substantial amounts of high-quality training \cite{ericsson_role_1993, hodges_predicting_2004, vaeyens_talent_2009, sosniak_learning_1985}. Unlike training novices to reach acceptable levels on a task, the main challenge for skilled performers is finding ways to improve beyond current levels of performance\cite{ericsson_development_2003, ericsson_scientific_1998, gray_plateaus_2017, williams_expertise_2008, du_relationship_2022}. One route to achieve this goal is to perfect one's current strategy\cite{krakauer_motor_2019, du_relationship_2022}. This is a slow improvement process that operates through honing the components underlying the automated solution\cite{du_relationship_2022}. A second route is to find new and superior ways to solve the task, which can sometimes lead to greater improvement leaps than simply repeating what is already automatized \cite{gray_plateaus_2017, du_relationship_2022, krakauer_motor_2019}. Finding such creative and innovative solutions is a hallmark of expertise development\cite{ericsson_scientific_1998, ericsson_development_2003}; however, we know little about which training strategies are most effective at developing these strategies, let alone how these strategies are developed\cite{taylor_cerebellar_2014, taylor_role_2012}. If superior methods to those currently used in standard practice exist, they could prove invaluable for training both present and future generations of learners.

In current practice, these strategies are typically trained with instruction-based approaches, where a coach tells learners what to do (e.g., take a shorter line around the gate) followed by corrective feedback (e.g., you can shorten the line even more) \cite{williams_practice_2005, williams_effective_2023, hodges_role_1999}. This teaching strategy can be likened to what motor learning refers to as supervised learning, where the teaching signal for skill improvement represents the disparity between the desired skill outcome and the learner outcome \cite{jordan_forward_1992, wolpert_motor_2010, doya_complementary_2000}. Through practice, this teaching signal can bring the learner closer to executing what is assumed to be the correct choice. However, is this teaching approach the best way to go if the goal is to help learners discover innovative and vastly more effective solutions that surpass our current imagination?

One drawback of the supervised learning strategy for training these decisions is that learners are simply told what to do based on what coaches believe to be a good strategy from their knowledge and experience. However, what coaches judge as a good strategy does not always align with reality, even for the best-trained eye \cite{supej_impact_2019, cochrum_visual_2021}. Learners might, therefore, miss opportunities to discover the best strategy when coaches opt for suboptimal strategies \cite{gray_plateaus_2017}. Supervised learning might also constrain learners to adopting a single ('universal') strategy for all situations rather than acquiring a repertoire of strategies and discerning the most effective strategies for each specific scenario. Finally, it remains uncertain whether the prescriptive approach is the most effective teaching strategy for achieving long-lasting learning effects \cite{wulf_instructions_1997, hodges_role_1999, williams_practice_2005,williams_effective_2023} 

Learning to choose good strategies can also happen without the direct influence of a coach providing advice. The cornerstone of reinforcement learning \cite{sutton_reinforcement_2018} is that learners can learn by exploring strategies and evaluating their outcomes, using the successes and failures of outcomes as teaching signals. That is, rather than being told the putatively correct solution to the problem, as in supervised learning, they learn the value of different strategies, which allows them to finally pick the best solution. Specifically, these values are learned by comparing a given choice's outcomes with the currently expected outcome of that choice. Outcomes that exceed or fall short of expectations result in errors in reward prediction, signaling that the learner must update their predictions to better anticipate future rewards following that action \cite{rescorla_theory_1972}. These reward prediction errors are then incorporated to form a new and better estimate of reward, by updating expectations through a weighted running average. Reinforcement learning has been tremendously powerful in explaining human and animal learning \cite{waelti_dopamine_2001, schultz_neural_1997, pessiglione_dopamine-dependent_2006}, improving skill learning in laboratory-based tasks \cite{lior_shmuelof_overcoming_2012, abe_reward_2011, truong_error-based_2023, hasson_reinforcement_2015}, as well as training AI to perform complex tasks such as computer games starting from pixel inputs, only\cite{mnih_human-level_2015}. Based on this evidence our question was whether reinforcement learning offer an alternative to standard coach-based supervised learning to improve strategy selection and performance for skilled performers.

To address this question, we conducted a three-day learning experiment with ninety-eight skilled and elite alpine ski racers from Norway and Sweden. To facilitate skill development in this group with already skilled athletes, we chose a type of terrain where athletes typically have potential to improve their performance. Specifically, we defined four motion strategies suitable to enhance performance in flat sections in slalom  (\ref{fig:courseandstrategies}b). Our hypothesis was that skiers in the reinforcement learning group would learn to choose better strategies and thus achieve better performance than skiers subject to traditional supervised learning with a coach. To test this, we assigned skiers to three different learning groups with different instructions and feedback (Fig. \ref{fig:experiment}b): In the reinforcement learning group, skiers chose a strategy on every run and saw their race times to inform these decisions. In the supervised (free choice) learning group, a ski coach made this choice for the skier, while in the supervised (target skill) learning group, we recruited current World-Cup ski coaches to instruct skiers to select the strategy that we defined as the theoretically best strategy based on computational modeling \cite{lind_physics_2013} and observations of elite skiers \cite{reid_alpine_2020}. Coaches in the two supervised learning groups saw the times but were instructed to not disclose these to the skiers. 

We found that the reinforcement learning group improved more during acquisition and performed better in retention than the supervised (free-choice) learning group. Both groups chose the individual skiers' estimated best strategy more often over the course of the sessions, but we did not find convincing evidence that the reinforcement learning group chose this strategy more often than the supervised (free choice) learning group. That said, we found that reinforcement learning had lower costs (that is, the expected difference between the individual skiers' estimated strategy and their chosen strategy) for suboptimally chosen strategies, suggesting that they had better learned to avoid bad strategies. This was not the sole explanation for their improved learning; however, the skiers improved more on one strategy that they picked often than on one supervised strategy (free choice), suggesting that reinforcement learning also caused motor vigor. We did not find convincing evidence that reinforcement learning learned better than supervised learning (target skill) and that selecting only one strategy all the time comes at is one’s own cost—the skiers did not learn to dissociate the effect of the other strategies.


\begin{figure}[H]
\centering
\includegraphics{figures/figure_method_courseandstrategy.pdf}
\caption{\textbf{a.} Illustrations of the two slalom courses used in the study. The main slalom course was a rhythmic course deployed in all sessions except for the transfer test. The course setting for the transfer test involved a progression in gate offset, starting with the largest offset and ending with the smallest offset. \textbf{b.} Illustration of the strategies defined to enhance racing performance on flat terrain in slalom: The "stand against" strategy emphasized maintaining a stable stance against external forces without body extension along the body's longitudinal axis or rocking skis forward; 'Rock skis forward' involved rocking skis forward from gate passage to completion of the turn; The "extend" strategy involves extending the body from a laterally tilted position during the turn, closer to the turn's center of rotation; The "extend with rocking skis forward" was expected to be the best strategy combining the two effects from extending and rocking skis forward, and we therefore defined this as the theoretical best strategy}
\label{fig:courseandstrategies}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{figures/figure_method_experiment.pdf}
\caption{Illustration of the experimental design and procedure. \textbf{a.} Timeline of the three-day skill learning experiment. During the baseline, skiers skied a slalom course as fast as they could without receiving racing time feedback. The skiers' performances were ranked to form blocks. Within each block, skiers were randomly assigned to one of three treatment groups (see b).Skiers underwent an acquisition phase in their designated treatment group comprising one forced exploration and two free-choice sessions. On the last day, skiers completed a retention and transfer test, again without feedback from coaches nor timing. \textbf{b.} Illustration of treatment groups in the study. Supervised (target skill) involved coaches consistently choosing the theoretically best strategy, while supervised (free choice) allowed coaches to select strategies freely. Skiers in both these treatment groups received feedback on strategy execution from their respective coach, while skiers in the reinforcement learning group independently selected strategies and received feedback from the timing system to facilitate value learning of each strategy. }
\label{fig:experiment}
\end{figure}

\section{Method}


\subsection{Participants}
We recruited ten alpine ski teams from Norway and Sweden comprising 98 alpine ski racers (age M = 18.1 years, SD= 2; 40 females, 58 males). Two skiers were excluded from the analysis due to an injury prior to the study (n=1) or sickness during the study (n=1); thus, a total of 96 skiers completed the entire study and were included in the analysis. We deliberately opted to recruit skiers with diverse skill levels for the study to augment the generalizability of our findings. However, to ensure a sufficient skill level to handle the specific icy snow conditions prepared in the skiing hall, we recruited only skiers aged 15 and older. Among the ski groups tested were five ski academies, three senior development teams, and two national ski teams. These skiers were generally highly skilled, with a median world rank of 605, but there was also considerable variability, as indicated by a substantial interquartile range (Q1 = 248, Q3 = 1390.5). A smaller subset of the participants (n = 13) was not world-ranked, as they had yet to record FIS points. Table 1 provides demographic information for each treatment group.

Conducting studies on alpine ski racing poses challenges related to environmental control and resource constraints. Our sample size approach involved recruiting as many skiers as possible during the summer of 2023, with a set minimum sample size of 80 skiers, which we deemed appropriate for this context. Prior to data collection, data and power simulations for sample sizes of 80, 100, and 120 skiers were conducted, revealing simulated powers of 0.60, 0.75, and 0.80, respectively, for the smallest effect size of interest (0.3 second difference between groups) (\url{https://osf.io/c4t28}). The smallest effect size of interest was based on our knowledge of alpine skiing and discussions with coaches, but it was intended for a 50-meter longer course and more training sessions than we ultimately ended up using due to practical considerations. The sample size justification, task design, and analysis plan were preregistered before data collection (\url{https://osf.io/tfb2w}).

Due to space and time constraints in the ski hall, it was not possible to test all the ski teams simultaneously. The 10 ski teams were therefore divided into 4 groups for which the study was conducted at different times. For each group that participated in the study, we recruited two coaches from the ski teams to serve as coaches in the supervised (free choice) learning group, totaling 8 coaches (2 women; 6 men). These coaches had extensive coaching experience in coaching alpine ski racers. In addition, for each group of ski teams that completed the experiment together, we recruited a third coach to coach the skiers in supervised (target skill) learning. To ensure that these coaches had sufficient credibility to make the skiers buy into our theoretical best strategy, we selectively recruited three highly experienced coaches from the Norwegian alpine ski team, with one coach serving twice due to illness of the fourth coach. Importantly, all the coaches remained unaware of the experimental manipulation. Table 2 provides demographic information of the coaches. All the skiers and coaches provided informed consent before the study. The study was approved by the Human Research Ethics Committee of The Norwegian School of Sport Sciences.


\subsection{The setup}
The experiment was conducted in the indoor ski hall SNØ in Oslo, Norway (\url{https://snooslo.no/}). In this hall, we used a 210-meter-long flat section of the race hill, which we water-injected before testing each group of skiers to ensure uniform and fair snow conditions for all skiers. For additional preparation details and images, please refer to Supplementary A. With our chosen course setup, this 210-meter-long flat section allowed space for 19 slalom gates. We sat two types of slalom courses in the experiment (Figure \ref{fig:design}a). The main slalom course was used in all sessions, except during the transfer test, and featured a 10 m distance and a 1.9 m offset. The course distance aligned with our previous study \cite{magelssen_is_2022}, but we opted for a slightly larger offset to better suit the skill level of our skiers. The transfer test aimed to evaluate how skiers transferred their learning to a slalom course that resembled a typical alpine ski racing course. To assess this, we set a course with a progression in gate offset, starting with five gates set at a 2.2-meter offset, followed by seven gates at a 1.7-meter offset, and concluding with seven gates at a 1.2-meter offset. Although we did not expect radical differences in strategy effects, we anticipated a greater emphasis on rocking the skis forward in gates with a 2.2-meter offset than in those with a 1.2-meter offset to enhance turn exit release. Both courses were set with stubbies (short gates) instead of long gates to minimize energy dissipation upon hitting the gate \cite{minetti_biomechanics_2018} and provide the skiers with space and freedom to execute the strategies. In addition, this approach helped us avoid creating holes in the course, which can occur when the long gate is forcefully slammed into the ground.

The start gate was positioned 20 meters before the first gate. Skiers were required to start in a static position to ensure consistency in the starts, with their binding front head placed behind the starting gate. The skiers started by putting their skis in parallel and lifting up the poles without using poling or skating for propulsion. For an illustration of the starting procedure and setup, see Supplement Video. We recorded the times using a wireless photocell timing system (HC Timing wiNode and wiTimer; Oslo, Norway). Timing started when the skier crossed the first photocell pair situated 10 meters below the starting gate. To minimize wear and tear on the course, we set two parallel and identical courses and routinely shifted between them.
 
\subsection{Experimental design}
We employed a between-subjects design and posed the learning question of discovering effective strategies as an \textit{n}-armed bandit problem \cite{sutton_reinforcement_2018}. The essence of this problem is that a learner repeatedly tries different options and observes their outcomes to learn which strategy is the best and, therefore, which one to choose. Finding the best strategy requires a delicate balance between exploiting the strategy known to yield the best payoff and exploring alternative strategies that may offer superior benefits. In our study, the options consisted of four strategies that skiers could employ to improve their race times on flat slopes in slalom, grounded in physics-based coaching manuals for alpine ski racing \cite{lemaster_skiers_1999, lemaster_ultimate_2010, lind_physics_2013}, biomechanical research on elite skiers \cite{reid_kinematic_2010, tjorhom_beskrivelse_2007, reid_alpine_2020, magelssen_is_2022}  or common strategy used by coaches. The four strategies were named "stand against", "rock skis forward", "extend", and "extend with rock skis forward". (Figure 1\ref{fig:courseandstrategies}b for a strategy illustration and \ref{sup_strategies} for an extended explanation)

To study how instruction and feedback drive learning of strategy selection, we designed and allocated skiers to three treatment groups that allowed us to compared reinforcement learning with traditional supervised learning with a coach:

For the supervised (target skill) learning group, we provided the best possible training program by engaging highly experienced and meritorious coaches who explained to the skiers that the 'extend with rock skis forward' strategy was the most effective for skiing fast on flat terrain in slalom. They imparted this information, citing a previous doctoral project \cite{reid_kinematic_2010}, which revealed that the fastest slalom racers spent more time aft of the skis, emphasizing the involvement of a dynamic rocking movement. In addition, the coaches informed them that simulations of ski racers had yielded similar results \cite{mote_accelerations_1983, lind_physics_2013}. The coach then guided the skiers to adopt this strategy and provided feedback on its execution after each trial. Note that the coach had access to the skiers' times after each trial but were prohibited from sharing this information with the skiers.

In the supervised (free choice) learning group, skiers were assigned to two coaches recruited from the tested ski group. We recruited two coaches to minimize waiting time and thus enhance training efficiency. To balance the skiers' skill levels within the treatment group, we created new blocks from the ranked list from baseline testing and randomly assigned them to the coaches. We instructed the coaches to improve the skiers' racing times as much as possible using all four defined skiing strategies. During each trial, the coach would select a strategy for the skier, observe the skier during his trial and provide feedback on its execution afterward. Similar to the supervised (target skill) learning group, the coaches had access to the skiers' times after each trial but could not share this information with the skiers.

In contrast, the reinforcement learning group was not assigned any coach. Instead of having a coach deciding the skiing strategy for them, the skiers in this treatment group were told to choose a strategy for each trial by themselves to ski the course as fast as possible. To help the skiers choose and learn from their choice and skiing execution, this group could see their racing times immediately after they crossed the finish line. Although this group had no coach, we assigned a person to communicate with the skiers to record their choices and encourage them to try skiing fast, also to equalize out some of the motivational effect of having a coach in the other treatment groups. 

\subsection{Procedures}\label{subsec4}
Figure \ref{fig:experiment}a illustrates the procedures employed in the study. 
In the baseline test, skiers began with two warm-up runs: one in free skiing warm-up course and one specific warm-up in the slalom course. During these warm-up runs, skiers were instructed, trained and verified on the start procedure and start technique by an instructor. As a first run in the baseline assessment, skiers completed a straight-gliding run, where they skied straight down from start to finish in a static, upright slalom posture. Subsequently, the skiers completed four runs in the slalom course. Skiers were encouraged to ski as quickly as they could, but they could not see their times, nor did they get any instruction on how to perform well. 

After the initial baseline assessment, the skiers took a 60-minute break. In the meantime, we allocated the skiers to the three treatment groups: reinforcement learning, supervised learning (target skill), and supervised learning (free choice). To allocate the skiers to groups, we deployed a randomized-blocked approach to account for pre-existing differences in the skiers’ performance levels \cite{maxwell_designing_2017}. Specifically, we computed each skier’s average performance across the four trials in the baseline and ranked them accordingly. We then created \textit{n} blocks with block sizes corresponding to our three treatments for the entire list of skiers and assigned these skiers to these predefined blocks. Finally, we randomly allocated the skiers to the different treatment groups within each block (Figure \ref{fig:experiment}b).

The treatment groups participated in sessions at different times to prevent treatment diffusion \cite{maxwell_designing_2017}. As the ski group comprised teams of skiers who know each other well and were residing together, we explicitly emphasized the importance of keeping information about the sessions private. To stay within the time frame at the ski hall, the two supervised learning groups underwent training together. To facilitate this, we arranged stations in the finish area with space and vision dividers and ample space to impede communication between coaches in the supervised learning groups. In addition, we developed a Python script that fetched racing times from the timing system, filtered the times for each coach, and transmitted it to the station where the coach was located, ensuring no information flow between coaches. The treatment group that initiated after the baseline session was randomized and counterbalanced across the group of ski teams we tested.

The first session after the treatment assignment involved a forced exploration. Here, skiers within the treatment group were gathered, and the session started by introducing them to the strategies. We explained that we had identified four strategies to enhance racing times on flats in slalom. Subsequently, each strategy was detailed, supported by illustrative drawings in \ref{fig:courseandstrategies}b Figure and corresponding word explanations as outlined in the \ref{subsec4}. To confirm comprehension, we conducted two short familiarization trials for each strategy, or until the execution met our performance standards. After reviewing the strategies with the participants, we gathered them in their respective treatment groups and asked them to rank the strategies online for what they believe are most to least suitable to improve performance in flat terrain slalom skiing. Throughout the instruction and ranking process, skiers were explicitly instructed not to discuss the strategies with each other. It is important to note that the same instructor was used for all treatment groups within a tested ski group. After this, the skiers conducted a total of eight trials on the course, with two trials for each strategy. During these trials, the reinforcement learning group got feedback from the timing whereas the coach gave feedback in the supervised learning. After completing the eight rounds, the skiers re-evaluated and ranked the strategies.

On the second day, skiers completed two free-choice sessions, each comprising a total of 6 trials in the same slalom courses that were used the baseline testing the day before. Prior to the 6 free - choice trials, the skiers performed one warm-up free skiing run and one warm-up run in the slalom course. In these sessions, supervised (target skill) learning consistently selected the theoretically best skiing strategy. Conversely, in supervised (free choice) and reinforcement learning, the coach and the skiers, respectively, had the autonomy to choose skiing strategy for each run. After each session, coaches (except supervised target skill) and skiers were asked to re-evaluate and rank the strategies.

On the third and last day, the skiers performed a retention and a transfer test to assess the effect of the training approaches on learning and performance. The retention test was performed in the same course as the baseline and acquisition sessions, whereas the transfer test was performed in the transfer course and involved a progression in gate offset from start to finish. Since the transfer test was a new course, we allowed the skiers to inspect the course before the test. The retention and transfer tests were conducted with all treatment groups together. None of the treatment groups received any feedback from coaches nor time during these tests. After each test, the skiers were asked to rank the strategies. 

\subsection{Analysis}

After the data were collected, the chosen strategies of skiers or coaches were manually inputted into the database. The resulting dataset, which included raw racing times and strategy choices, was downloaded as separate CSV files and stored in the 'data' folder. These files were subsequently imported into R (v4.2.2, 2022) and processed using custom functions built upon the base R and tidyverse\cite{wickham_package_2017} packages . Information logged during the experiment, such as when skiers did not complete the course ('Did Not Finish'), was then added to the relevant trials in the dataset. After this, the data underwent a cleaning process, during which trials with implausible high or low racing times were detected and replaced with “not applicable” (NA). Instances where the timing system failed to start were manually added as trials with NA values, along with corresponding strategy information. Finally, the data underwent extensive validation, including trial counting and visual inspection of the racing times for errors. The validated dataset was saved as a new CSV file. An extensive report can be found at OSF (\url{https://osf.io/ub98v}).

Due to the hierarchical structure of the data, our general statistical strategy relies on multilevel modeling. At the first level, each skier performed multiple trials during each session. At the second level, each skier was nested within groups of ski teams that performed the experiment together. To account for these multilevel data structures, we leveraged linear mixed-effects models. To model random effects, we adopted a design-driven approach \cite{barr_random_2013, barr_learning_2021}, where we sought to account for all nonindependence introduced by repeated sampling from the same ski group and skier. We deployed classical frequentist statistics and fitted these models with the lme4 package \cite{bates_fitting_2015} in the R programming language. We used a simple coding scheme for our predictors where the intercepts represent the estimated mean of the cell means and the contrasts represent the estimated difference with respect to the reference level, which we set for reinforcement learning. Two-tailed p values and degrees of freedom for each model were derived using the lmerTest package \cite{kuznetsova_lmertest_2017} via the Satterthwaite approximation method. Alpha was set to 0.05 for all test statistics.

 \subsubsection{Race time}

Race time was analyzed using linear mixed-effect regression models. 

Initially, we planned to normalize the racing time by expressing the racing time as the difference from the straight-gliding time performed at the beginning of every session. This difference better approximate the skiers' actual skill improvement by considering the variance in snow conditions. However, practical considerations led us to deviate from this approach. This change was necessary because we had to flip or shift the course after each day to ensure snow conditions with the least damage. Unfortunately, these adjustments made maintaining a clean, straight-gliding lane difficult since the straight gliding lane crossed many areas with damage to the snow surface (holes) from the previous course set (see \ref{sup_coursesetting} for an image of these holes). Collisions with these holes affected the race time, adding noise to the results. Therefore, we used a more conservative approach and analyzed the raw racing times instead of analyzing the normalized racing times.

For the acquisition session, we modeled race time using session (forced exploration, free choice 1, free choice 2) and treatment (reinforcement learning, supervised (free choice) learning, supervised (target skill) learning and their interactions as predictors. For retention and transfer, we modeled racing times at these sessions, with treatment added as a predictor. In addition, we used the average performance for each skier on the baseline test as a predictor to improve estimate precision and adjust for group differences at baseline testing. 

To model the effect and development of the strategies we broke the analysis up into smaller sub-models: One analysis focused on differences in the strategies and groups regarding forced exploration, where all participants had completed all strategies. Another analysis examined the transition from forced exploration to retention for both supervised (free choice) and reinforcement learning. The final model investigated the development between groups specifically for the "extend with rock skis forward" strategy. Session was coded as a continuous variable in all models. 


\subsubsection{Strategy choices}

Strategy choices were analyzed using generalized linear mixed-effect regression models with a binomial logit-link function.

To model the selection of the theoretical best strategy, we inputted the data as logistic, where for each trial (\(i\)) per skier (\(j\)) within ski group (\(k\)), we counted \(y_{ijk}=1\) when the skier chose the theoretical best strategy (that is, 'extend with rock skis forward') or 0 when they did not. We included treatment and session and their interaction as two variables. To account for the nonindependence of the data structure, we allowed the intercept to vary by including a random intercept for the skier and ski groups.

 We adopted the same model formula to model the selection of the estimated best strategy. This time, however, we counted \(y_{ijk}=1\) when the skier chose their estimated best strategy or 0 when they did not select that strategy. To estimate the best strategy for each skier in the sample, we used the sample-average method \cite{sutton_reinforcement_2018} to average the racing time for each strategy and select one of the strategies with the lowest estimated (that is, best) value. The following were used for the acquisition sessions: forced exploration, free choice 1, and free choice 2. Due to the scaling issues with generalized linear models, we followed the recommendation to determine the size and significance of the effects of interest using marginal effects on the probability scale \cite{mize_best_2019, mccabe_interpreting_2022}. Interactions were assessed using discrete difference (also second difference), which is also in line with these recommendations. To derive these estimates, we used the emmeans package \cite{lenth_emmeans_2023}.

 \subsubsection{Sensitivity to feedback}

To learn how skiers and coaches used their feedback to guide their choices, we constructed a 'win-stay, lose-switch' model (WSLS; \cite{nowak_strategy_1993, worthy_comparison_2014, iyer_probing_2020}). For this WSLS analysis, we z-scored the race times for each skier for free choices 1 and 2 and counted \(y_{ijk}=1\) when the skier repeated the previous strategy and 0 when it was not. The data were modeled using a generalized linear mixed-effect regression model with a binomial logit-link function, with treatment and z-transformed race time and their interaction as the two variables. To test for differences in error sensitivity, we used the marginal effects at the mean (MEM) derived from the emmeans package \cite{lenth_emmeans_2023}.


\subsubsection{Evaluations of the strategies}
To analyze the strategies' rankings, we used single-level linear regression owing to the singularity of our multilevel models. In this model, we inputted session (familiarization, forced exploration, free choice 1, free choice 2, retention and transfer) as a continuous variable and treatment (reinforcement learning, supervised (free choice) learning and supervised (target skill) as the predictors. For supervised (free choice) learning, we used the coaches' ranking during the sessions where they selected strategies and skiers when they selected strategies.



\section{Results}





\begin{sidewaystable}\label{descriptive_skier}
\caption{\textbf{Skier characteristics}}
\centering
\begin{tabular}[H]{l|c|c|c|c|c|c}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{\textbf{Reinforcement learning}} & \multicolumn{2}{c|}{\textbf{Supervised (free choice)}} & \multicolumn{2}{c}{\textbf{Supervised (target skill)}} \\
\textbf{Sex} & \textbf{F}, N = 13 & \textbf{M}, N = 19 & \textbf{F}, N = 14 & \textbf{M}, N = 19 & \textbf{F}, N = 13 & \textbf{M}, N = 20\\
\hline
Age & 18.4 (2.3) & 17.7 (1.8) & 18.1 (2.3) & 18.3 (1.8) & 17.8 (2.4) & 18.2 (2.1)\\
\hline
Training group &  &  &  &  &  & \\
\hline
\hspace{1em}National team & 1 (7.7\%) & 1 (5.3\%) & 2 (14\%) & 1 (5.3\%) & 2 (15\%) & 4 (20\%)\\
\hline
\hspace{1em}Senior team & 3 (23\%) & 5 (26\%) & 1 (7.1\%) & 5 (26\%) & 0 (0\%) & 5 (25\%)\\
\hline
\hspace{1em}Ski academy & 9 (69\%) & 13 (68\%) & 11 (79\%) & 13 (68\%) & 11 (85\%) & 11 (55\%)\\
\hline
FIS points &  &  &  &  &  & \\
\hline
\hspace{1em}Median (IQR) & 54 (42, 80) & 46 (34, 90) & 58 (26, 80) & 44 (35, 61) & 49 (28, 66) & 31 (28, 63)\\
\hline
\hspace{1em}Not recorded & 3 & 2 & 1 & 2 & 2 & \vphantom{1} 3\\
\hline
World ranking &  &  &  &  &  & \\
\hline
\hspace{1em}Median (IQR) & 630 (394, 1,217) & 707 (364, 2,317) & 709 (133, 1,224) & 662 (387, 1,274) & 527 (145, 882) & 314 (220, 1,360)\\
\hline
\hspace{1em}Not recorded & 3 & 2 & 1 & 2 & 2 & 3\\
\hline
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} Mean (SD); n (\%)}\\
\end{tabular}
\end{sidewaystable}


\begin{sidewaystable}\label{descriptive_coach}
\caption{\textbf{Coach characteristics}}
\centering
\begin{tabular}[H]{l|c|c|c}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{\textbf{Supervised (free choice)}} & \multicolumn{1}{c}{\textbf{Supervised (target skill)}} \\
\textbf{Characteristic} & \textbf{F}, N = 2 & \textbf{M}, N = 6 & \textbf{M}, N = 3\\
\hline
Age & 38.5 (3.5) & 44.3 (8.8) & 48.0 (7.0)\\
\hline
Ski education (highest achieved) &  &  & \\
\hline
\hspace{1em}Level 2 & 0 (0\%) & 1 (17\%) & \\
\hline
\hspace{1em}Level 3/4 & 2 (100\%) & 5 (83\%) & 3 (100\%)\\
\hline
Sport science degree (highest achieved) &  &  & \\
\hline
\hspace{1em}MSc & 1 (50\%) & 0 (0\%) & 1 (33\%)\\
\hline
\hspace{1em}BSc & 1 (50\%) & 1 (17\%) & 1 (33\%)\\
\hline
\hspace{1em}No & 0 (0\%) & 4 (67\%) & \\
\hline
\hspace{1em}One-year program & 0 (0\%) & 1 (17\%) & 1 (33\%)\\
\hline
Coaching experience (years) &  &  & \\
\hline
\hspace{1em}National team (WC/EC)/Senior teams & 5.00 (1.41) & 5.00 (4.24) & 15.67 (1.15)\\
\hline
\hspace{1em}Ski academy & 7.5 (3.5) & 7.5 (6.5) & 2.00 (3.46)\\
\hline
\hspace{1em}Ski club & 2.5 (3.5) & 6.5 (8.7) & 6.0 (6.6)\\
\hline
\multicolumn{4}{l}{\rule{0pt}{1em}\textsuperscript{1} Mean (SD); n (\%)}\\
\end{tabular}
\end{sidewaystable} 

\subsection{Greater improvement during acquisition with reinforcement learning than supervised (free choice) learning}\label{result_racetime_acquisition}
We anticipated that the three treatment groups would show distinct improvements in race times during acquisition. Specifically, our hypothesis posited that reinforcement learning would yield greater race time improvements, driven by the acquisition of superior strategies through evaluations rather than instructions. Figure \ref{fig: racetime}a presents mean racing time estimates for treatment groups during acquisition.

In forced exploration, where skiers executed all strategies, the average race times did not significantly differ between reinforcement learning and supervised (free choice) learning ($\beta$ = 0.06 , 95\% CI [-0.2, 0.32], $t$(92.727) = 0.48, $p$ = 0.631) and supervised (target skill) learning ($\beta$ = 0.18, 95\% CI[-0.08, 0.44], $t$(92.663) = 1.37, $p$ = 0.174). 

When skiers entered the free choice 1, the first session where they or their coach had the autonomy to select strategies, notable variations in average race time improvements became evident. First, all treatments significantly improved their race times from forced exploration to this session (Reinforcement learning: $\beta$ = -0.38, 95\% CI[-0.45, -0.31], $t$(91.632) = -10.82, $p$ $<$ 0.001; Supervised (free choice): $\beta$ = -0.3, 95\% CI[-0.37, -0.23], $t$(93.472) = -8.96, $p$ $<$ 0.001; Supervised (target skill): $\beta$ = -0.5, 95\% CI[-0.57, -0.44], $t$(91.95) = -15.08, $p$ $<$ 0.001). However, this improvement was significantly larger for supervised (target skill) learning, with coaches selecting the best theoretical strategy than reinforcement learning ($\beta$ = -0.12, 95\% CI[-0.22, -0.03], $t$(91.777) = -2.58, $p$ = 0.012). In contrast, supervised (free choice) learning, where coaches made strategy choices for the skiers, improved less from forced exploration to free choice 1, but this was not statistically significant ($\beta$ = 0.08, 95\% CI[-0.02, 0.17], $t$(92.5) = 1.61, $p$ = 0.110).

Significant improvement from forced exploration persisted for all groups in free choice 2 (Reinforcement learning: $\beta$ = -0.45, 95\% CI[-0.54, -0.36], $t$(95.164) = -9.99, $p$ < 0.001; Supervised (free choice): $\beta$ = -0.31, 95\% CI[-0.39, -0.22], $t$(96.389) = -7.17, $p$ < 0.001; Supervised (target skill): $\beta$ = -0.43, 95\% CI[-0.52, -0.35], $t$(96.196) = -10.07, $p$ < 0.001). The average race time for the reinforcement learning group continued to drop, however, but plateaued for supervised (free choice) learning, resulting in a significant difference in change between the groups ($\beta$ = 0.14, 95\% CI[0.02, 0.26], $t$(95.743) = 2.26, $p$ = 0.026). As supervised (target skill) learning' race times declined from free choice 1 to 2, their initial greater race time improvement attenuated resulting in a non-significant interaction effect ($\beta$ = 0.02, 95\% CI[-0.11, 0.14], $t$(95.651) = 0.26, $p$ = 0.798).

However, we did not find statistical evidence that reinforcement learning performed better than supervised (freechoice) or (target skill) learning at free choice 1 or 2 (Supplementary Table \ref{suptable_racetime_groupdiffeachsession}).
Collectively, these findings indicate that the treatment groups exhibited distinct improvements during acquisition.


\begin{figure}[H]
\centering
\includegraphics{figures/figure_racingtimes_2.pdf}
\caption{Race time across the different sessions for the three treatment groups. \textbf{a}. Displays the race time estimated from the model during acquisition. Forced exploration refers to the sessions wherein skiers tried all strategies, whereas free choice refers to the session wherein skiers or coaches selected strategies according to their assigned treatment groups. \textbf{b.} Displays the estimated race time from the model for retention. \textbf{c.} Displays the estimated race time from the model for transfer. Intervals represent the 95\% confidence interval derived from the models. Asterisks (*) indicate a statistically significant interaction effect. Each light gray point represents a singular trial by a skier, thus showing the full multilevel structure of the dataset}
\label{fig: racetime}
\end{figure}




\subsection{Better retention with reinforcement learning than supervised (free choice) but not supervised (target skill) learning} \label{result_racetime_retentipon}
We found evidence that reinforcement learning improved more during initial learning than supervised (free choice) learning. During the learning phase, however, it was the coach who chose the strategy for the performer. The question is what happens when skier chose the strategy themselves after an overnight sleep. We hypothesized that skill retention was better for reinforcement learning after 24 hours compared to supervised learning. Figure \ref{fig: racetime}b presents the mean race time estimates during retention.

The race times for the reinforcement learning group were on average significantly better than those for supervised (free choice) learning when controlling for baseline differences ($\beta$ = 0.12, 95\% CI[0.01, 0.24], $t$(101.422) = 2.12, $p$ = 0.037). The difference between reinforcement learning and supervised learning (target skill) also favored reinforcement learning but was smaller and not statistically significant ($\beta$ = 0.07, 95\% CI[ -0.04, 0.19], $t$(101.63) = 1.27, $p$ = 0.206). 

\subsection{No convincing evidence for better transfer for reinforcement learning compared to supervised learning} \label{result_racetime_transfer}
Another key question concerns the knowledge transfers to new situations. We also hypothesized that reinforcement learning would improve skill transfer to a new slalom course. Figure \ref{fig: racetime}c presents the mean race time estimates during retention.

As for retention, the race time on the transfer course was on average better in reinforcement learning than in supervised (free choice), yet the difference was smaller and not statistically significant when controlling for baseline differences ($\beta$ = 0.1, 95\% CI[-0.02, 0.21], $t$( 99.979) = 1.7, $p$  = 0.091). The race times for reinforcement learning and supervised (target skill) learning was on average identical when controlling for baseline differences ($\beta$ = 0, 95\% CI[-0.12, 0.11], $t$(100.033) = -0.04, $p$ = 0.967). Thus, we did not find convincing evidence for improved transfer.


\subsection{No evidence that reinforcement learning chose the theoretical best strategy more often than supervised (free choice) learning}\label{subsubsec3}
We found that reinforcement learning accelerated the learning process during acquisition and improved skill retention compared to supervised (free choice) learning. Conversely, we failed to find convincing evidence of outperforming supervised (target skill) learning during acquisition or retention. We hypothesized that this behavior was caused by variations in strategy selection during acquisition, retention, and transfer, with reinforcement learning showing better learning to choose better strategies than supervised (free choice) learning. Figure \ref{fig: choice_descriptives} displays the percentage selections of the four strategies across sessions.

\begin{figure}[H]
\centering
\includegraphics{figures/figure_choice_descriptivecount_4.pdf}
\caption{Strategy choices for skiers in each session, with bars indicating percentages and the numbers to the right of the bars showing the total count for
each strategy. Data has been aggregated from our multi-level structure for clarity}\label{fig: choice_descriptives}
\end{figure}


We first tested whether reinforcement learning was more likely to choose the theoretical best strategy. The figure \ref{fig: choice_estimated} displays the predicted probabilities for the treatment groups across the sessions. 

In Free Choice 1, neither reinforcement learning (0.43) nor supervised (free choice) learning (0.42) preferred the theoretically optimal strategy. This predicted probability difference was not significant ($\beta$ = 0.01, 95\% CI[-0.24, 0.27], $z$ = 0.11, $p$ = 0.912). 

Both groups remained conservative in their strategy selection during Free choice 2. The reinforcement learning group increased its predicted probability marginally from 0.43 in Free choice 1 to 0.45 in Free Choice 2, with no statistical significance ($\beta$ = 0.02, 95\% CI[-0.11, 0.14], $z$ = 0.26, $p$ = 0.791). Similarly, the supervised (free choice) learning group saw a slight increase from 0.43 in Free choice 1 to 0.5 in Free Choice 2, and this change was also not statistically significant ($\beta$ = 0.08, 95\% CI[-0.04, 0.21], $z$ = 1.32, $p$ = 0.188). The difference in change between groups was not statistically significant ($\beta$ = 0.07, 95\% CI[-0.11, 0.24], $z$ = 0.74, $p$ = 0.458) nor was the group difference at Free Choice 2 ($\beta$ = -0.05, 95\% CI[-0.31, 0.2], $z$ = -0.4, $p$ = 0.693). 

A noticeable trend emerged in retention, with skiers in supervised (free choice) learning also having the autonomy to independently select strategies, free from the coach's guidance. Here, supervised (free choice) learning significantly increased its predicted probability of selecting the theoretical best strategy, rising from 0.43 in Free Choice 2 to 0.45 in Retention ($\beta$ = 0.23, 95\% CI[0.09, 0.36], $z$ = 3.31, $p$ < 0.001). In the same transition, the change from 0.45 to 0.5 for the reinforcement learning group was not statistically significant ($\beta$ = 0.05, 95\% CI [-0.09, 0.19], $z$ = 0.72, $p$ = 0.469). The difference in change between the groups was not statistically significant ($\beta$ = 0.17, 95\% CI[-0.02,  0.37], $z$ = 1.74, $p$ = 0.081), not was their difference at retention ($\beta$ = -0.23, 95\% CI[-0.48, 0.03], $z$ = -1.75, $p$ = 0.079).

The predicted probability of choosing the theoretical best strategy further increased both groups, but this increase was not statistically significant for reinforcement learning ($\beta$ = 0.11 , 95\% CI [ -0.04 ,  0.27 ], $z$ = 1.42 , $p$  =  0.155) or supervised (free choice) learning ($\beta$ = 0.05, 95\% CI[-0.07, 0.18], $z$ = 0.85, $p$ = 0.396). Neither their difference in change from retention ($\beta$ = -0.06, 95\% CI[-0.25, 0.14], $z$ = -0.58, $p$  =  0.564), nor the differences between the groups on transfer were significant ($\beta$ = -0.17, 95\% CI [-0.4, 0.07], $z$ = -1.41, $p$ = 0.159).


\begin{figure}[H]
\centering
\includegraphics{figures/figure_choice_estimated_4.pdf}
\caption{Strategy selection for reinforcement learning (red) and supervised (free choice) learning (blue) during acquisition. \textbf{a.} Displays the estimated probability of choosing the theoretically best strategy (that is, extend with rocking skis forward) for both reinforcement learning and supervised (free choice) learning. \textbf{b.} Presents the estimated probability of selecting the empirically determined best strategy (that is, estimated using the sampling average method for each skier in the dataset). Intervals represent the 95\% confidence interval derived from the models. Asterisks (*) indicate a statistically significant interaction effect.}\label{fig: choice_estimated}
\end{figure}

\subsection{A higher proportion of skiers in the supervised (free choice) learning had the theoretical best strategy as their estimated best}\label{subsubsec3}
We did not find evidence that the predicted probability of choosing the theoretical best strategy was higher on average among skiers in the reinforcement learning group compared to those in the supervised (free choice) learning group. In contrast, it was the supervised (free choice) learning group that increased their selection of this strategy. As a follow-up analysis, we investigated whether a higher proportion of skiers in the supervised (free choice) learning group had the theoretically best strategy as their estimated best strategy. Such a trend could suggest that the feedback from coaches during the learning attempts may have made this strategy more effective. 

Figure \ref{fig: choice_estimated}b shows the proportion of skiers for each group that had the theoreticalp best strategy as their estimated best strategy. In this analysis, we found that 40\% of skiers in reinforcement learning had the theoretically best strategy as their estimated best strategy compared to 75\% in the supervised (free choice) group. A chi-square test revealed a association between difference between the groups was significant $\chi^2$ = 6.42, $p$ = 0.01). Because all skiers in the supervised (target skill) group had the theoretical best as their estimated best strategy, we did not include this group in the test.  


\subsection{Reinforcement learning increased their choices ??chose their best strategy more often than supervised (free choice) learning}\label{subsubsec3}
The observation that skiers in supervised (free choice) learning increasingly opted for the theoretically best strategy more across sessions than those in reinforcement learning does not necessarily imply a more frequent selection of their estimated best strategy. Consequently, we investigated whether reinforcement learning resulted in a higher predicted probability of selecting the estimated best strategy. Figure \ref{fig: choice_estimated}c displays the predicted probabilities of choosing the estimated best strategy for the treatment groups across the sessions. 

In free choice 1, the predicted probability difference between reinforcement learning (0.58) and supervised (free choice) learning (0.57) was small and not significant ($\beta$ = 0.01, 95\% CI [-0.22, 0.24], $z$ = 0.12, $p$ = 0.904). 

In free choice 2, the predicted probability of choosing the estimated best strategy significantly increased for the reinforcement learning group ($\beta$ = 0.2, 95\% CI [0.09, 0.32], $z$ = 3.39, $p$  $<$ 0.001), while it did not significantly increase for supervised (free choice) learning ($\beta$ = 0.09, 95\% CI [-0.03, 0.21], $z$ = 1.48, $p$ = 0.140). Their difference in change was not significant, however ($\beta$ = -0.11, 95\% CI [-0.28, 0.05], $z$ = -1.33, $p$ = 0.184), nor was their predicted probability difference at free choice 3 ($\beta$ = 0.13, 95\% CI [-0.06, 0.32 ], $z$ = 1.3 , $p$  =  0.194).

When skiers in the supervised (free choice) learning were allowed freedom to choose strategies on retention, their predicted probabilities of choosing their estimated best strategy also increase. This increase was not statistically significant ($\beta$ = 0.12, 95\% CI [0, 0.24], $z$ = 1.88, $p$ = 0.060). This was in contrast to the reinforcement learning group, where the predicted probability decreased by 0.07 from free choice 2 to retention, but this was not statistically significant ($\beta$ = -0.07, 95\% CI [-0.19, 0.04], $z$ = -1.25, $p$ = 0.213). However, their difference in change was statistically significant ($\beta$ = 0.19, 95\% CI [0.02, 0.36], $z$ = 2.2, $p$ = 0.028). We did, however,  not find any statistically significant difference between the groups at retention ($\beta$ = -0.06, 95\% CI[-0.26, 0.14], $z$ = -0.61, $p$ = 0.544).

Neither reinforcement learning ($\beta$ = 0.08, 95\% CI[-0.04, 0.2], $z$ = 1.31, $p$ = 0.190) nor supervised (free choice) learning ($\beta$ = 0.02, 95\% CI[-0.09, 0.13], $z$ = 0.35, $p$ = 0.727) showed a significant increase in predicted probabilities from the retention phase to the transfer phase. This difference in change was not significant ($\beta$ = -0.06 , 95\% CI [-0.23, 0.1], $z$ = -0.73, $p$ = 0.467), nor was the predicted probability difference at transfer ($\beta$ = 0, 95\% CI [-0.17, 0.17], $z$ = 0, $p$ = 0.999).

\subsection{Reinforcement learning had a lower costs when performing suboptimal strategies compared to  reinforcement learning}\label{subsubsec3}
Choosing the estimated best strategy is one thing, but avoiding the selection of a strategy significantly worse than the estimated best strategy is another. In a follow-up analysis, we computed the expected difference between the skiers' chosen suboptimal strategy and their estimated best strategy, which we referred to as 'cost.' A lower cost implies that the performer has better grasped the effects of various strategies. This analysis revealed that reinforcement had lower costs than supervised (free choice) learning did  ($\beta$ = 0.06 , 95\% CI [ 0.01 ,  0.12 ], $t$( 30.789 ) = 2.55 , $p$  =  0.016), suggesting that they had better learned the strategies.



\subsection{Sensitivity to feedback}\label{subsec5}
We conducted a win-stay, lose-switch (WSLS) analysis to test if reinforcement learning demonstrated greater sensitivity to feedback compared to supervised (free-choice) learning. In this analysis, heightened sensitivity would be indicated by a high probability of repeating an action following positive feedback and a low probability following negative feedback on a previous trial. As depicted in the model predictions (Fig. \ref{fig: choice_wsls}), both reinforcement learning and supervised (free-choice) learning demonstrated a higher probability of repeating a strategy after receiving positive feedback compared to negative feedback, as indicated by the negative slope for both groups. We found that the estimated marginal effects at the mean (MEM) was considerably lower for reinforcement learning (-0.18) compared to compared to supervised (free choice) learning (-0.11), yet this difference was not statistically significant %+(\input{latex/statistics/contrasts/contrast_winstay_loseswitch_rl_minus_slfc}).




\begin{figure}[H]
\centering
\includegraphics{figures/figure_winstaylooseshift.pdf}
\caption{Win-stay, lose-switch comparison between reinforcement learning and supervised (free choice) learning. The line shows the predicted probability of repeating the previously chosen strategy based on its trial feedback, along with a 95\%CI in the ribbon. In this model, higher or lower probabilities with better and worse feedback mean greater sensitivity to feedback}\label{fig: choice_wsls}
\end{figure}



\subsection{Larger cognitive separation of strategies in the reinforcement and supervised (free choice) learning than in supervised (target skill) learning}\label{subsec4}
Directly reflecting their understanding of the effectiveness of strategies is how skiers or coaches evaluate them. We expected that these evaluations would diverge among the treatment groups and that reinforcement learning would have achieved a more consistent evaluation with their actual race times. Figure \ref{fig: rank}a displays the rankings for individuals with the authority  to choose strategies across all sessions, except for supervised (target skill) learning, where the skiers' rankings are shown because these coaches were informed ahead of the experiment about which strategies were expected to be the best. 

As shown in Figure \ref{fig: rank}a and in supplementary table \ref{suptable_strategyranking_strategydifffam}, all treatment groups initially ranked 'extend with rock skis forward' as the best, followed by 'extend,' 'rock skis forward,' and 'stand against' immediately after the strategies' introduction during the familiarization phase. We did not find statistically significant differences in ranking between reinforcement learning and the two supervised learning groups for the strategies (Supplementary table \ref{suptable_strategyranking_diffgroupfam}, except that supervised (target skill) learning ranked 'extend' worse ($\beta$ = 0.45, 95\% CI[0.24,  0.67], $t$() = 4.17, $p$ $<$ 0.001) and 'extend with rock skis forward' better  ($\beta$ = -0. 42, 95\% CI[-0.63, -0.2], $t$() = -3.83, $p$ $<$ 0.001) than reinforcement learning did.

The data depicted in the figure and the regression model reported in supplementary table \ref{suptable_strategyranking_strategiesslope} indicate that the positional ranks for the strategies "stand against" and "extend with rock skis forward" remained stable throughout all sessions across the treatment groups, yet there were some interesting trends. The rank of 'stand against' descriptively worsened over the sessions for all groups, but the change was only statistically significant for the supervised (target skill) learning group  ($\beta$ = 0.09 , 95\% CI [ 0.04 ,  0.13 ], $t$() = 3.4 , $p$  $<$  0.001). In contrast, the 'extend with rock skis forward' strategy was ranked better over the sessions for all groups. However, the change was only statistically significant for the supervised (target skill) learning group ($\beta$ = -0.06 , 95\% CI[-0.11, -0.01], $t$( ) = -2.51, $p$ = 0.012). 

The most interesting trend shifts were observed for the two middle-ranked strategies: 'extend' and 'rock skis forward'. The reinforcement learning group ranked the strategy 'extend' better and 'rock skis forward' worse over the sessions, and these changes were found to be statistically significant (Extend: $\beta$ = -0.1 , 95\% CI [ -0.15 ,  -0.05 ], $t$() = -3.73 , $p$  <  0.001; Rock skis forward:  $\beta$ = 0.09 , 95\% CI [ 0.04 ,  0.15 ], $t$(  ) = 3.57 , $p$  <  0.001). Although supervised (target skill) and supervised (free choice) exhibited the same trend, they magnitude was smaller and did not reach statistical significance. We also identified a statistically significant interaction effect between reinforcement learning and supervised (target skill) learning for the 'rock skis forward' strategy ($\beta$ = -0.07, 95\% CI[-0.14 to 0], $t$() = -1.97,$p$ = 0.049), providing evidence that reinforcement learning learned to separate strategies better over time over time than the supervised (target skill) learning did. We did, however, not find other interaction effects (Supplementary table \ref{suptable_strategyranking_diffindiff_strategiesslope})) 


\begin{figure}[H]
\centering
\includegraphics[]{figures/figure_ranking_average_3.pdf}
\caption{Strategy evaluations and effects. \textbf{a. }Average descriptive ranking of the four strategies per treatment group. Rankings range from 1 (best) to 4 (worst). For supervised (free choice) learning, the coach's ranking during the acquisition phase and the skier's ranking during the retention and transfer phases are plotted, reflecting the decision- maker for strategy selection. The circle represents the mean, and the ribbon indicates the standard deviation (SD). \textbf{b.} Average race time of the four strategies across the three treatment groups. The circle represents the mean, and the ribbon represents the SD. Note that all skiers tested the strategies during the forced exploration phase, but as the study progressed, there may have been fewer observations for some strategies. Consequently, the calculation of the mean might be heavily influenced by these observations. The mean race time was calculated by first determining each participant's average time for each strategy per session, followed by calculating the mean of these averages.}\label{fig: rank}
\end{figure}
\subsection{Performance strategies}\label{subsec5}
Similar to the variations in how skiers ranked the strategies, we also observed divergent race times achieved by the skiers during their execution. On average, skiers performed worst with the "stand against" strategy and notably improved their race time with the other strategies (Fig.\ref{fig: rank}b and Supplementary Table \ref{Supplementarytable_strategyeffect_5}). 

During forced exploration, there were no statistically significant differences between the reinforcement learning and supervised learning groups for any of the strategies:  "stand against", "rock skis forward", "extend", or "extend with rock skis forward" (Fig.\ref{fig: rank}b and Supplementary Table \ref{Supplementarytable_strategyeffect_6}). 

The race times for all strategies improved significantly from the forced exploration to retention for both reinforcement learning and supervised (free-choice) learning, except for the "stand-against" strategy, which was not statistically significant in the reinforcement learning group. This deviation may be attributed to the limited number of observations for this strategy within the group. Interestingly, we found that the reinforcement learning group improved more on the "extend" strategy from forced exploration to retention than the supervised (free choice) learning group ($\beta$ = 0.04 , 95\% CI[0.01, 0.07], $t$(1378.879) = 2.34, $p$ = 0.020). We did not find evidence for such an interaction effect for any of the other three strategies ( Supplementary Table \ref{Supplementarytable_strategyeffect_7}). 

As supervised (target skill) learning only trained on "extend with rock skis forward" strategy during the free choice sessions and almost exclusively during retention, we conducted a separate analysis comparing all groups on this strategy. This analysis revealed that all groups improved on the "extend with rock skis forward" strategy  (Supplementary Table \ref{Supplementarytable_strategyeffect_8}). However, no statistically significant differences were found in skill improvement for this strategy between reinforcement learning and either supervised (free choice) learning ($\beta$ = 0, 95\% CI[-0.03, 0.02], $t$(1064.204) = -0.3, $p$ = 0.765) or supervised (target skill) learning ($\beta$ = 0.02, 95\% CI [-0.01, 0.04], $t$(1062.577) = 1.07, $p$ = 0.283).



\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Section title of first appendix}\label{secA1}

An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
