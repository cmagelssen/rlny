%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  

%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
\RequirePackage{amsthm}

\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Reference 
% <some more package/preamble stuff>



%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{longtable}
\usepackage{float}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Reinforcement learning accelerates expertise in skilled alpine ski racers compared to standard coaching}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[]{\fnm{Christian} \sur{Magelssen}}\email{cmagelssen@gmail.com}

\author[2]{\fnm{Robert} \sur{Reid}}\email{iiauthor@gmail.com}


\author[1]{\fnm{Matthias} \sur{Gilgien}}\email{iiauthor@gmail.com}


\author[1]{\fnm{Per} \sur{Haugen}}\email{perh@nih.no}


\author[2]{\fnm{Simen Leithe} \sur{Tajet}}\email{iiiauthor@gmail.com}


\author[1]{\fnm{Thomas} \sur{Losnegard}}\email{iiiauthor@gmail.com}


\author[4]{\fnm{Romy} \sur{Frömer}}\email{r.froemer@bham.ac.uk}


\affil*[1]{\orgdiv{Institute for Physical Performance}, \orgname{Norwegian School of Sport Sciences}, \orgaddress{\street{Sognsveien 220}, \city{Oslo}, \postcode{0863}, \state{Oslo}, \country{Norway}}}

\affil[2]{\orgdiv{Institute of Sport and Social Science}, \orgname{Norwegian School of Sport Sciences}, \orgaddress{\street{Sognsveien 220}, \city{Oslo}, \postcode{0863}, \state{Oslo}, \country{Norway}}}

\affil[3]{\orgdiv{The Norwegian Ski Federation}, \orgname{Organization}, \orgaddress{\street{Sognsveien 75 B1}, \city{Oslo}, \postcode{0840}, \state{Oslo}, \country{Norway}}}

\affil[4]{\orgdiv{School of Psychology}, \orgname{University of Birmingham}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Learning to select effective technical and tactical solutions to situations which arise during competition is a hallmark of expertise. Typically, these strategies are often taught by a coach who tells learners what to do. We propose that these strategies may be better learned with reinforcement learning, which emphasizes learning from evaluations. To test this idea, ninety-eight skilled alpine skiers participated in a three-day learning experiment aimed at improving their performance on flats on slalom via four strategies. In the reinforcement learning group, the skiers chose strategies themselves based on trial feedback for evaluation. We compared this group to two supervised learning groups, one wherein we recruited coaches from the tested ski teams to coach on the strategy they believed would be best (free choice) and another where we recruited current World Cup coaches to coach on the theoretical best strategy (target skill). We found that skiers in the reinforcement learning group learned better than did those in the supervised (free choice) learning group but not compared to those in the supervised (target skill) group. However, selecting only one strategy comes at its own cost. Together, our results suggest that reinforcement learning can be an important training strategy for accelerating expertise}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Reinforcement learning, Supervised learning, Expertise development, Alpine skiing}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}

The development of expertise demands extensive amounts of high-quality training \cite{ericsson_role_1993, hodges_predicting_2004, vaeyens_talent_2009, sosniak_learning_1985}. Unlike training novices to reach acceptable levels on a task, the key challenge for skilled performers is finding ways to improve beyond current levels of performance\cite{ericsson_development_2003, ericsson_scientific_1998, gray_plateaus_2017, williams_expertise_2008, du_relationship_2022}. One route to achieve this goal is to perfect an already chosen strategy \cite{krakauer_motor_2019, du_relationship_2022}. By choosing to stay with current strategy, learning progresses slowly and operates through honing the components of the machinery underlying the automated solution \cite{du_relationship_2022}. A second route is to switch to a new and better strategy, which in many cases can lead to a greater leap in performance than simply repeating what is already automated  \cite{gray_plateaus_2017, du_relationship_2022, krakauer_motor_2019}. Finding such clever and innovative solutions is a key characteristic of expertise \cite{ericsson_scientific_1998, ericsson_development_2003}. Yet, surprisingly we know little about which teaching methods are most effective in stimulating learners to make good strategic choices, let alone the sources of information that drive strategy selection and development\cite{taylor_cerebellar_2014, taylor_role_2012}. If methods superior to those currently used in standard practice exist, they could prove invaluable for training both present and future generations of learners.

In current practice, these strategies are typically taught through instructional methods, where a coach or teacher conveys what to do (e.g., take a shorter line around the gate), followed by corrective feedback (e.g., you can shorten the line even more) \cite{williams_practice_2005, williams_effective_2023, hodges_role_1999}. This teaching strategy can be likened to what motor learning refers to as supervised learning, where the teaching signal for skill improvement represents the disparity between the desired skill outcome and the learner outcome \cite{jordan_forward_1992, wolpert_motor_2010, doya_complementary_2000}. Through practice, this teaching signal can bring the learner closer to executing what is assumed to be the correct choice. However, is it always wise to listen to others' proposed solutions, or are there better methods for discovering innovative and vastly more effective strategies?

One drawback of the supervised learning strategy for training these decisions is that learners are simply told what to do based on what coaches believe to be a good strategy from their knowledge and experience. However, what coaches judge as a good strategy does not always align with reality, even for the best-trained eye \cite{supej_impact_2019, cochrum_visual_2021}. Learners might, therefore, miss opportunities to discover the best strategy when coaches opt for suboptimal strategies \cite{gray_plateaus_2017}. Worse, these suboptimal strategies might turn into habits and be difficult to break if they are practiced enough \cite{popp_effect_2020}.  Supervised learning might also constrain learners to adopting a single ('universal') strategy for all situations rather than acquiring a repertoire of strategies and discerning the most effective strategies for each specific scenario. Finally, it remains uncertain whether the prescriptive approach is the most effective teaching strategy for achieving long-lasting learning effects \cite{wulf_instructions_1997, hodges_role_1999, williams_practice_2005,williams_effective_2023} 

Learning to choose good strategies can also happen without the direct influence of a coach providing advice. The cornerstone of reinforcement learning \cite{sutton_reinforcement_2018} is that learners can learn by exploring strategies and evaluating their outcomes, using the successes and failures of outcomes as teaching signals. That is, rather than being told the putatively correct solution to the problem, as in supervised learning, they learn the value of different strategies, which allows them to finally pick the best solution. Specifically, these values are learned by comparing a given choice's outcomes with the currently expected outcome of that choice. Outcomes that exceed or fall short of expectations result in errors in reward prediction, signaling that the learner must update their predictions to better anticipate future rewards following that action \cite{rescorla_theory_1972}. These reward prediction errors are then incorporated to form a new and better estimate of reward, by updating expectations through a weighted running average. Reinforcement learning has been tremendously powerful in explaining human and animal learning \cite{waelti_dopamine_2001, schultz_neural_1997, pessiglione_dopamine-dependent_2006}, improving skill learning in laboratory-based tasks \cite{lior_shmuelof_overcoming_2012, abe_reward_2011, truong_error-based_2023, hasson_reinforcement_2015}, as well as training AI to perform complex tasks such as computer games starting from pixel inputs, only\cite{mnih_human-level_2015}. Based on this evidence, our question was whether reinforcement learning offers an alternative to standard coach-based supervised learning to improve strategy selection and performance for skilled performers. 

To address this question, we conducted a three-day learning experiment with ninety-eight skilled and elite alpine ski racers from Norway and Sweden. To achieve performance improvement among this strong cohort of athletes, we chose to focus on flat sections in slalom, an area with considerable potential for enhancement even among the best performers. To further develop the skill of proficiently skiing flat sections, we delineated four strategies (Figure \ref{fig:courseandstrategies}b), each carefully selected to enhance performance by being rooted in mechanics \cite{lind_physics_2013}, observations of elite skiers\cite{magelssen_is_2022, reid_kinematic_2010}, simulation studies \cite{mote_accelerations_1983, luginbuhl_identification_2023} or coaching knowledge. Our hypothesis was that skiers in the reinforcement learning group would learn to choose better strategies and thus achieve better performance than skiers subject to traditional supervised learning with a coach. To test this, we assigned skiers to three different learning groups with different instructions and feedback (Fig. \ref{fig:experiment}b): In the reinforcement learning group, skiers chose a strategy on every run and saw their race times to inform these decisions. In the supervised (free choice) learning group, we recruited ski coaches from the tested ski teams to coach on the strategy they believed to be the best or appropriate for the skier. In the supervised (target skill) learning, we recruited ski coaches to instruct skiers to select the strategy that we defined as the theoretically best strategy based on computational modeling \cite{lind_physics_2013, mote_accelerations_1983} and observations of elite skiers \cite{reid_alpine_2020, magelssen_is_2022}. The coaches in both supervised learning groups were highly experienced (Table \ref{descriptive_coach}). Coaches in the two supervised learning groups saw the times but were instructed to not disclose these to the skiers. 

We found that the reinforcement learning group improved more during acquisition and performed better in retention than the supervised (free choice) learning group. Both groups chose the individual skiers' estimated best strategy more often over the course of the sessions, but we did not find convincing evidence that the reinforcement learning group chose this strategy more often than the supervised (free choice) learning group. That said, we found that reinforcement learning had lower costs for suboptimally chosen strategies (that is, the expected difference between the individual skiers' estimated strategy and their chosen strategy), suggesting that they had better learned to avoid bad strategies. This was not the sole explanation for their improved learning, however. The skiers also improved more on one strategy that they picked often than the supervised (free choice) learning, suggesting that reinforcement learning also increased motor vigor. We did not find convincing evidence that reinforcement learning learned better than supervised learning (target skill), providing evidence for the importance of selecting effective strategies.  However, selecting only one strategy all the time came at its own cost—the skiers in the supervised (target skill) learning did not learn to dissociate the effect of the other strategies despite large differences in race times. 


\begin{figure}[H]
\centering
\includegraphics{figures/figure_method_courseandstrategy.pdf}
\caption{\textbf{a.} Illustrations of the two slalom courses used in the study. The main slalom course was a rhythmic course deployed in all sessions except for the transfer test. The course setting for the transfer test involved a progression in gate offset, starting with the largest offset and ending with the smallest offset. \textbf{b.} Illustration of the strategies defined to enhance racing performance on flat terrain in slalom: The "stand against" strategy emphasized maintaining a stable stance against external forces without body extension along the body's longitudinal axis or rocking skis forward; 'Rock skis forward' involved rocking skis forward from gate passage to completion of the turn; The "extend" strategy involves extending the body from a laterally tilted position during the turn, closer to the turn's center of rotation; The "extend with rocking skis forward" was expected to be the best strategy combining the two effects from extending and rocking skis forward, and we therefore defined this as the theoretical best strategy}
\label{fig:courseandstrategies}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{figures/figure_method_experiment.pdf}
\caption{Illustration of the experimental design and procedure. \textbf{a.} Timeline of the three-day learning experiment. During the baseline, skiers skied a slalom course in the shortest amount of time possible without receiving race time feedback. The skiers were then assigned to three treatment groups (see b). In their assigned group, skiers underwent an acquisition phase in their designated treatment group comprising one Forced Exploration (skiers performed all strategies) and two Free Choice sessions (skiers or coaches could choose strategies themselves). On the last day, skiers completed a retention and transfer test where they could pick strategy themselves, again without receiving race time feedback. \textbf{b.} Illustration of treatment groups in the study. Supervised (target skill) learning involved coaches consistently choosing the theoretically best strategy (except during Forced Exploration), while supervised (free choice) learning allowed coaches to freely select strategies. Skiers in both of these treatment groups received feedback on strategy execution from their respective coach, while skiers in the reinforcement learning group independently selected strategies and received feedback from the timing system to facilitate value learning of each strategy}
\label{fig:experiment}
\end{figure}

\section{Method}


\subsection{Participants}
Conducting studies on alpine ski racing poses challenges related to environmental control and resource constraints. Our sample size approach involved recruiting as many skiers as possible during June 2023, when we had a short time window to test skiers in the indoor ski hall. We set the minimum sample size to 80 skiers, which we deemed appropriate for this context. Prior to data collection, data and power simulations for sample sizes of 80, 100, and 120 skiers were conducted, revealing simulated powers of 0.60, 0.75, and 0.80, respectively, for the smallest effect size of interest (0.3 second difference between groups) (\url{https://osf.io/c4t28}). The smallest effect size of interest was based on our knowledge of alpine skiing and discussions with coaches, but it was intended for a 50-meter longer course and more training sessions than we ultimately ended up using due to practical considerations. We deliberately opted to recruit skiers with diverse skill levels for the study to augment the generalizability of our findings. However, to ensure a sufficient skill level to handle the specific icy snow conditions prepared in the skiing hall, we recruited only skiers aged 15 and older. The sample size justification, task design, and analysis plan were preregistered before data collection (\url{https://osf.io/tfb2w}).

We managed to recruit ten alpine ski teams comprising 98 alpine ski racers from Norway and Sweden (age M = 18.1 years, SD= 2; 40 females, 58 males). Two skiers were excluded from the analysis due to an injury prior to the study (n=1) or sickness during the study (n=1); thus, a total of 96 skiers completed the entire study and were included in the analysis.  Among the ski groups tested were five ski academies, three senior development teams, and two national ski teams. These skiers were generally highly skilled, with a median world rank of 605, but there was also considerable variability, as indicated by a substantial interquartile range (Q1 = 248, Q3 = 1390.5). A smaller subset of the participants (n = 13) was not world-ranked, as they had yet to compete in internationally sanctioned races that form the basis for calculating athlete points and rankings. Table 1 provides demographic information for each treatment group.

We also recruited 11 coaches (2 women; 6 men) to serve as coaches for the supervised learning groups. This selection was rather pragmatic because it was not possible to test all ski teams simultaneously due to space and time constraints in the skill hall. The 10 ski teams were therefore divided into 4 groups for which the study was conducted at different times. For each of the four groups, we recruited two coaches from the ski teams to serve as coaches in the supervised (free choice) learning group, totaling 8 coaches (2 women; 6 men). These coaches had extensive coaching experience in coaching alpine ski racers. In addition, for each group of ski teams that completed the experiment together, we recruited a third coach to coach the skiers in supervised (target skill) learning. To ensure that these coaches had sufficient credibility to make the skiers buy into our theoretical best strategy, we selectively recruited three highly experienced coaches from the Norwegian alpine ski team (one coach had to serve twice due to illness of the fourth coach). Importantly, all the coaches remained unaware of the experimental manipulation. Table 2 provides demographic information about the coaches. All the skiers and coaches provided informed consent before the study. The study was approved by the Human Research Ethics Committee of The Norwegian School of Sport Sciences.


\subsection{The setup}
The experiment was conducted in the indoor ski hall SNØ in Oslo, Norway (\url{https://snooslo.no/}). In this hall, we used a 210-meter-long flat section of the race hill, which we water-injected before testing each group of skiers to ensure uniform and fair snow conditions for all skiers. For additional preparation details and images, please refer to Supplementary \ref{sup_snowprep}. With our chosen course setup, this 210-meter-long flat section allowed space for 19 slalom gates. We sat two types of slalom courses in the experiment (Figure \ref{fig:design}a). The main slalom course was used in all sessions, except during the transfer test, and featured a 10 m distance and a 1.9 m offset. The course distance aligned with our previous study \cite{magelssen_is_2022}, but we opted for a slightly larger offset to better suit the skill level of our skiers. The transfer test aimed to evaluate how skiers transferred their learning to a slalom course that resembled a typical alpine ski racing course. To assess this, we set a course with a progression in gate offset, starting with five gates set at a 2.2-meter offset, followed by seven gates at a 1.7-meter offset, and concluding with seven gates at a 1.2-meter offset. Although we did not expect radical differences in strategy effects, we anticipated a greater emphasis on rocking the skis forward in gates with a 2.2-meter offset than in those with a 1.2-meter offset to enhance turn exit release. Both courses were set with stubbies (short gates) instead of long gates to minimize energy dissipation upon hitting the gate \cite{minetti_biomechanics_2018}. Using long gates can also be a distracting element in that skiers' attention is allocated to clearing the gate instead of focusing on executing the skill. Finally, this approach helped us avoid creating holes in the course, which can occur when the long gate is forcefully slammed into the ground.

The start gate was positioned 20 meters before the first gate. Skiers were required to start in a static position to ensure consistency in the starts, with their binding front head placed behind the starting gate. The skiers started by putting their skis in parallel and lifting up the poles without using poling or skating for propulsion. For an illustration of the starting procedure and setup, see Supplement Video. We recorded the times using a wireless photocell timing system (HC Timing wiNode and wiTimer; Oslo, Norway). Timing started when the skier crossed the first photocell pair situated 10 meters below the starting gate. To minimize wear and tear on the course, we set two parallel and identical courses and routinely shifted between them.
 
\subsection{Experimental design}
We employed a between-subjects design and posed the learning question of discovering effective strategies as an \textit{n}-armed bandit problem \cite{sutton_reinforcement_2018}. The essence of this problem is that a learner repeatedly tries different options and observes their outcomes to learn which strategy is the best and, therefore, which one to choose. Finding the best strategy requires a delicate balance between exploiting the strategy known to yield the best payoff and exploring alternative strategies that may offer superior benefits. In our study, the options consisted of four strategies that skiers could employ to improve their race times on flat slopes in slalom, grounded in physics-based coaching manuals for alpine ski racing \cite{lemaster_skiers_1999, lemaster_ultimate_2010, lind_physics_2013}, biomechanical research on elite skiers \cite{reid_kinematic_2010, tjorhom_beskrivelse_2007, reid_alpine_2020, magelssen_is_2022}  or common strategy used by coaches. The four strategies were named "stand against", "rock skis forward", "extend", and "extend with rock skis forward". (Figure 1\ref{fig:courseandstrategies}b for a strategy illustration and \ref{sup_strategies} for an extended explanation)

To study how instruction and feedback drive learning of strategy selection, we designed and allocated skiers to three treatment groups that allowed us to compared reinforcement learning with traditional supervised learning with a coach:

For the supervised (target skill) learning group, we provided the best possible training program by engaging highly experienced and meritorious coaches who explained to the skiers that the 'extend with rock skis forward' strategy was the most effective for skiing fast on flat terrain in slalom. They imparted this information, citing a previous doctoral project \cite{reid_kinematic_2010}, which revealed that the fastest slalom racers spent more time aft of the skis, emphasizing the involvement of a dynamic rocking movement. In addition, the coaches informed them that simulations of ski racers had yielded similar results \cite{mote_accelerations_1983, lind_physics_2013}. The coach then guided the skiers to adopt this strategy and provided feedback on its execution after each trial. Note that the coach had access to the skiers' times after each trial but were prohibited from sharing this information with the skiers.

In the supervised (free choice) learning group, skiers were assigned to two coaches recruited from the tested ski group. We recruited two coaches to minimize waiting time and thus enhance training efficiency. To balance the skiers' skill levels within the treatment group, we created new blocks from the ranked list from baseline testing and randomly assigned them to the coaches. We instructed the coaches to improve the skiers' racing times as much as possible using all four defined skiing strategies. During each trial, the coach would select a strategy for the skier, observe the skier during his trial and provide feedback on its execution afterward. Similar to the supervised (target skill) learning group, the coaches had access to the skiers' times after each trial but could not share this information with the skiers.

In contrast, the reinforcement learning group was not assigned any coach. Instead of having a coach deciding the skiing strategy for them, the skiers in this treatment group were told to choose a strategy for each trial by themselves to ski the course as fast as possible. To help the skiers choose and learn from their choice and skiing execution, this group could see their racing times immediately after they crossed the finish line. Although this group had no coach, we assigned a person to communicate with the skiers to record their choices and encourage them to try skiing fast, also to equalize out some of the motivational effect of having a coach in the other treatment groups. 

\subsection{Procedures}\label{subsec4}
Figure \ref{fig:experiment}a illustrates the procedures employed in the study. 
In the baseline test, skiers began with two warm-up runs: one in free skiing warm-up course and one specific warm-up in the slalom course. During these warm-up runs, skiers were instructed, trained and verified on the start procedure and start technique by an instructor. As a first run in the baseline assessment, skiers completed a straight-gliding run, where they skied straight down from start to finish in a static, upright slalom posture. Subsequently, the skiers completed four runs in the slalom course. Skiers were encouraged to ski as quickly as they could, but they could not see their times, nor did they get any instruction on how to perform well. 

After the initial baseline assessment, the skiers took a 60-minute break. In the meantime, we allocated the skiers to the three treatment groups: reinforcement learning, supervised learning (target skill), and supervised learning (free choice). To allocate the skiers to groups, we deployed a randomized-blocked approach to account for pre-existing differences in the skiers’ performance levels \cite{maxwell_designing_2017}. Specifically, we computed each skier’s average performance across the four trials in the baseline and ranked them accordingly. We then created \textit{n} blocks with block sizes corresponding to our three treatments for the entire list of skiers and assigned these skiers to these predefined blocks. Finally, we randomly allocated the skiers to the different treatment groups within each block (Figure \ref{fig:experiment}b).

The treatment groups participated in sessions at different times to prevent treatment diffusion \cite{maxwell_designing_2017}. As the ski group comprised teams of skiers who know each other well and were residing together, we explicitly emphasized the importance of keeping information about the sessions private. To stay within the time frame at the ski hall, the two supervised learning groups underwent training together. To facilitate this, we arranged stations in the finish area with space and vision dividers and ample space to impede communication between coaches in the supervised learning groups. In addition, we developed a Python script that fetched racing times from the timing system, filtered the times for each coach, and transmitted it to the station where the coach was located, ensuring no information leak between coaches. The treatment group that initiated after the baseline session was randomized and counterbalanced across the group of ski teams we tested.

The first session after the treatment assignment involved a forced exploration. Here, skiers within the treatment group were gathered, and the session started by introducing them to the strategies. We explained that we had identified four strategies to enhance racing times on flats in slalom. Subsequently, each strategy was detailed, supported by illustrative drawings in \ref{fig:courseandstrategies}b Figure and corresponding word explanations as outlined in the \ref{subsec4}. To confirm comprehension, we conducted two short familiarization trials for each strategy, or until the execution met our performance standards. After reviewing the strategies with the participants, we gathered them in their respective treatment groups and asked them to rank the strategies online for what they believe are most to least suitable to improve performance in flat terrain slalom skiing. Throughout the instruction and ranking process, skiers were explicitly instructed not to discuss the strategies with each other. It is important to note that the same instructor was used for all treatment groups within a tested ski group. After this, the skiers conducted a total of eight trials on the course, with two trials for each strategy. During these trials, the reinforcement learning group got feedback from the timing whereas the coach gave feedback in the supervised learning. After completing the eight rounds, the skiers re-evaluated and ranked the strategies.

On the second day, skiers completed two free-choice sessions, each comprising a total of 6 trials in the same slalom courses that were used the baseline testing the day before. Prior to the 6 free - choice trials, the skiers performed one warm-up free skiing run and one warm-up run in the slalom course. In these sessions, supervised (target skill) learning consistently selected the theoretically best skiing strategy. Conversely, in supervised (free choice) and reinforcement learning, the coach and the skiers, respectively, had the autonomy to choose skiing strategy for each run. After each session, coaches (except supervised target skill) and skiers were asked to re-evaluate and rank the strategies.

On the third and last day, the skiers performed a retention and a transfer test to assess the effect of the training approaches on learning and performance. The retention test was performed in the same course as the baseline and acquisition sessions, whereas the transfer test was performed in the transfer course and involved a progression in gate offset from start to finish. Since the transfer test was a new course, we allowed the skiers to inspect the course before the test. The retention and transfer tests were conducted with all treatment groups together. None of the treatment groups received any feedback from coaches nor time during these tests. After each test, the skiers were asked to rank the strategies. 

\subsection{Analysis}
The data were cleaned with custom functions built on tidyverse \cite{wickham_welcome_2019} packages in R \cite{r_core_team_r_2022}. After this process, we validated the data by performing trial counting and visual inspection of the race time to screen for errors. An extensive report of this cleaning and validation process can be found at OSF  (\url{https://osf.io/2jxgk}).

Due to the hierarchical structure of the data, our general statistical strategy relies on multilevel modeling. At the first level, each skier performed multiple trials during each session. At the second level, each skier was nested within groups of ski teams that performed the experiment together. To account for these multilevel data structures, we leveraged linear mixed-effects models. To model random effects, we adopted a design-driven approach \cite{barr_random_2013, barr_learning_2021}, where we sought to account for all nonindependence introduced by repeated sampling from the same ski group and skier. We deployed classical frequentist statistics and fitted these models with the lme4 package \cite{bates_fitting_2015} in the R programming language. We used a simple coding scheme for our predictors where the intercepts represent the estimated mean of the cell means and the contrasts represent the estimated difference with respect to the reference level, which we set for reinforcement learning. Two-tailed p values and degrees of freedom for each model were derived using the lmerTest package \cite{kuznetsova_lmertest_2017} via the Satterthwaite approximation method. Alpha was set to 0.05 for all test statistics.

 \subsubsection{Race time}

Race time was analyzed using linear mixed-effect regression models. Initially, we planned to normalize the racing time by expressing the racing time as the difference from the straight-gliding time performed at the beginning of every session. This difference better approximate the skiers' actual skill improvement by considering the variance in snow conditions. However, practical considerations led us to deviate from this approach. This change was necessary because we had to flip or shift the course after each day to ensure snow conditions with the least damage. Unfortunately, these adjustments made maintaining a clean, straight-gliding lane difficult since the straight gliding lane crossed many areas with damage to the snow surface (holes) from the previous course set (see \ref{sup_coursesetting} for an image of these holes). Collisions with these holes affected the race time, adding noise to the results. Therefore, we used a more conservative approach and analyzed the raw racing times instead of analyzing the normalized racing times.

For the acquisition session, we modeled race time using Session and Treatment, and their interactions, as predictors. For retention and transfer, we modeled racing times at these sessions, with treatment added as a predictor. In addition, we used the average performance for each skier on the baseline test as a predictor to improve estimate precision and adjust for group differences at baseline testing. 

To model the effect and development of the strategies we broke the analysis up into different sub-models. One analysis focused on differences in the strategies and groups regarding Forced Exploration, where all participants had completed all strategies. Another analysis examined the transition from Forced Exploration to retention for both supervised (free choice) and reinforcement learning. The final model investigated the development between groups specifically for the "extend with rock skis forward" strategy. Session was coded as a continuous variable in all models. 


\subsubsection{Strategy choices}

Strategy choices were analyzed using generalized linear mixed-effect regression models with a binomial logit-link function. To model the selection of the theoretical best strategy, we inputted the data as logistic, where for each trial (\(i\)) per skier (\(j\)) within ski group (\(k\)), we counted \(y_{ijk}=1\) when the skier chose the theoretical best strategy (that is, 'extend with rock skis forward') or 0 when they did not. We included Treatment and Session and their interaction as two variables. To account for the nonindependence of the data structure, we allowed the intercept to vary by including a random intercept for the skier and ski groups.

We adopted the same model formula to model the selection of the estimated best strategy. This time, however, we counted \(y_{ijk}=1\) when the skier chose their estimated best strategy and 0 when they did not select that strategy. To estimate the best strategy for each skier in the sample, we used the sample-average method \cite{sutton_reinforcement_2018} to average the race time for each strategy and selected the strategy with the lowest estimated (that is, best) value. The sessions that we used to form this average were Forced Exploration, Free Choice 1, and Free Choice 2.  Due to the scaling issues with generalized linear models, we followed the recommendation to determine the size and significance of the effects of interest using marginal effects on the probability scale \cite{mize_best_2019, mccabe_interpreting_2022}. Interactions were assessed using discrete difference (also second difference), which is also in line with these recommendations. To derive these estimates, we used the emmeans package \cite{lenth_emmeans_2023}.

\subsubsection{Sensitivity to feedback}

To learn how skiers and coaches used feedback to guide their choices, we constructed a 'win-stay, lose-switch' model (WSLS; \cite{nowak_strategy_1993, worthy_comparison_2014, iyer_probing_2020}). For this WSLS analysis, we z-scored the race times for each skier for Free Choices 1 and 2 and counted \(y_{ijk}=1\) when the skier repeated the previous strategy and 0 when it was not. The data were modeled using a generalized linear mixed-effect regression model with a binomial logit-link function, with Treatment and z-transformed Race Time and their interaction as the two variables. To test for differences in error sensitivity, we used the marginal effects at the mean (MEM) derived from the emmeans package \cite{lenth_emmeans_2023}.


\subsubsection{Evaluations of the strategies}
To analyze the strategies' rankings, we used single-level linear regression owing to the singularity of our multilevel models. In this model, we inputted Session as a continuous variable and Treatment as the predictors. For supervised (free choice) learning, we used the coaches' rankings during the sessions where they selected strategies, and we used the skiers' rankings when they selected strategies during the retention and transfer tests .



\section{Results}


\begin{sidewaystable}\label{descriptive_skier}
\caption{\textbf{Skier characteristics}}
\centering
\begin{tabular}[H]{l|c|c|c|c|c|c}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{\textbf{Reinforcement learning}} & \multicolumn{2}{c|}{\textbf{Supervised (free choice)}} & \multicolumn{2}{c}{\textbf{Supervised (target skill)}} \\
\textbf{Sex} & \textbf{F}, N = 13 & \textbf{M}, N = 19 & \textbf{F}, N = 14 & \textbf{M}, N = 19 & \textbf{F}, N = 13 & \textbf{M}, N = 20\\
\hline
Age & 18.4 (2.3) & 17.7 (1.8) & 18.1 (2.3) & 18.3 (1.8) & 17.8 (2.4) & 18.2 (2.1)\\
\hline
Training group &  &  &  &  &  & \\
\hline
\hspace{1em}National team & 1 (7.7\%) & 1 (5.3\%) & 2 (14\%) & 1 (5.3\%) & 2 (15\%) & 4 (20\%)\\
\hline
\hspace{1em}Senior team & 3 (23\%) & 5 (26\%) & 1 (7.1\%) & 5 (26\%) & 0 (0\%) & 5 (25\%)\\
\hline
\hspace{1em}Ski academy & 9 (69\%) & 13 (68\%) & 11 (79\%) & 13 (68\%) & 11 (85\%) & 11 (55\%)\\
\hline
FIS points &  &  &  &  &  & \\
\hline
\hspace{1em}Median (IQR) & 54 (42, 80) & 46 (34, 90) & 58 (26, 80) & 44 (35, 61) & 49 (28, 66) & 31 (28, 63)\\
\hline
\hspace{1em}Not recorded & 3 & 2 & 1 & 2 & 2 & \vphantom{1} 3\\
\hline
World ranking &  &  &  &  &  & \\
\hline
\hspace{1em}Median (IQR) & 630 (394, 1,217) & 707 (364, 2,317) & 709 (133, 1,224) & 662 (387, 1,274) & 527 (145, 882) & 314 (220, 1,360)\\
\hline
\hspace{1em}Not recorded & 3 & 2 & 1 & 2 & 2 & 3\\
\hline
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} Mean (SD); n (\%)}\\
\end{tabular}
\end{sidewaystable}


\begin{sidewaystable}\label{descriptive_coach}
\caption{\textbf{Coach characteristics}}
\centering
\begin{tabular}[H]{l|c|c|c}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{\textbf{Supervised (free choice)}} & \multicolumn{1}{c}{\textbf{Supervised (target skill)}} \\
\textbf{Characteristic} & \textbf{F}, N = 2 & \textbf{M}, N = 6 & \textbf{M}, N = 3\\
\hline
Age & 38.5 (3.5) & 44.3 (8.8) & 48.0 (7.0)\\
\hline
Ski education (highest achieved) &  &  & \\
\hline
\hspace{1em}Level 2 & 0 (0\%) & 1 (17\%) & \\
\hline
\hspace{1em}Level 3/4 & 2 (100\%) & 5 (83\%) & 3 (100\%)\\
\hline
Sport science degree (highest achieved) &  &  & \\
\hline
\hspace{1em}MSc & 1 (50\%) & 0 (0\%) & 1 (33\%)\\
\hline
\hspace{1em}BSc & 1 (50\%) & 1 (17\%) & 1 (33\%)\\
\hline
\hspace{1em}No & 0 (0\%) & 4 (67\%) & \\
\hline
\hspace{1em}One-year program & 0 (0\%) & 1 (17\%) & 1 (33\%)\\
\hline
Coaching experience (years) &  &  & \\
\hline
\hspace{1em}National team (WC/EC)/Senior teams & 5.00 (1.41) & 5.00 (4.24) & 15.67 (1.15)\\
\hline
\hspace{1em}Ski academy & 7.5 (3.5) & 7.5 (6.5) & 2.00 (3.46)\\
\hline
\hspace{1em}Ski club & 2.5 (3.5) & 6.5 (8.7) & 6.0 (6.6)\\
\hline
\multicolumn{4}{l}{\rule{0pt}{1em}\textsuperscript{1} Mean (SD); n (\%)}\\
\end{tabular}
\end{sidewaystable} 

\subsection{Greater improvement during acquisition with reinforcement learning than supervised (free choice) learning}\label{result_racetime_acquisition}
We anticipated that the three treatment groups would show distinct improvements in race times during acquisition. Specifically, our hypothesis posited that reinforcement learning would yield greater race time improvements, driven by the acquisition of superior strategies through evaluations rather than instructions. Figure \ref{fig: racetime}a presents mean racing time estimates for treatment groups during acquisition.

In forced exploration, where skiers executed all strategies, the average race times did not significantly differ between reinforcement learning and supervised (free choice) learning ($\beta$ = 0.06 , 95\% CI [-0.2, 0.32], $t$(92.727) = 0.48, $p$ = 0.631) and supervised (target skill) learning ($\beta$ = 0.18, 95\% CI[-0.08, 0.44], $t$(92.663) = 1.37, $p$ = 0.174). 

When skiers entered the free choice 1, the first session where they or their coach had the autonomy to select strategies, notable variations in average race time improvements became evident. First, all treatments significantly improved their race times from forced exploration to this session (Reinforcement learning: $\beta$ = -0.38, 95\% CI[-0.45, -0.31], $t$(91.632) = -10.82, $p$ $<$ 0.001; Supervised (free choice): $\beta$ = -0.3, 95\% CI[-0.37, -0.23], $t$(93.472) = -8.96, $p$ $<$ 0.001; Supervised (target skill): $\beta$ = -0.5, 95\% CI[-0.57, -0.44], $t$(91.95) = -15.08, $p$ $<$ 0.001). However, this improvement was significantly larger for supervised (target skill) learning, with coaches selecting the best theoretical strategy than reinforcement learning ($\beta$ = -0.12, 95\% CI[-0.22, -0.03], $t$(91.777) = -2.58, $p$ = 0.012). In contrast, supervised (free choice) learning, where coaches made strategy choices for the skiers, improved less from forced exploration to Free Choice 1, but this was not statistically significant ($\beta$ = 0.08, 95\% CI[-0.02, 0.17], $t$(92.5) = 1.61, $p$ = 0.110).

Significant improvement from forced exploration persisted for all groups in Free Choice 2 (Reinforcement learning: $\beta$ = -0.45, 95\% CI[-0.54, -0.36], $t$(95.164) = -9.99, $p$ < 0.001; Supervised (free choice): $\beta$ = -0.31, 95\% CI[-0.39, -0.22], $t$(96.389) = -7.17, $p$ < 0.001; Supervised (target skill): $\beta$ = -0.43, 95\% CI[-0.52, -0.35], $t$(96.196) = -10.07, $p$ < 0.001). The average race time for the reinforcement learning group continued to drop, however, but plateaued for supervised (free choice) learning, resulting in a significant difference in change between the groups ($\beta$ = 0.14, 95\% CI[0.02, 0.26], $t$(95.743) = 2.26, $p$ = 0.026). As supervised (target skill) learning' race times declined from free choice 1 to 2, their initial greater race time improvement attenuated resulting in a non-significant interaction effect ($\beta$ = 0.02, 95\% CI[-0.11, 0.14], $t$(95.651) = 0.26, $p$ = 0.798).

However, we did not find statistical evidence that reinforcement learning performed better than supervised (free choice) or (target skill) learning at Free Choice 1 or 2 (Supplementary Table \ref{suptable_racetime_groupdiffeachsession}).
Collectively, these findings indicate that the treatment groups exhibited distinct improvements during acquisition.


\begin{figure}[H]
\centering
\includegraphics{figures/figure_racingtimes_2.pdf}
\caption{Race time across the different sessions for the three treatment groups. \textbf{a}. Displays the race time estimated from the model during acquisition. Forced exploration refers to the sessions wherein skiers tried all strategies, whereas free choice refers to the session wherein skiers or coaches selected strategies according to their assigned treatment groups. \textbf{b.} Displays the estimated race time from the model for retention. \textbf{c.} Displays the estimated race time from the model for transfer. Intervals represent the 95\% confidence interval derived from the models. Asterisks (*) indicate a statistically significant interaction effect. Each light gray point represents a singular trial by a skier, thus showing the full multilevel structure of the dataset}
\label{fig: racetime}
\end{figure}




\subsection{Better retention with reinforcement learning than supervised (free choice) but not supervised (target skill) learning} \label{result_racetime_retentipon}
We found evidence that reinforcement learning improved more during initial learning than supervised (free choice) learning. During the learning phase, however, it was the coach who chose the strategy for the performer. The question is what happens when skier chose the strategy themselves after an overnight sleep. We hypothesized that skill retention was better for reinforcement learning after 24 hours compared to supervised learning. Figure \ref{fig: racetime}b presents the mean race time estimates during retention.

The race times for the reinforcement learning group were on average significantly better than those for supervised (free choice) learning,when controlling for baseline differences ($\beta$ = 0.12, 95\% CI[0.01, 0.24], $t$(101.422) = 2.12, $p$ = 0.037). The difference between reinforcement learning and supervised learning (target skill) also favored reinforcement learning but was smaller and not statistically significant ($\beta$ = 0.07, 95\% CI[ -0.04, 0.19], $t$(101.63) = 1.27, $p$ = 0.206). 

\subsection{No convincing evidence for better transfer for reinforcement learning compared to supervised learning} \label{result_racetime_transfer}
Another key question concerns the knowledge transfers to new situations. We also hypothesized that reinforcement learning would improve skill transfer to a new slalom course. Figure \ref{fig: racetime}c presents the mean race time estimates during retention.

As for retention, the race time on the transfer course was on average better in reinforcement learning than in supervised (free choice), yet the difference was smaller and not statistically significant when controlling for baseline differences ($\beta$ = 0.1, 95\% CI[-0.02, 0.21], $t$( 99.979) = 1.7, $p$  = 0.091). The race times for reinforcement learning and supervised (target skill) learning was on average identical when controlling for baseline differences ($\beta$ = 0, 95\% CI[-0.12, 0.11], $t$(100.033) = -0.04, $p$ = 0.967). Thus, we did not find convincing evidence for improved transfer.


\subsection{No evidence that reinforcement learning chose the theoretical best strategy more often than supervised (free choice) learning}\label{subsubsec3}
We found that reinforcement learning accelerated the learning process during acquisition and improved skill retention compared to supervised (free choice) learning. Conversely, we failed to find convincing evidence of outperforming supervised (target skill) learning during acquisition or retention. We hypothesized that this behavior was caused by variations in strategy selection during acquisition, retention, and transfer, with reinforcement learning showing better learning to choose better strategies than supervised (free choice) learning. Figure \ref{fig: choice_descriptives} displays the percentage selections of the four strategies across sessions.

\begin{figure}[H]
\centering
\includegraphics{figures/figure_choice_descriptivecount_4.pdf}
\caption{Strategy choices for skiers in each session, with bars indicating percentages and the numbers to the right of the bars showing the total count for
each strategy. Data has been aggregated from our multi-level structure for clarity}\label{fig: choice_descriptives}
\end{figure}


We first tested whether reinforcement learning was more likely to choose the theoretical best strategy. The figure \ref{fig: choice_estimated} displays the predicted probabilities for the treatment groups across the sessions. 

In Free Choice 1, neither reinforcement learning (0.43) nor supervised (free choice) learning (0.42) preferred the theoretically optimal strategy. This predicted probability difference was not significant ($\beta$ = 0.01, 95\% CI[-0.24, 0.27], $z$ = 0.11, $p$ = 0.912). 

Both groups remained conservative in their strategy selection during Free choice 2. The reinforcement learning group increased its predicted probability marginally from 0.43 in Free choice 1 to 0.45 in Free Choice 2, with no statistical significance ($\beta$ = 0.02, 95\% CI[-0.11, 0.14], $z$ = 0.26, $p$ = 0.791). Similarly, the supervised (free choice) learning group saw a slight increase from 0.43 in Free choice 1 to 0.5 in Free Choice 2, and this change was also not statistically significant ($\beta$ = 0.08, 95\% CI[-0.04, 0.21], $z$ = 1.32, $p$ = 0.188). The difference in change between groups was not statistically significant ($\beta$ = 0.07, 95\% CI[-0.11, 0.24], $z$ = 0.74, $p$ = 0.458) nor was the group difference at Free Choice 2 ($\beta$ = -0.05, 95\% CI[-0.31, 0.2], $z$ = -0.4, $p$ = 0.693). 

A noticeable trend emerged in retention, with skiers in supervised (free choice) learning also having the autonomy to independently select strategies, free from the coach's guidance. Here, supervised (free choice) learning significantly increased its predicted probability of selecting the theoretical best strategy, rising from 0.43 in Free Choice 2 to 0.45 in Retention ($\beta$ = 0.23, 95\% CI[0.09, 0.36], $z$ = 3.31, $p$ < 0.001). In the same transition, the change from 0.45 to 0.5 for the reinforcement learning group was not statistically significant ($\beta$ = 0.05, 95\% CI [-0.09, 0.19], $z$ = 0.72, $p$ = 0.469). The difference in change between the groups was not statistically significant ($\beta$ = 0.17, 95\% CI[-0.02,  0.37], $z$ = 1.74, $p$ = 0.081), nor was their difference at retention ($\beta$ = -0.23, 95\% CI[-0.48, 0.03], $z$ = -1.75, $p$ = 0.079).

The predicted probability of choosing the theoretical best strategy further increased for both groups in Transfer, but this increase was not statistically significant for reinforcement learning ($\beta$ = 0.11 , 95\% CI [ -0.04 ,  0.27 ], $z$ = 1.42 , $p$  =  0.155) or supervised (free choice) learning ($\beta$ = 0.05, 95\% CI[-0.07, 0.18], $z$ = 0.85, $p$ = 0.396). Neither their difference in change from retention ($\beta$ = -0.06, 95\% CI[-0.25, 0.14], $z$ = -0.58, $p$  =  0.564), nor the differences between the groups on transfer were significant ($\beta$ = -0.17, 95\% CI [-0.4, 0.07], $z$ = -1.41, $p$ = 0.159).


\begin{figure}[H]
\centering
\includegraphics{figures/figure_choice_estimated_4.pdf}
\caption{Strategy selection for reinforcement learning (red) and supervised (free choice) learning (blue) during acquisition. \textbf{a.} Displays the estimated probability of choosing the theoretically best strategy (that is, extend with rocking skis forward) for both reinforcement learning and supervised (free choice) learning. \textbf{b.} Presents the estimated probability of selecting the empirically determined best strategy (that is, estimated using the sampling average method for each skier in the dataset). Intervals represent the 95\% confidence interval derived from the models. Asterisks (*) indicate a statistically significant interaction effect.}\label{fig: choice_estimated}
\end{figure}

\subsection{A higher proportion of skiers in the supervised (free choice) learning had the theoretical best strategy as their estimated best}\label{subsubsec3}
We did not find evidence that the predicted probability of choosing the theoretical best strategy was higher on average among skiers in the reinforcement learning group compared to those in the supervised (free choice) learning group. In contrast, it was the supervised (free choice) learning group that increased their selection of this strategy. As a follow-up analysis, we investigated whether a higher proportion of skiers in the supervised (free choice) learning group had the theoretically best strategy as their estimated best strategy. Such a trend could suggest that the feedback from coaches during the learning attempts may have made this strategy more effective. 

Figure \ref{fig: choice_estimated}b shows the proportion of skiers for each group that had the theoretical best strategy as their estimated best strategy. In this analysis, we found that 40\% of skiers in reinforcement learning had the theoretically best strategy as their estimated best strategy compared to 75\% in the supervised (free choice) group. A chi-square test revealed a statistical significant association between groups $\chi^2$ = 6.42, $p$ = 0.01). Because all skiers in the supervised (target skill) group had the theoretical best as their estimated best strategy, we did not include this group in this test.  


\subsection{Reinforcement learning increased their choices ??chose their best strategy more often than supervised (free choice) learning}\label{subsubsec3}
The observation that skiers in supervised (free choice) learning increasingly opted for the theoretically best strategy more across sessions than those in reinforcement learning does not necessarily imply a more frequent selection of their estimated best strategy. Consequently, we investigated whether reinforcement learning resulted in a higher predicted probability of selecting the estimated best strategy. Figure \ref{fig: choice_estimated}c displays the predicted probabilities of choosing the estimated best strategy for the treatment groups across the sessions. 

In free choice 1, the predicted probability difference between reinforcement learning (0.58) and supervised (free choice) learning (0.57) was small and not significant ($\beta$ = 0.01, 95\% CI [-0.22, 0.24], $z$ = 0.12, $p$ = 0.904). 

In free choice 2, the predicted probability of choosing the estimated best strategy significantly increased for the reinforcement learning group ($\beta$ = 0.2, 95\% CI [0.09, 0.32], $z$ = 3.39, $p$  $<$ 0.001), but not for the supervised (free choice) learning group ($\beta$ = 0.09, 95\% CI [-0.03, 0.21], $z$ = 1.48, $p$ = 0.140). Their difference in change was not significant, however ($\beta$ = -0.11, 95\% CI [-0.28, 0.05], $z$ = -1.33, $p$ = 0.184), nor was their predicted probability difference at free choice 3 ($\beta$ = 0.13, 95\% CI [-0.06, 0.32 ], $z$ = 1.3 , $p$  =  0.194).

When skiers in the supervised (free choice) learning group were given the freedom to choose strategies on retention, their predicted probabilities of choosing their estimated best strategy also increased, although this increase was not statistically significant ($\beta$ = 0.12, 95\% CI [0, 0.24], $z$ = 1.88, $p$ = 0.060). This was in contrast to the reinforcement learning group, where the predicted probability decreased by 0.07 from free choice 2 to retention, but this also was not statistically significant ($\beta$ = -0.07, 95\% CI [-0.19, 0.04], $z$ = -1.25, $p$ = 0.213). However, their difference in change was statistically significant ($\beta$ = 0.19, 95\% CI [0.02, 0.36], $z$ = 2.2, $p$ = 0.028). We did, however,  not find any statistically significant difference between the groups at retention ($\beta$ = -0.06, 95\% CI[-0.26, 0.14], $z$ = -0.61, $p$ = 0.544).

Neither reinforcement learning ($\beta$ = 0.08, 95\% CI[-0.04, 0.2], $z$ = 1.31, $p$ = 0.190) nor supervised (free choice) learning ($\beta$ = 0.02, 95\% CI[-0.09, 0.13], $z$ = 0.35, $p$ = 0.727) showed a significant increase in predicted probabilities from the retention phase to the transfer phase. This difference in change was not significant ($\beta$ = -0.06 , 95\% CI [-0.23, 0.1], $z$ = -0.73, $p$ = 0.467), nor was the predicted probability difference at transfer ($\beta$ = 0, 95\% CI [-0.17, 0.17], $z$ = 0, $p$ = 0.999).

\subsection{Reinforcement learning had a lower costs when performing suboptimal strategies compared to  reinforcement learning}\label{subsubsec3}
Choosing the estimated best strategy is one thing, but avoiding the selection of a strategy significantly worse than the estimated best strategy is another. In a follow-up analysis, we computed the expected difference between the skiers' chosen suboptimal strategy and their estimated best strategy, which we referred to as 'cost.' A lower cost implies that the performer has better grasped the effects of various strategies. This analysis revealed that reinforcement learning had lower costs than supervised (free choice) learning did  ($\beta$ = 0.06 , 95\% CI [ 0.01 ,  0.12 ], $t$( 30.789 ) = 2.55 , $p$  =  0.016), suggesting that they had better learned the strategies.

\subsection{Large win-stay, lose-switch signatures but no convincing evidence for difference between groups}\label{wsls}
To test the extent to which coaches in supervised (free choice) learning and skiers in reinforcement learning utilized trial feedback to make decisions, we conducted a 'win-stay, lose-switch' (WSLS) analysis. In this analysis, heightened sensitivity is indicated by a high predicted probability of repeating an action following good feedback and a low predicted probability following bad feedback on a previous trial.

Figure \ref{fig: choice_wsls} shows the predicted probability of repeating the strategy on the previous trial if the feedback was good. We found statistically significant estimated marginal effects at the mean (MEM) for both reinforcement learning  ($\beta$ = -0.18, 95\% CI[-0.26, -0.11], $z$ = -4.8,$p$ < 0.001) and supervised (free choice) learning ($\beta$ = -0.11 , 95\% CI [ -0.17 ,  -0.04 ], $z$ = -3.29 , $p$  <  0.001). These findings suggest that both groups had a higher predicted probability of repeating a strategy if the previous trial feedback was good. Despite the large descriptive difference in the marginal effect between groups, this difference was not statistically  significant ($\beta$ = -0.08, 95\% CI [-0.17, 0.02], $z$ = -1.55, $p$ $=$ 0.121).




\begin{figure}[H]
\centering
\includegraphics{figures/figure_winstaylooseshift.pdf}
\caption{Win-stay, lose-switch comparison between reinforcement learning and supervised (free choice) learning. The line shows the predicted probability of repeating the previously chosen strategy based on its trial feedback, along with a 95\%CI in the ribbon. In this model, higher or lower probabilities with better and worse feedback mean greater sensitivity to feedback}\label{fig: choice_wsls}
\end{figure}



\subsection{Larger cognitive separation of strategies in the reinforcement and supervised (free choice) learning than in supervised (target skill) learning}\label{subsec4}
Directly reflecting their understanding of the effectiveness of strategies is how skiers or coaches evaluate them. We expected that these evaluations would diverge among the treatment groups and that reinforcement learning would have achieved a more consistent evaluation with their actual race times. Figure \ref{fig: rank}a displays the rankings for individuals with the authority  to choose strategies across all sessions, except for supervised (target skill) learning, where the skiers' rankings are shown because these coaches were informed ahead of the experiment about which strategies were expected to be the best. 

As shown in Figure \ref{fig: rank}a and in supplementary table \ref{suptable_strategyranking_strategydifffam}, all treatment groups initially ranked 'extend with rock skis forward' as the best, followed by 'extend,' 'rock skis forward,' and 'stand against' immediately after the strategies' introduction during the familiarization phase. We did not find statistically significant differences in ranking between reinforcement learning and the two supervised learning groups for the strategies (Supplementary Table \ref{suptable_strategyranking_diffgroupfam}), except that supervised (target skill) learning ranked 'extend' worse ($\beta$ = 0.45, 95\% CI[0.24,  0.67], $t$(1700) = 4.17, $p$ $<$ 0.001) and 'extend with rock skis forward' better  ($\beta$ = -0.42, 95\% CI[-0.63, -0.2], $t$(1700) = -3.83, $p$ $<$ 0.001) than reinforcement learning did.

The data depicted in the figure and the regression model reported in supplementary table \ref{suptable_strategyranking_strategiesslope} indicate that the positional ranks for the strategies "stand against" and "extend with rock skis forward" remained stable throughout all sessions across the treatment groups, yet there were some interesting trends. The rank of 'stand against' descriptively worsened over the sessions for all groups, but the change was only statistically significant for the supervised (target skill) learning group  ($\beta$ = 0.09 , 95\% CI [ 0.04 ,  0.13 ], $t$(1700) = 3.4 , $p$  $<$  0.001). In contrast, the 'extend with rock skis forward' strategy was ranked better over the sessions for all groups. However, the change was only statistically significant for the supervised (target skill) learning group ($\beta$ = -0.06 , 95\% CI[-0.11, -0.01], $t$(1700) = -2.51, $p$ = 0.012). 

The most interesting trend shifts were observed for the two middle-ranked strategies: 'extend' and 'rock skis forward'. The reinforcement learning group ranked the strategy 'extend' better and 'rock skis forward' worse over the sessions, and these changes were found to be statistically significant (Extend: $\beta$ = -0.1 , 95\% CI [ -0.15 ,  -0.05 ], $t$(1700) = -3.73 , $p$  <  0.001; Rock skis forward:  $\beta$ = 0.09 , 95\% CI [ 0.04 ,  0.15 ], $t$(1700) = 3.57 , $p$  <  0.001). Although supervised (target skill) and supervised (free choice) exhibited the same trend, the magnitude was smaller and did not reach statistical significance. We also identified a statistically significant interaction effect between reinforcement learning and supervised (target skill) learning for the 'rock skis forward' strategy ($\beta$ = -0.07, 95\% CI[-0.14 to 0], $t$(1700) = -1.97,$p$ = 0.049), providing evidence that reinforcement learning learned to separate strategies better over time over time than the supervised (target skill) learning did. We did, however, not find other interaction effects (Supplementary table \ref{suptable_strategyranking_diffindiff_strategiesslope})) 


\begin{figure}[H]
\centering
\includegraphics[]{figures/figure_ranking_average_3.pdf}
\caption{Strategy evaluations and effects. \textbf{a. }Average descriptive ranking of the four strategies per treatment group. Rankings range from 1 (best) to 4 (worst). For supervised (free choice) learning, the coach's ranking during the acquisition phase and the skier's ranking during the retention and transfer phases are plotted, reflecting the decision- maker for strategy selection. The circle represents the mean, and the ribbon indicates the standard deviation (SD). \textbf{b.} Average race time of the four strategies across the three treatment groups. The circle represents the mean, and the ribbon represents the SD. Note that all skiers tested the strategies during the forced exploration phase, but as the study progressed, there may have been fewer observations for some strategies. Consequently, the calculation of the mean might be heavily influenced by these observations. The mean race time was calculated by first determining each participant's average time for each strategy per session, followed by calculating the mean of these averages.}\label{fig: rank}
\end{figure}
\subsection{Performance strategies}\label{subsec5}
Similar to the variations in how skiers ranked the strategies, we also observed divergent race times achieved by the skiers during their execution. On average, skiers performed worst with the "stand against" strategy and notably improved their race time with the other strategies (Fig.\ref{fig: rank}b and Supplementary Table \ref{Supplementarytable_strategyeffect_5}). 

During forced exploration, there were no statistically significant differences between the reinforcement learning and supervised learning groups for any of the strategies:  "stand against", "rock skis forward", "extend", or "extend with rock skis forward" (Fig.\ref{fig: rank}b and Supplementary Table \ref{Supplementarytable_strategyeffect_6}). 

The race times for all strategies improved significantly from the forced exploration to retention for both reinforcement learning and supervised (free-choice) learning, except for the "stand-against" strategy, which was not statistically significant in the reinforcement learning group. This deviation may be attributed to the limited number of observations for this strategy within the group. Interestingly, we found that the reinforcement learning group improved more on the "extend" strategy from forced exploration to retention than the supervised (free choice) learning group ($\beta$ = 0.04 , 95\% CI[0.01, 0.07], $t$(1378.879) = 2.34, $p$ = 0.020). We did not find evidence for such an interaction effect for any of the other three strategies ( Supplementary Table \ref{Supplementarytable_strategyeffect_7}). 

As supervised (target skill) learning only trained on the "extend with rock skis forward" strategy during the free choice sessions and almost exclusively during retention, we conducted a separate analysis comparing all groups on this strategy. This analysis revealed that all groups improved on the "extend with rock skis forward" strategy  (Supplementary Table \ref{Supplementarytable_strategyeffect_8}). However, no statistically significant differences were found in skill improvement for this strategy between reinforcement learning and either supervised (free choice) learning ($\beta$ = 0, 95\% CI[-0.03, 0.02], $t$(1064.204) = -0.3, $p$ = 0.765) or supervised (target skill) learning ($\beta$ = 0.02, 95\% CI [-0.01, 0.04], $t$(1062.577) = 1.07, $p$ = 0.283).


\section{Discussion}

Learning to select effective strategies is a distinctive characteristic of expertise. Typically, these strategies come from a coach who imparts knowledge on which strategies to adopt. Here, we instead asked whether shifting the training strategy from direct instruction to evaluation can accelerate the development of expertise in skilled performers. To address this question, we developed four strategies with the potential to improve race time on flat sections in slalom and allocated ninety-eight skilled alpine ski racers to one of three treatment groups who learned the strategy choices in different ways. The skiers in the reinforcement learning group were tasked with finding the best of these strategies themselves using trial feedback for evaluation. The skiers in the supervised (free choice) learning group were assigned to a coach from the tested ski teams who selected a strategy for the skier and provided feedback on their execution. Finally, in the supervised (target skill) learning group, the skiers were assigned a current national team coach to coach in the 'extend with rock skis forward' skiing strategy — a technique we defined as the theoretically best strategy based on observations of elite skiers, simulations and theory. Overall, we found that reinforcement learning learned better than the supervised (free choice) learning group [usikker på om jeg skal ta med noe mer hva vi fant her faktisk?]

To begin, our findings that reinforcement learning improved more during acquisition and demonstrated greater retention than supervised (free choice) learning are in accordance with previous studies showing that reinforcement learning improves skill retention \cite{therrien_effective_2016, truong_error-based_2023, hasson_reinforcement_2015}. One suggested explanation for these findings is that reinforcement learning trains the slow-learning process that secures long-term learning \cite{huang_rethinking_2011}. The better race times in reinforcement learning also align with prior studies on the 'discovery learning' approach, where practice without explicit instruction yields better learning effects \cite{wulf_instructions_1997, hodges_learning_2001, hodges_role_1999}. We did, however, not find convincing evidence that reinforcement learning learned better than supervised (target skill) learning, although their race times were descriptively better during both acquisition and retention. The selection of strategy therefore appears to have played an important role, but we will offer a second account later in this discussion.

Contrary to our expectation, we did not find convincing evidence for improved transfer in the reinforcement learning group. One explanation for this is that reinforcement learning only improves learning in situations where rewards have been previously received \cite{robertson_memory_2018}, and aligns with \cite{hasson_reinforcement_2015} who found better retention but not transfer with reinforcement learning compared with supervised learning. It it possible that a more structured learning approach, where learners are exposed to frequent switches between situations, is necessary to grasp the task's structure and promote transfer \cite{braun_structure_2010}. Future research should possibly investigate the effect of structural learning. 

Our work sought to explain these differences in race times through the choice of strategies. We did not, however,  find convincing evidence that the reinforcement learning group selected the theoretical best or the individual skiers' estimated best strategy more often than the supervised (free choice) learning group. The choices were also characterized by a clear 'win-stay, lose-switch' signature in both groups, where the chooser opted to stick with choices that yielded good outcomes. However, despite significant descriptive differences favoring reinforcement learning, this pattern was not statistically significant either. An obvious explanation for this was the high coaching experience the coaches in the study possessed, as well as their access to the outcomes and undergoing considerable learning themselves. 



Despite the lack of convincing evidence that reinforcement learning selected the best strategy more often than supervised (free-choice) learning did, reinforcement learning had a lower expected cost (or regret) for skiers whose chosen strategies were suboptimal during retention. One explanation is that skiers in reinforcement learning gained better insight into the effectiveness of the strategies by objectively evaluating them during acquisition, which enabled them to choose strategies that did not adversely affect their times too much. This account cannot fully explain the group differences in race time, however. Reinforcement learning also improved more on the 'extend' strategy over the course of the sessions than supervised (free choice) learning. Given that this group was not assigned a coach to help them improve, it is likely that the reinforcement feedback increased motor vigor \cite{shadmehr_vigor_2020, pietro_mazzoni_why_2007, niv_normative_2006} when performing that strategy. Indeed, previous studies have shown that people make saccades \cite{takikawa_modulation_2002} and reach faster \cite{summerside_vigor_2018} toward targets paired with rewards than unpaired targets. Comments from a few coaches, who watched the retention and transfer from the sideline, mentioned that skiers in the reinforcement learning group used more forceful arm movements than skiers in the other groups, although the instructions did not explicitly tell them to do that.

The vigor perspective may also help explaining why reinforcement learning did not learn better than the supervised (target skill) learning group, as previous studies also have found that training with explicit knowledge boosts motor vigor much like the effect of reward itself \cite{anderson_rewards_2020, wong_explicit_2015}. It may therefore be that the getting information from a current national team coach that one strategy was best boosted the implicit motivation to perform this strategy well. 

A surprising and interesting discovery was that supervised (target skill) learning, in contrast to the reinforcement learning group, did not learn to cognitively dissociate the 'extend' and 'rock skis forward' strategies, despite big race time differences. This finding suggests that learners may miss potential learning by only being exposed to one strategy instead of a broader exploration of alternatives. Over time, such exploration could prove crucial in developing innovative strategies, as athletes cultivate a deeper comprehension of the relationship between their actions and performance outcomes \cite{ericsson_scientific_1998}. Future studies should investigate this learning further. 

The insights learned from our study suggest important implications for coaches when designing training sessions to improve skills. Based on our findings, coaches are advised to formulate strategies tailored to their respective sports and to aid learners in impartially evaluating these strategies. This pedagogical approach aligns with previous recommendations emphasizing the importance of fostering learners' cognitive representations to develop innovative solutions rather than merely imparting knowledge from a coach \cite{ericsson_scientific_1998}. These strategies may start out broadly for young athletes but should progressively become more focused as athletes advance in expertise. It is essential to clarify that we do not propose replacing traditional teaching methods with this approach but suggest integrating it as a supplementary tool to augment decision-making training.

Before practitioners embrace our recommendation to incorporate more strategy evaluation into their coaching practices, it is important to consider the practical significance of the effect size and its potential amplifying and counteracting mechanisms\cite{anvari_not_2023}. Notably, the estimated effect size during retention was smaller than our predefined smallest effect size of interest. This benchmark, however, was set for a longer slalom course and more training sessions than we could execute due to space and time constraints in the ski hall. Consequently, we exercise caution in outright dismissing its practical significance. However, our estimated effect size might be meaningful if we consider that our slalom course approximately equals one-third of a full slalom race course and that a slalom race consists of two runs. Therefore, the 0.12-second effect size could be scaled up by a factor of 6, but it is more realistic to assume that flat sections of a course constitute only one-third of the entire course. On this basis, the 0.12-second difference translates into an improved FIS world ranking of 27 positions for females and 65 for males, based on a median ranking of 600 in our sample (see Supplement Discussion \ref{supdiscussion}). This effect could be important for coaches, but we must remember that sports expertise involves cognitive decisions  \cite{mangalam_investigating_2023, krakauer_motor_2019}, such as switching from one strategy to another during a race. Our study did not capture such decisions because we focused on flat sections, only. Finally, the estimated effects would possibly been larger if the skiers in the reinforcement learning and supervised had performed the retention and transfer test in separate sessions with no information leakage. When we tested the skiers simultaneously allowed the skiers to observe each other, possibly diluting some of the effect. However, this decision was made to mirror the conditions of alpine competitions and gave us confidence that athletes experienced similar conditions during testing.


\subsection{Limitation of the study}

One limitation of the study is that we did not include motion capture of the skiers performing the strategies. Therefore, we do not know precisely what the skiers did when executing the strategies, other than verifying that they were able to perform the strategy satisfactorily during the famiration phase. However, we followed the recommendation to make discrete strategies. 

\section{Conclusion}
In conclusion, our data showed that reinforcement learning performed better than the standard coaching approach, although this learning did not transfer to a new slalom course. Only by informing coaches about what we believed to be the best strategy for improving race time on flats on slalom were we able to produce learning effects similar to those of reinforcement learning. However, these race times were still descriptively slower than the reinforcement learning. Always picking the correct strategy had as its own cost, however. The learner may not learn the causal structure supporting their performance, which might prevent them from developing truly intelligent solutions \cite{ericsson_scientific_1998}. Our findings corroborate previous research showing that reinforcement learning can be an important learning mechanism \cite{hasson_reinforcement_2015} and that it can be an important training strategy for training skilled performers \cite{lohse_errors_2019}.



\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}

Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

Please refer to Journal-level guidance for any specific requirements.

\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Description of strategies}\label{sup_strategies}
This supplementary describes each strategy in more detail. 

The "stand against" strategy emphasized maintaining a stable stance against external forces without body extension along the body's longitudinal axis or rocking skis forward. This term is frequently used by Norwegian ski coaches when communicating with skiers to help them improve their race times. 

The "rock skis forward" strategy involves rocking the ski forward during the turning phase. This action effectively regulates the distribution of pressure over the skis. During the initiation and control phases of a turn, the pressure is generally shifted forward to bend the ski's forebody, increasing friction with the snow and enabling it to turn more sharply. However, at some point, during turn progression, the skier aims to stop turning and therefore shifts the pressure further back on the skis, thereby reducing turning and braking forces \cite{lemaster_skiers_1999, lemaster_ultimate_2010}. Investigations of elite alpine ski racers have shown that high performing skiers tend to rock skis more forward and pressure the back part of the ski for considerable longer time during a turn, than slower skiers \cite{reid_kinematic_2010, tjorhom_beskrivelse_2007, reid_alpine_2020}. To make the information more specific for the skiers, we communicated, that the maximum range of the rocking movement was about 30-50 centimeters from gate passage to completion of the turn, which is in correspondence with biomechanical evidence of elite ski racers \cite{reid_kinematic_2010}. 

The "extend" strategy involves extending the body from a laterally tilted position during the turn, closer to the turn's center of rotation. This pushing motion leads to more proper handling of a reservoir of energy available when skiers lower their bodies in the switch between turns. When skiers extend their bodies, while being laterally inclined in the slalom turn, they create ground reaction force. The radial component of the ground reaction force moving the center of mass closer to the center of rotation increases velocity, in accord with the principle of mechanical energy conservation \cite{lind_physics_2013}. Simulation studies of individuals extending their body in rollers or during carved slalom turns have shown that this movement can increase speed \cite{mote_accelerations_1983,luginbuhl_identification_2023}, and the effect has been observed in various ways with elite skiers during training and competition \cite{reid_kinematic_2010, magelssen_is_2022, supej_differential_2008}. 

Finally, "extend with rocking skis forward" was expected to be the best strategy combining the two effects from extending and rocking skis forward, and we therefore defined this as the theoretical best strategy. Simulations of skiers extending their bodies in the bottom of a roller have observed an additional effect of rocking the skis forward \cite{mote_accelerations_1983}


\section{Snow preparation}\label{sup_snowprep}
We dedicated a substantial amount of time and effort to prepare the hill for our learning experiment. Our primary objective was to ensure that the snow conditions were as identical and fair as possible for all participants. To achieve this, we collaborated closely with the SNØ facility’s staff and the coaches of the Norwegian Alpine Ski team. Together, we devised a comprehensive plan to achieve consistency before each group of skiers took to the slopes. This report will detail the steps we took to ensure the snow was ready for our skiers and provide insight into the reasoning behind our choices. The purpose of this supplementary note is to assist you in evaluating our study and to transparently document our commitment to delivering the best possible conditions for our skiers.

\subsection*{Ski group A}
About two weeks before testing Group A, new snow was created on the racing hill. The evening before data collection, machines groomed the racing hill (Fig. \ref{fig:snowprep}a:left). We then watered the hill and left it to freeze overnight to create a hard and firm surface (Fig. \ref{fig:snowprep}a:right). 

\subsection*{Ski group B}
Ski group B began testing the day after ski group A completed their testing. Once group A finished, we inspected the race hill(Fig. \ref{fig:snowprep}b:left), noticed some small holes, and decided to fill them before watering it again (Fig. \ref{fig:snowprep}b:right).  

\subsection*{Ski group C}
After ski group B, the race hill started getting icy, lacking grip in some areas. We were concerned that watering it again might worsen conditions, making it too challenging for skiers. Consequently, we opted to gently groom the hill with a machine and let the grooves set for a couple of days. See figure \ref{fig:snowprep}c for the result of this process.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/figure_appendix_snowprep.jpg}
\caption{Images showing the hill preparation for the four ski groups. \textbf{a}. Shows the racing hill for ski group A. The left image displays the racing hill the evening before data collection after it was groomed. The right image shows the watering process for ski group A. \textbf{b}. Shows the racing hill for ski group B. The left image depicts the course inspection immediately after ski group A finished their testing. The right image shows the watering process for ski group B. Note that we did not groom the course this time, so in the image, you can see some uneven surfaces on the snow, which were evened out by watering it. \textbf{c}. Shows the racing hill for ski group C. The left image displays the hill after it was groomed and left overnight. The right image shows the same but from the bottom. \textbf{d}. Shows the racing hill for ski group D. The left image illustrates the hill for ski group C on their retention test. Note the icy surface, which was the reason why we produced new snow. The right image shows the watering process for ski group D. 
}
\label{fig:snowprep}
\end{figure}
 
\subsection*{Ski group D}
After Ski Group C, the race hill needed new snow because some areas had become icy, with minimal grip (Fig. ref{fig:snowprep}d:left)). Although the conditions were suitable for Ski Group C, they would not have worked for a new ski group undergoing testing. Consequently, we decided to produce new snow two days before Ski Group D started their training. This fresh snow was pushed into the racing hill the day before testing and groomed. Subsequently, we watered the hill and let it freeze overnight (Fig. \ref{fig:snowprep}d:right)



\section{Course setting}\label{sup_coursesetting}
We used a standard procedure to set the slalom courses, ensuring a fixed length and offset. First, we stretched a taut rope between two nails on either side of the ski hill. This rope helped us locate the exact starting line consistently from day to day. From the nail on the skier's right, we measured 6 meters into the slope. We did the same from a fixed point approximately 50 meters down the course, but here we measured 3.4 meters out. Then, we pulled a 50-meter-long measuring tape between the two points to establish the line down the hill   We chose a measuring tape over a rope because ropes tend to expand and contract when they get wet and dry, respectively (see Fig.\ref{fig:coursesetting}a for an image illustrating this process).

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/figure_appendix_coursesetting.jpg}
\caption{Images showing the procedure for course setting. \textbf{a}. The image illustrates the process of establishing the straight reference line down the hill. The long blue gate is positioned 6 meters from the nail on the skier's right. \textbf{b}. This image showcases the gate-setting method. A white rope is secured to the reference line with a carabiner hook, and a marker on the rope indicates a distance of 1.9 meters for placing the gate. \textbf{c}. The image reveals the tracks left behind by a group of skiers that was tested
}
\label{fig:coursesetting}
\end{figure}
 

Once the 50-meter straight line was established, we laid out a rope segment attached to a carabiner hooked onto the long rope. We moved this segment 1.9 meters out from the rope in one turn and 0 meters in the other, with a 10-meter vertical space. This ensured that the course followed a straight line down the slope and was set with the correct offset (see figure\ref{fig:coursesetting}b for an image illustrating this process). Once all the gates along the 50-meter measurement tape were set, we used a new fixation point down the slope and continued down the course. We practiced this procedure several times before the experiment, and the variation in course setting was a maximum of 10 cm at the end of the course.

Due to wear and tear on the trails, we opted to shift the course laterally or rotate it, depending on the situation. With the exception of one skiing group, we followed the following practice. On day 1, we set the course as described above. On day 2, we rotated the course so that the first turn went in the opposite direction of day 1. On the third day, we shifted the course closer to the wall on skiers' right (see figure \ref{fig:coursesetting}c for an image showing the tracks in the hill left after a skigroup had completed the experiment). 


\section{Tables}


\subsection{Supplementary Table}

\subsubsection{Race time}

\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}\label{suptable_racetime_groupdiffeachsession}
\caption*{
{\large Race time} \\ 
{\small Difference between groups at each session}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 16.62 & 16.62 & 16.27 to 16.97 & 132.68 &  $<$ 0.001 \\ 
session2 & -0.39 & -0.39 & -0.43 to -0.35 & -20.09 &  $<$  0.001 \\ 
session3 & -0.40 & -0.40 & -0.45 to -0.35 & -15.74 &  $<$  0.001 \\ 
sessionblock1 : treatment2 & 0.06 & 0.06 & -0.20 to 0.32 & 0.48 &  =  0.631 \\ 
sessionblock2 : treatment2 & 0.14 & 0.14 & -0.11 to 0.39 & 1.11 &  =  0.272 \\ 
sessionblock3 : treatment2 & 0.20 & 0.20 & -0.08 to 0.49 & 1.43 &  =  0.157 \\ 
sessionblock1 : treatment3 & 0.18 & 0.18 & -0.08 to 0.44 & 1.37 &  =  0.174 \\ 
sessionblock2 : treatment3 & 0.05 & 0.05 & -0.20 to 0.31 & 0.43 &  =  0.672 \\ 
sessionblock3 : treatment3 & 0.19 & 0.19 & -0.09 to 0.48 & 1.37 &  =  0.176 \\ 
sd\_\_(Intercept) & 0.51 & 0.51 & & & & \\ 
cor\_\_(Intercept).session2 & -0.03 & -0.03 & & & & \\ 
cor\_\_(Intercept).session3 & 0.23 & 0.23 & & & &  \\ 
sd\_\_session2 & 0.15 & 0.15 & & & &  \\ 
cor\_\_session2.session3 & 0.73 & 0.73 & & & & \\ 
sd\_\_session3 & 0.22 & 0.22 & & & & \\ 
sd\_\_(Intercept) & 0.23 & 0.23 & & & & \\ 
sd\_\_Observation & 0.21 & 0.21 & & & & \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: racetime \textasciitilde{} session / treatment + (1  \textbar{} skigroup) + (1 + session \textbar{} skigroup:skier)\\
\end{minipage}



\subsubsection{Strategy ranking}\label{suptable_strategyranking}
\clearpage

\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}\label{suptable_strategyranking_strategydifffam}
\caption*{
{\large Strategy ranking} \\ 
{\small Estimated difference between strategies during familiarization}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 2.50 & 2.50 & 2.44 to 2.56 & 85.64 &  $<$  0.001 \\ 
Supervised (free choice) & 0.00 & 0.00 & -0.16 to 0.16 & 0.00 &  =  1.000 \\ 
Supervised (target skill) & 0.00 & 0.00 & -0.11 to 0.11 & 0.00 &  =  1.000 \\ 
Ranktime & 0.00 & 0.00 & -0.02 to 0.02 & 0.00 &  =  1.000 \\ 
Reinforcement learning : b & -1.72 & -1.72 & -1.94 to -1.50 & -15.47 &  $<$  0.001 \\ 
Supervised (free choice) : b & -1.43 & -1.43 & -1.81 to -1.05 & -7.38 &  $<$  0.001 \\ 
Supervised (target skill) : b & -1.20 & -1.20 & -1.41 to -0.99 & -11.32 &  $<$  0.001 \\ 
Reinforcement learning : c & -1.05 & -1.05 & -1.27 to -0.83 & -9.42 &  $<$  0.001 \\ 
Supervised (free choice) : c & -0.70 & -0.70 & -1.08 to -0.32 & -3.60 &  $<$  0.001 \\ 
Supervised (target skill) : c & -0.95 & -0.95 & -1.16 to -0.74 & -8.94 &  $<$  0.001 \\ 
Reinforcement learning : d & -2.06 & -2.06 & -2.28 to -1.84 & -18.49 &  $<$  0.001 \\ 
Supervised (free choice) : d & -2.09 & -2.09 & -2.47 to -1.71 & -10.77 &  $<$  0.001 \\ 
Supervised (target skill) : d & -2.41 & -2.41 & -2.61 to -2.20 & -22.67 &  $<$  0.001 \\ 
Supervised (free choice) : Ranktime & 0.00 & 0.00 & -0.04 to 0.04 & 0.00 &  =  1.000 \\ 
Supervised (target skill) : Ranktime & 0.00 & 0.00 & -0.04 to 0.04 & 0.00 &  =  1.000 \\ 
Reinforcement learning : b : Ranktime & -0.15 & -0.15 & -0.22 to -0.07 & -3.95 &  $<$  0.001 \\ 
Supervised (free choice): b : Ranktime & -0.12 & -0.12 & -0.22 to -0.02 & -2.45 &  =  0.014 \\ 
Supervised (target skill) : b : Ranktime & -0.13 & -0.13 & -0.20 to -0.06 & -3.65 &  $<$  0.001 \\ 
Reinforcement learning : c : Ranktime & 0.05 & 0.05 & -0.03 to 0.12 & 1.21 &  =  0.225 \\ 
Supervised (free choice): c : Ranktime & -0.01 & -0.01 & -0.11 to 0.09 & -0.27 &  =  0.791 \\ 
Supervised (target skill) : c : Ranktime & -0.06 & -0.06 & -0.13 to 0.01 & -1.78 &  =  0.076 \\ 
Reinforcement learning : d : Ranktime & -0.09 & -0.09 & -0.17 to -0.02 & -2.51 &  =  0.012 \\ 
Supervised (free choice) : d : Ranktime & -0.09 & -0.09 & -0.19 to 0.01 & -1.85 &  =  0.065 \\ 
Supervised (target skill) : d : Ranktime & -0.15 & -0.15 & -0.22 to -0.08 & -4.18 &  $<$  0.001 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lm(Rank \textasciitilde{} strategy/treatment * ranktime) \\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward". 
\end{minipage}



\clearpage
\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}\label{suptable_strategyranking_diffgroupfam}
\caption*{
{\large Strategy ranking} \\ 
{\small Estimated difference in difference for each strategy between groups}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 2.50 & 2.50 & 2.44 to 2.56 & 85.64 &  $<$ 0.001 \\ 
b & -1.45 & -1.45 & -1.61 to -1.29 & -17.58 &  $<$  0.001 \\ 
c & -0.90 & -0.90 & -1.06 to -0.74 & -10.88 &  $<$  0.001 \\ 
d & -2.18 & -2.18 & -2.35 to -2.02 & -26.46 &  $<$  0.001 \\ 
Ranktime & 0.00 & 0.00 & -0.02 to 0.02 & 0.00 &  =  1.000 \\ 
a : Supervised (free choice) & -0.15 & -0.15 & -0.46 to 0.16 & -0.96 &  =  0.340 \\ 
b : Supervised (free choice) & 0.14 & 0.14 & -0.17 to 0.45 & 0.87 &  =  0.385 \\ 
c : Supervised (free choice) & 0.20 & 0.20 & -0.11 to 0.51 & 1.26 &  =  0.209 \\ 
d : Supervised (free choice) & -0.19 & -0.19 & -0.50 to 0.13 & -1.17 &  =  0.242 \\ 
a : Supervised (target skill) & -0.07 & -0.07 & -0.28 to 0.15 & -0.63 &  =  0.531 \\ 
b : Supervised (target skill) & 0.45 & 0.45 & 0.24 to 0.67 & 4.17 &  $<$  0.001 \\ 
c : Supervised (target skill) & 0.03 & 0.03 & -0.18 to 0.24 & 0.29 &  =  0.772 \\ 
d : Supervised (target skill) & -0.42 & -0.42 & -0.63 to -0.20 & -3.83 &  $<$  0.001 \\ 
b : Ranktime & -0.13 & -0.13 & -0.18 to -0.09 & -5.56 &  $<$  0.001 \\ 
c : Ranktime & -0.01 & -0.01 & -0.06 to 0.04 & -0.43 &  =  0.665 \\ 
d : Ranktime & -0.11 & -0.11 & -0.16 to -0.06 & -4.65 &  $<$  0.001 \\ 
a : Supervised (free choice) : Ranktime & 0.01 & 0.01 & -0.08 to 0.10 & 0.19 &  =  0.847 \\ 
b : Supervised (free choice) : Ranktime & 0.03 & 0.03 & -0.05 to 0.12 & 0.73 &  =  0.465 \\ 
c : Supervised (free choice) : Ranktime & -0.05 & -0.05 & -0.14 to 0.04 & -1.13 &  =  0.259 \\ 
d : Supervised (free choice) : Ranktime & 0.01 & 0.01 & -0.08 to 0.10 & 0.21 &  =  0.837 \\ 
a : Supervised (target skill) : Ranktime & 0.04 & 0.04 & -0.03 to 0.11 & 1.00 &  =  0.317 \\ 
b : Supervised (target skill) : Ranktime & 0.05 & 0.05 & -0.02 to 0.13 & 1.48 &  =  0.140 \\ 
c : Supervised (target skill) : Ranktime & -0.07 & -0.07 & -0.14 to 0.00 & -1.97 &  =  0.049 \\ 
d : Supervised (target skill) : Ranktime & -0.02 & -0.02 & -0.09 to 0.05 & -0.50 &  =  0.615 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lm(Rank \textasciitilde{} strategy/treatment * ranktime)\\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward". 
\end{minipage}



\clearpage
\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}\label{suptable_strategyranking_strategiesslope}
\caption*{
{\large Strategy ranking} \\ 
{\small Estimated slopes per strategy for each treatment group}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 2.50 & 2.50 & 2.44 to 2.56 & 85.64 &  $<$  0.001 \\ 
Supervised (free choice) & 0.00 & 0.00 & -0.16 to 0.16 & 0.00 &  =  1.000 \\ 
Supervised (target skill)  & 0.00 & 0.00 & -0.11 to 0.11 & 0.00 &  =  1.000 \\ 
b & -1.45 & -1.45 & -1.61 to -1.29 & -17.58 &  $<$  0.001 \\ 
c  & -0.90 & -0.90 & -1.06 to -0.74 & -10.88 &  $<$  0.001 \\ 
d & -2.18 & -2.18 & -2.35 to -2.02 & -26.46 &  $<$  0.001 \\ 
Supervised (free choice) : b & 0.29 & 0.29 & -0.15 to 0.73 & 1.29 &  =  0.197 \\ 
Supervised (target skill) : b & 0.52 & 0.52 & 0.22 to 0.82 & 3.39 &  $<$  0.001 \\ 
Supervised (free choice) : c & 0.35 & 0.35 & -0.09 to 0.79 & 1.56 &  =  0.118 \\ 
Supervised (target skill)  : c & 0.10 & 0.10 & -0.20 to 0.40 & 0.65 &  =  0.517 \\ 
Supervised (free choice) : d & -0.03 & -0.03 & -0.47 to 0.41 & -0.15 &  =  0.879 \\ 
Supervised (target skill)  : d & -0.35 & -0.35 & -0.65 to -0.05 & -2.26 &  =  0.024 \\ 
Reinforcement learning : a : Ranktime & 0.05 & 0.05 & 0.00 to 0.10 & 1.86 &  =  0.064 \\ 
Supervised (free choice) : a : Ranktime & 0.06 & 0.06 & -0.01 to 0.13 & 1.61 &  =  0.107 \\ 
Supervised (target skill) : a : Ranktime & 0.09 & 0.09 & 0.04 to 0.13 & 3.40 &  $<$  0.001 \\ 
Reinforcement learning : b : Ranktime & -0.10 & -0.10 & -0.15 to -0.05 & -3.73 &  $<$  0.001 \\ 
Supervised (free choice) : b : Ranktime & -0.07 & -0.07 & -0.14 to 0.00 & -1.85 &  =  0.065 \\ 
Supervised (target skill) : b : Ranktime & -0.04 & -0.04 & -0.09 to 0.00 & -1.77 &  =  0.077 \\ 
Reinforcement learning : c : Ranktime & 0.09 & 0.09 & 0.04 to 0.15 & 3.57 &  $<$  0.001 \\ 
Supervised (free choice) : c : Ranktime & 0.04 & 0.04 & -0.03 to 0.11 & 1.24 &  =  0.216 \\ 
Supervised (target skill) : c : Ranktime & 0.02 & 0.02 & -0.03 to 0.07 & 0.89 &  =  0.376 \\ 
Reinforcement learning : d : Ranktime & -0.04 & -0.04 & -0.10 to 0.01 & -1.70 &  =  0.090 \\ 
Supervised (free choice) : d : Ranktime & -0.04 & -0.04 & -0.11 to 0.03 & -1.00 &  =  0.317 \\ 
Supervised (target skill) : d : Ranktime & -0.06 & -0.06 & -0.11 to -0.01 & -2.51 &  =  0.012 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lm(Rank \textasciitilde{} treatment * strategy/ranktime)\\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward". 
\end{minipage}

\clearpage
\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}\label{suptable_strategyranking_diffindiff_strategiesslope}
\caption*{
{\large Strategy ranking} \\ 
{\small Estimated difference in difference for each strategy between groups}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 2.50 & 2.50 & 2.44 to 2.56 & 85.64 &  $<$ 0.001 \\ 
b & -1.45 & -1.45 & -1.61 to -1.29 & -17.58 &  $<$  0.001 \\ 
c & -0.90 & -0.90 & -1.06 to -0.74 & -10.88 &  $<$  0.001 \\ 
d & -2.18 & -2.18 & -2.35 to -2.02 & -26.46 &  $<$  0.001 \\ 
Ranktime & 0.00 & 0.00 & -0.02 to 0.02 & 0.00 &  =  1.000 \\ 
a : Supervised (free choice) & -0.15 & -0.15 & -0.46 to 0.16 & -0.96 &  =  0.340 \\ 
b : Supervised (free choice) & 0.14 & 0.14 & -0.17 to 0.45 & 0.87 &  =  0.385 \\ 
c : Supervised (free choice) & 0.20 & 0.20 & -0.11 to 0.51 & 1.26 &  =  0.209 \\ 
d : Supervised (free choice) & -0.19 & -0.19 & -0.50 to 0.13 & -1.17 &  =  0.242 \\ 
a : Supervised (target skill) & -0.07 & -0.07 & -0.28 to 0.15 & -0.63 &  =  0.531 \\ 
b : Supervised (target skill) & 0.45 & 0.45 & 0.24 to 0.67 & 4.17 &  $<$  0.001 \\ 
c : Supervised (target skill) & 0.03 & 0.03 & -0.18 to 0.24 & 0.29 &  =  0.772 \\ 
d : Supervised (target skill) & -0.42 & -0.42 & -0.63 to -0.20 & -3.83 &  $<$  0.001 \\ 
b : Ranktime & -0.13 & -0.13 & -0.18 to -0.09 & -5.56 &  $<$  0.001 \\ 
c : Ranktime & -0.01 & -0.01 & -0.06 to 0.04 & -0.43 &  =  0.665 \\ 
d : Ranktime & -0.11 & -0.11 & -0.16 to -0.06 & -4.65 &  $<$  0.001 \\ 
a : Supervised (free choice) : Ranktime & 0.01 & 0.01 & -0.08 to 0.10 & 0.19 &  =  0.847 \\ 
b : Supervised (free choice) : Ranktime & 0.03 & 0.03 & -0.05 to 0.12 & 0.73 &  =  0.465 \\ 
c : Supervised (free choice) : Ranktime & -0.05 & -0.05 & -0.14 to 0.04 & -1.13 &  =  0.259 \\ 
d : Supervised (free choice) : Ranktime & 0.01 & 0.01 & -0.08 to 0.10 & 0.21 &  =  0.837 \\ 
a : Supervised (target skill) : Ranktime & 0.04 & 0.04 & -0.03 to 0.11 & 1.00 &  =  0.317 \\ 
b : Supervised (target skill) : Ranktime & 0.05 & 0.05 & -0.02 to 0.13 & 1.48 &  =  0.140 \\ 
c : Supervised (target skill) : Ranktime & -0.07 & -0.07 & -0.14 to 0.00 & -1.97 &  =  0.049 \\ 
d : Supervised (target skill) : Ranktime & -0.02 & -0.02 & -0.09 to 0.05 & -0.50 &  =  0.615 \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lm(Rank \textasciitilde{} strategy/treatment * ranktime)\\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward". 
\end{minipage}
\clearpage

\subsubsection{Strategies effect}

\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}
\caption*{
{\large Strategy effect} \\ 
{\small Estimated difference between strategies during forced exploration}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\ 
\midrule\addlinespace[2.5pt]
(Intercept) & 16.88 & 16.88 & 16.56 to 17.20 & 147.94 &  $<$  0.001 \\ 
Supervised (free choice) & 0.06 & 0.06 & -0.21 to 0.32 & 0.42 &  =  0.674 \\ 
Supervised (target skill) & 0.17 & 0.17 & -0.09 to 0.43 & 1.30 &  =  0.197 \\ 
Reinforcement learning : b & -0.44 & -0.44 & -0.51 to -0.38 & -12.95 &  $<$  0.001 \\ 
Supervised (free choice) : b & -0.32 & -0.32 & -0.39 to -0.25 & -9.29 &  $<$  0.001 \\ 
Supervised (target skill) : b & -0.45 & -0.45 & -0.51 to -0.38 & -13.56 &  $<$  0.001 \\ 
Reinforcement learning : c & -0.22 & -0.22 & -0.29 to -0.15 & -6.39 &  $<$  0.001 \\ 
Supervised (free choice) : c & -0.16 & -0.16 & -0.23 to -0.10 & -4.80 &  $<$  0.001 \\ 
Supervised (target skill) : c & -0.22 & -0.22 & -0.28 to -0.15 & -6.53 &  $<$  0.001 \\ 
Reinforcement learning : d & -0.44 & -0.44 & -0.51 to -0.38 & -12.87 &  $<$  0.001 \\ 
Supervised (free choice) : d & -0.37 & -0.37 & -0.44 to -0.30 & -11.10 &  $<$  0.001 \\ 
Supervised (target skill) : d & -0.45 & -0.45 & -0.52 to -0.39 & -13.65 &  $<$  0.001 \\ 
sd(Intercept) & 0.52 & 0.52 &  &  &   \\ 
sd(Intercept) & 0.20 & 0.20 &  &  &  \\ 
sd(Observation) & 0.19 & 0.19 &  &  &   \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lmer(racingtime \textasciitilde{} treatment / strategy + (1 \textbar{} skigroup / skier)\\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward".\\
\end{minipage}
\clearpage



\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}
\caption*{
{\large Strategy effect} \\ 
{\small Estimated group difference on strategies during forced exploration}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\
\midrule\addlinespace[2.5pt]
(Intercept) & 16.88 & 16.88 & 16.56 to 17.20 & 147.94 &  $<$  0.001 \\ 
b & -0.40 & -0.40 & -0.44 to -0.37 & -20.63 &  $<$  0.001 \\ 
c & -0.20 & -0.20 & -0.24 to -0.16 & -10.22 &  $<$  0.001 \\ 
d & -0.42 & -0.42 & -0.46 to -0.38 & -21.72 &  $<$  0.001 \\ 
a : Supervised (free choice) & -0.01 & -0.01 & -0.28 to 0.26 & -0.07 &  =  0.945 \\ 
b : Supervised (free choice) & 0.12 & 0.12 & -0.15 to 0.39 & 0.87 &  =  0.386 \\ 
c : Supervised (free choice) & 0.05 & 0.05 & -0.22 to 0.32 & 0.36 &  =  0.716 \\ 
d : Supervised (free choice) & 0.06 & 0.06 & -0.20 to 0.33 & 0.48 &  =  0.632 \\ 
a : Supervised (target skill) & 0.17 & 0.17 & -0.09 to 0.44 & 1.28 &  =  0.203 \\ 
b : Supervised (target skill) & 0.17 & 0.17 & -0.10 to 0.44 & 1.26 &  =  0.209 \\ 
c : Supervised (target skill) & 0.18 & 0.18 & -0.09 to 0.45 & 1.33 &  =  0.188 \\ 
d : Supervised (target skill) & 0.16 & 0.16 & -0.11 to 0.43 & 1.21 &  =  0.231 \\ 
sd(Intercept) & 0.52 & 0.52 &  &  &  &  \\ 
sd(Intercept) & 0.20 & 0.20 &  &  &  &  \\ 
sd(Observation) & 0.19 & 0.19 &  &  &  &  \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lmer(racingtime \textasciitilde{}  strategy / treatment + (1 \textbar{} skigroup/skier)\\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward".\\
\end{minipage}
\clearpage



\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}
\caption*{
{\large Strategy effect} \\ 
{\small Estimated slope difference for strategies between treatment groups}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\
\midrule\addlinespace[2.5pt]
(Intercept) & 16.80 & 16.80 & 16.48 to 17.12 & 146.23 &  $<$  0.001 \\ 
b & -0.44 & -0.44 & -0.48 to -0.40 & -20.15 &  $<$  0.001 \\ 
c & -0.21 & -0.21 & -0.25 to -0.16 & -8.82 &  $<$  0.001 \\ 
d & -0.45 & -0.45 & -0.50 to -0.41 & -21.04 &  $<$  0.001 \\ 
Session number & -0.09 & -0.09 & -0.10 to -0.07 & -9.99 &  $<$  0.001 \\ 
a : Supervised (free choice) & -0.02 & -0.02 & -0.30 to 0.25 & -0.17 &  =  0.866 \\ 
b : Supervised (free choice) & 0.15 & 0.15 & -0.12 to 0.42 & 1.12 &  =  0.268 \\ 
c : Supervised (free choice) & 0.05 & 0.05 & -0.23 to 0.32 & 0.33 &  =  0.740 \\ 
d : Supervised (free choice) & 0.06 & 0.06 & -0.21 to 0.33 & 0.45 &  =  0.653 \\ 
b : Session number & -0.06 & -0.06 & -0.12 to 0.00 & -1.98 &  =  0.047 \\ 
c : Session number & -0.09 & -0.09 & -0.16 to -0.03 & -2.78 &  =  0.005 \\ 
d : Session number & -0.07 & -0.07 & -0.13 to -0.01 & -2.44 &  =  0.015 \\ 
a : Supervised (free choice) : Session number & -0.08 & -0.08 & -0.19 to 0.03 & -1.40 &  =  0.163 \\ 
b : Supervised (free choice) : Session number & 0.04 & 0.04 & 0.01 to 0.07 & 2.34 &  =  0.020 \\ 
c : Supervised (free choice) : Session number & -0.01 & -0.01 & -0.07 to 0.06 & -0.23 &  =  0.819 \\ 
d : Supervised (free choice) : Session number & 0.00 & 0.00 & -0.03 to 0.03 & 0.20 &  =  0.842 \\ 
sd(Intercept) & 0.52 & 0.52 &  &  &  & \\ 
sd(Intercept) & 0.19 & 0.19 &  &  &  & \\ 
sd(Observation) & 0.19 & 0.19 &  &  &  & \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lmer(racingtime \textasciitilde{} strategy / treatment * session number + (1 \textbar{} skigroup/skier)\\
Note: “a” = "stand against"; b = "extend"; "c" = "rock skis forward"; d = "extend with rock skis forward".\\
\end{minipage}
\clearpage

\setlength{\LTpost}{0mm}
\begin{longtable}{lrrrrrl}
\caption*{
{\large Strategy effect} \\ 
{\small Estimated improvement across sessions for "extend with rock skis forward" for the treatment groups}
} \\ 
\toprule
Predictors & Estimates & SE & CI & t & p \\
\midrule\addlinespace[2.5pt]
(Intercept) & 16.59 & 16.59 & 16.20 to 16.98 & 101.76 &  $<$ 0.001 \\ 
Supervised (free choice) & 0.08 & 0.08 & -0.18 to 0.35 & 0.64 &  =  0.522 \\ 
Supervised (target skill) & 0.06 & 0.06 & -0.20 to 0.32 & 0.44 &  =  0.660 \\ 
Reinforcement learning : Session number & -0.09 & -0.09 & -0.11 to -0.07 & -8.15 &  $<$  0.001 \\ 
Supervised (free choice) : Session number & -0.10 & -0.10 & -0.11 to -0.08 & -10.01 &  $<$  0.001 \\ 
Supervised (target skill) : Session number & -0.08 & -0.08 & -0.09 to -0.06 & -9.25 &  $<$  0.001 \\ 
sd(Intercept) & 0.51 & 0.51 &  &  &  &  \\ 
sd(Intercept) & 0.27 & 0.27 &  &  &  &  \\ 
sd(Observation) & 0.18 & 0.18 &  &  &  &  \\ 
\bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
Formula: lmer(racingtime \textasciitilde{} treatment / session number  + (1  \textbar{} skigroup / skier)\\
\end{minipage}

\section{Supplementary discussion}\label{supdiscussion}
To convert race time to FIS World Ranking, we assumed that each second corresponded to a 7-point FIS. We could then multiply by 0.12 * 7 to find the difference in FIS points. Then, we used the median rank in our sample and calculated what this difference corresponded to in the World Ranking. We performed this analysis for both females and men. Due to confidentiality, we do not want to say which FIS list we used for this conversion. If the reader thinks 7 points is too small or large, then we welcome the reader to change this number up or down. We only used this conversion to help readers evaluating the effect. 









%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
