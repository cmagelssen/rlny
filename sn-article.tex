%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  

\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
\RequirePackage{amsthm}

%\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Reference 
% <some more package/preamble stuff>



%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{longtable}
\usepackage{float}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Learning strategy choices through evaluation-based training improves strategy-outcome understanding and skill learning compared to traditional coaching instruction}

%Reinforcement learning improves skill learning in skilled alpine ski racers compared to standard coaching

%Learning strategies through evaluation improves skill learning and reasoning compared to traditional instruction with a coach

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[]{\fnm{Christian} \sur{Magelssen}}\email{cmagelssen@gmail.com}



\author[1]{\fnm{Matthias} \sur{Gilgien}}\email{iiauthor@gmail.com}


\author[2]{\fnm{Simen Leithe} \sur{Tajet}}\email{iiiauthor@gmail.com}


\author[1]{\fnm{Thomas} \sur{Losnegard}}\email{iiiauthor@gmail.com}

\author[1]{\fnm{Per} \sur{Haugen}}\email{perh@nih.no}

\author[3]{\fnm{Robert} \sur{Reid}}\email{iiauthor@gmail.com}


\author[4]{\fnm{Romy} \sur{Frömer}}\email{r.froemer@bham.ac.uk}


\affil*[1]{\orgdiv{Institute for Physical Performance}, \orgname{Norwegian School of Sport Sciences}, \orgaddress{\street{Sognsveien 220}, \city{Oslo}, \postcode{0863}, \state{Oslo}, \country{Norway}}}

\affil[2]{\orgdiv{Institute of Sport and Social Science}, \orgname{Norwegian School of Sport Sciences}, \orgaddress{\street{Sognsveien 220}, \city{Oslo}, \postcode{0863}, \state{Oslo}, \country{Norway}}}

\affil[3]{\orgdiv{The Norwegian Ski Federation}, \orgname{Organization}, \orgaddress{\street{Sognsveien 75 B1}, \city{Oslo}, \postcode{0840}, \state{Oslo}, \country{Norway}}}

\affil[4]{\orgdiv{School of Psychology}, \orgname{University of Birmingham}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Skilled performers need skillful and adaptive movement strategies to solve tasks effectively. Typically, performers learn these strategies with instruction-based teaching methods where coaches offer performers a correct solution. Based on recent evidence from decision neuroscience, we asked whether skilled performers learn strategy choices better with an evaluation-based training strategy (reinforcement learning). To address this question, we conducted a three-day learning experiment with skilled alpine ski racers (n=98) designed to improve their performance on flat slopes on slaloms with four strategies at their disposal to achieve this goal. We found that the skiers in the reinforcement learning group showed greater improvements in their race times during the training sessions and outperformed their counterparts in the supervised (free choice) learning group, who received instruction in strategies from their coach. Surprisingly, the skiers in the reinforcement learning group also showed descriptive (but not significantly) better performance than did those in the supervised (target skill) learning group, despite the latter being coached in the theoretically optimal strategy for skiing well on flats. Our findings expand the earlier literature showing that reinforcement learning can be an important training strategy for improving strategy decisions and learning in skilled performers}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Reinforcement learning, Supervised learning, Expertise development, Alpine ski racing, Reasoning, Strategy learning}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle


\section{Introduction}
Skilled performers need effective movement strategies to solve task situations successfully\cite{krakauer_motor_2019,wolpert_motor_2010, wolpert_principles_2011, gray_plateaus_2017, diedrichsen_motor_2015, chen_effects_2018, stanley_motor_2013, du_relationship_2022, mangalam_investigating_2023, taylor_role_2012, tsay_strategy_2023}. In sports, these situations are often diverse and erratic, and the optimal strategy depends on the specific circumstances. Consequently, skilled performers must build an extensive repertoire of strategies and learn to select the best strategy for each scenario to achieve optimal performance\cite{krakauer_motor_2019, du_relationship_2022,mangalam_investigating_2023,wolpert_principles_2011, gallivan_decision-making_2018}. For example, alpine ski racers should select a different strategy to perform well on steep and flat slopes \cite{supej_impact_2015} but also be able to switch between strategies depending on the particular conditions on the hill and the slalom course \cite{reid_kinematic_2010}. The need to develop an effective strategy for each of these situations takes time and partly explains why skilled performers need extensive training to unleash their full potential \cite{krakauer_motor_2019}. Therefore, if better training methods exist to empower performers in making ingenious and adaptive strategic decisions beyond conventional training techniques, this could profoundly influence the training of present and future learners.

%The development of sporting expertise demands extensive amounts of high-quality training \cite{ericsson_role_1993, hodges_predicting_2004, vaeyens_talent_2009, sosniak_learning_1985}. Unlike training novices to reach acceptable skill levels, the key challenge for skilled performers is finding ways to improve beyond current levels of performance\cite{ericsson_development_2003, ericsson_scientific_1998, gray_plateaus_2017, williams_expertise_2008, du_relationship_2022}. One route to achieve this skill improvement is to continue perfecting an already chosen strategy. By staying with the current choice, learning progresses slowly and operates through honing the machinery components underlying the automated solution \cite{du_relationship_2022}. A second route is to switch away from current strategy to seek a better alternative, which can accommodate greater improvement than simply repeating what is already automated   \cite{gray_plateaus_2017, du_relationship_2022, krakauer_motor_2019}. Skillful, adaptive strategy selection is a key characteristic of expertise \cite{ericsson_scientific_1998, ericsson_development_2003, krakauer_motor_2019, stanley_motor_2013}. Yet we know little about which teaching methods are most effective in stimulating learners to make good strategy choices, let alone the sources of information that drive these learning processes \cite{taylor_cerebellar_2014, taylor_role_2012}. If methods superior to those currently used in standard practice exist, they could prove invaluable for training both present and future generations of learners.

In current practice, these strategies are typically taught through instructional methods, where a coach or teacher conveys 'what to do' (e.g., take a shorter line around the gate), followed by corrective feedback (e.g., you can shorten the line even more) \cite{williams_practice_2005, williams_effective_2023, hodges_role_1999}. This teaching strategy can be likened to what motor learning refers to as supervised learning, where the teaching signal for skill improvement represents the disparity between the desired skill outcome and the learner outcome \cite{jordan_forward_1992, wolpert_motor_2010, doya_complementary_2000}. Through practice, this teaching signal can bring the learner closer to executing what is assumed to be the correct choice. The question, however, is whether this is the most effective for training learners to choose effective strategies. 

One drawback of the supervised learning strategy for training these decisions is that the choice of strategy the coach chooses may not be optimal for further skill progression, as what coaches judge as a good strategy does not always align with reality, even for the best-trained eye \cite{supej_impact_2019, cochrum_visual_2021}. Learners might, therefore, miss opportunities to discover the best strategy when coaches opt for suboptimal strategies \cite{gray_plateaus_2017}. Worse, these learned suboptimal strategies might turn into habits that can be difficult to break \cite{popp_effect_2020}. Supervised learning might also constrain learners to adopting a single ('universal') strategy for all situations rather than acquiring a repertoire of strategies and discerning the most effective strategies for each specific scenario. Finally, it remains uncertain whether the prescriptive approach is the most effective teaching strategy for achieving long-lasting learning effects \cite{wulf_instructions_1997, hodges_role_1999, williams_practice_2005,williams_effective_2023}.

Learning to choose good strategies can also occur without the direct influence of a coach providing advice. The cornerstone of reinforcement learning \cite{sutton_reinforcement_2018} is that learners can learn by exploring strategies and evaluating their outcomes, using the successes and failures of outcomes as teaching signals. That is, rather than being told the putatively correct solution to the problem, as in supervised learning, they learn the value of different strategies, which allows them to finally pick the best solution. Specifically, these values are learned by comparing a given choice's outcomes with the currently expected outcome of that choice. Outcomes that exceed or fall short of expectations result in errors in reward prediction, signaling that the learner must update their predictions to better anticipate future rewards following that action \cite{rescorla_theory_1972}. These reward prediction errors are then incorporated to form a new and better estimate of reward, by updating expectations through a weighted running average. Reinforcement learning has been tremendously powerful in explaining human and animal learning \cite{waelti_dopamine_2001, schultz_neural_1997, pessiglione_dopamine-dependent_2006,lee_neural_2012}, improving skill learning in laboratory-based tasks \cite{lior_shmuelof_overcoming_2012, abe_reward_2011, truong_error-based_2023, hasson_reinforcement_2015}, as well as training AI to perform complex tasks such as computer games starting from pixel inputs, only\cite{mnih_human-level_2015}. Based on this evidence, our question was whether reinforcement learning offers a better alternative for training learners to make better decisions about strategies than traditional supervised learning with a coach. 

To address this question, we conducted a three-day learning experiment with ninety-eight skilled alpine ski racers from Norway and Sweden. To achieve performance improvement among this skilled cohort of athletes, we focused on improving flat sections in slalom, an area with considerable potential for enhancement, even among the best performers\cite{supej_new_2011}, and delineated four strategies, each carefully selected to enhance performance in this section by taking advantage of principles from mechanics (Fig. \ref{fig:experiment}a). To study how different instructions and feedback influenced learning and choices of these strategies, we assigned the skiers to three different learning groups (Fig. \ref{fig:experiment}b): In the reinforcement learning group, the skiers chose a strategy on every trial and saw their race times immediately after each trial to inform these decisions. In the supervised (free choice) learning group, we recruited ski coaches from the tested ski teams to coach on the strategy they believed to be the best or most appropriate for the skier. In the supervised (target skill) learning group, we recruited ski coaches to instruct skiers to select the strategy that we defined as the theoretically best strategy based on computational modelling \cite{lind_physics_2013, mote_accelerations_1983, luginbuhl_identification_2023} and observations of elite skiers \cite{reid_alpine_2020, magelssen_is_2022}. This group serves as a benchmark for the upper limit of performance achievable through optimal strategy choices, and thus offers an upward comparison for the performance yield through reinforcement learning of strategy selection. Coaches in the two supervised learning groups saw the times but were instructed to not disclose these to the skiers. We hypothesized that the skiers in the reinforcement learning group would learn to choose better strategies and thus achieve better performance than skiers subject to traditional supervised learning with a coach (supervised learning: free choice), possibly reaching similar performance as the supervised (target skill) learning group that uses the best strategy throughout. 




\begin{figure}[H]
\centering
\includegraphics{figures/figure_method_courseandstrategy.pdf}
\caption{\textbf{a.} Illustrations of the two slalom courses used in the study. The main slalom course was a rhythmic course deployed in all sessions except for the transfer session. The course setting for the transfer session involved a progression in gate offset, starting with the largest offset and ending with the smallest offset. \textbf{b.} Illustration of the strategies defined to enhance racing performance on flat terrain in slalom: The "stand against" strategy emphasized maintaining a stable stance against external forces without body extension along the body's longitudinal axis or rocking skis forward; 'Rock skis forward' involved rocking skis forward from gate passage to completion of the turn; The "extend" strategy involves extending the body from a laterally tilted position during the turn, closer to the turn's center of rotation; The "extend with rocking skis forward" was expected to be the best strategy combining the two effects from extending and rocking skis forward, and we therefore defined this as the theoretical best strategy}
\label{fig:courseandstrategies}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{figures/figure_method_experiment.pdf}
\caption{Illustration of the experimental design and procedure. \textbf{a.} Timeline of the three-day learning experiment. During the baseline, skiers skied a slalom course in the shortest amount of time possible without receiving race time feedback. The skiers were then assigned to three treatment groups (see b). In their assigned group, skiers underwent an acquisition phase in their designated treatment group comprising one forced exploration (skiers performed all strategies) and two free choice sessions (skiers or coaches could choose strategies themselves). On the last day, skiers completed a retention and transfer test where they could pick strategy themselves, again without receiving race time feedback. \textbf{b.} Illustration of treatment groups in the study. Supervised (target skill) learning involved coaches consistently choosing the theoretically best strategy (except during forced exploration), while supervised (free choice) learning allowed coaches to freely select strategies. Skiers in both of these treatment groups received feedback on strategy execution from their respective coach, while skiers in the reinforcement learning group independently selected strategies and received feedback from the timing system to facilitate value learning of each strategy}
\label{fig:experiment}
\end{figure}

\section{Method}


\subsection{Participants}
Conducting studies on alpine ski racing poses challenges related to environmental control and resource constraints. Our sample size approach involved recruiting as many skiers as possible during June 2023, when we had a short time window to test skiers in the indoor ski hall. We set the minimum sample size to 80 skiers, which we deemed appropriate for this context. Prior to data collection, data and power simulations for sample sizes of 80, 100, and 120 skiers were conducted, revealing simulated powers of 0.60, 0.75, and 0.80, respectively, for the smallest effect size of interest (0.3 second difference between groups) (\url{https://osf.io/c4t28}). The smallest effect size of interest was based on our knowledge of alpine skiing and discussions with coaches, but it was intended for a 50-meter longer course and more training sessions than we ultimately ended up using due to practical considerations. We deliberately opted to recruit skiers with diverse skill levels for the study to augment the generalizability of our findings. However, to ensure a sufficient skill level to handle the specific icy snow conditions prepared in the ski hall, we recruited only skiers aged 15 and older. The sample size justification, task design, and analysis plan were preregistered before data collection (\url{https://osf.io/tfb2w}).

We managed to recruit ten alpine ski teams comprising 98 alpine ski racers from Norway and Sweden (age M = 18.1 years, SD= 2; 40 females, 58 males). Two skiers were excluded from the analysis due to an injury prior to the study (n=1) or sickness during the study (n=1); thus, a total of 96 skiers completed the entire study and were included in the analysis.  Among the ski groups tested were five ski academies, three senior development teams, and two national ski teams. These skiers were generally highly skilled, with a median world rank of 605, but there was also considerable variability, as indicated by a substantial interquartile range (Q1 = 248, Q3 = 1390.5). A smaller subset of the participants (n = 13) were not world-ranked, as they had yet to compete in internationally sanctioned races that form the basis for calculating athlete points and rankings. Table 1 provides demographic information for each treatment group.

In addition, we recruited coaches for the two supervised learning groups. This sampling process was carried out pragmatically due to space and time constraints in the ski hall, which made it impossible to test all ski teams simultaneously. The 10 ski teams with alpine skiers that were recruited were therefore divided into 4 groups for which the study was conducted at different times. For each of the four groups, we recruited two coaches from the ski teams to serve as coaches in the supervised (free choice) learning group, totaling 8 coaches (2 women; 6 men). These coaches had extensive coaching experience in coaching alpine ski racers. In addition, for each group of ski teams that completed the experiment together, we recruited a third coach to instruct the skiers in supervised (target skill) learning. To ensure that these coaches had sufficient credibility to make the skiers buy into our theoretical best strategy, we selectively recruited three highly experienced coaches from the Norwegian alpine ski team (one coach served twice). Importantly, all the coaches remained unaware of the experimental manipulation. Table \ref{descriptive_coach} provides demographic information about the coaches. All the skiers and coaches provided informed consent before the study. The study was approved by the Human Research Ethics Committee of The Norwegian School of Sport Sciences (ref. 279-040523) and the Norwegian Agency for Shared Services in Education and Research (ref. 871468).

\begin{sidewaystable}
\label{descriptive_skier}
\caption{\textbf{Skier characteristics for each learning group}}
\centering
\begin{tabular}[H]{l|c|c|c|c|c|c}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{\textbf{Reinforcement learning}} & \multicolumn{2}{c|}{\textbf{Supervised (free choice)}} & \multicolumn{2}{c}{\textbf{Supervised (target skill)}} \\
\textbf{Sex} & \textbf{F}, N = 13 & \textbf{M}, N = 19 & \textbf{F}, N = 14 & \textbf{M}, N = 19 & \textbf{F}, N = 13 & \textbf{M}, N = 20\\
\hline
Age & 18.4 (2.3) & 17.7 (1.8) & 18.1 (2.3) & 18.3 (1.8) & 17.8 (2.4) & 18.2 (2.1)\\
\hline
Training group &  &  &  &  &  & \\
\hline
\hspace{1em}National team & 1 (7.7\%) & 1 (5.3\%) & 2 (14\%) & 1 (5.3\%) & 2 (15\%) & 4 (20\%)\\
\hline
\hspace{1em}Senior team & 3 (23\%) & 5 (26\%) & 1 (7.1\%) & 5 (26\%) & 0 (0\%) & 5 (25\%)\\
\hline
\hspace{1em}Ski academy & 9 (69\%) & 13 (68\%) & 11 (79\%) & 13 (68\%) & 11 (85\%) & 11 (55\%)\\
\hline
FIS points &  &  &  &  &  & \\
\hline
\hspace{1em}Median (IQR) & 54 (42, 80) & 46 (34, 90) & 58 (26, 80) & 44 (35, 61) & 49 (28, 66) & 31 (28, 63)\\
\hline
\hspace{1em}Not recorded & 3 & 2 & 1 & 2 & 2 & \vphantom{1} 3\\
\hline
World ranking &  &  &  &  &  & \\
\hline
\hspace{1em}Median (IQR) & 630 (394, 1,217) & 707 (364, 2,317) & 709 (133, 1,224) & 662 (387, 1,274) & 527 (145, 882) & 314 (220, 1,360)\\
\hline
\hspace{1em}Not recorded & 3 & 2 & 1 & 2 & 2 & 3\\
\hline
\multicolumn{7}{l}{\rule{0pt}{1em}\textsuperscript{1} Mean (SD); n (\%)}\\
\end{tabular}
\end{sidewaystable}


\begin{sidewaystable}\label{descriptive_coach}
\caption{\textbf{Coach characteristics for each learning group}}
\centering
\begin{tabular}[H]{l|c|c|c}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{\textbf{Supervised (free choice)}} & \multicolumn{1}{c}{\textbf{Supervised (target skill)}} \\
\textbf{Characteristic} & \textbf{F}, N = 2 & \textbf{M}, N = 6 & \textbf{M}, N = 3\\
\hline
Age & 38.5 (3.5) & 44.3 (8.8) & 48.0 (7.0)\\
\hline
Ski education (highest achieved) &  &  & \\
\hline
\hspace{1em}Level 2 & 0 (0\%) & 1 (17\%) & \\
\hline
\hspace{1em}Level 3/4 & 2 (100\%) & 5 (83\%) & 3 (100\%)\\
\hline
Sport science degree (highest achieved) &  &  & \\
\hline
\hspace{1em}MSc & 1 (50\%) & 0 (0\%) & 1 (33\%)\\
\hline
\hspace{1em}BSc & 1 (50\%) & 1 (17\%) & 1 (33\%)\\
\hline
\hspace{1em}No & 0 (0\%) & 4 (67\%) & \\
\hline
\hspace{1em}One-year program & 0 (0\%) & 1 (17\%) & 1 (33\%)\\
\hline
Coaching experience (years) &  &  & \\
\hline
\hspace{1em}National team (WC/EC)/Senior teams & 5.00 (1.41) & 5.00 (4.24) & 15.67 (1.15)\\
\hline
\hspace{1em}Ski academy & 7.5 (3.5) & 7.5 (6.5) & 2.00 (3.46)\\
\hline
\hspace{1em}Ski club & 2.5 (3.5) & 6.5 (8.7) & 6.0 (6.6)\\
\hline
\multicolumn{4}{l}{\rule{0pt}{1em}\textsuperscript{1} Mean (SD); n (\%)}\\
\end{tabular}
\end{sidewaystable} 


\subsection{The setup}
The experiment was conducted in the indoor ski hall SNØ in Oslo, Norway (\url{https://snooslo.no/}). In this hall, we used a 210-meter-long flat slope section of the race hill, which we water-injected before testing each group of skiers to ensure uniform and fair snow conditions for all skiers (Supplementary \ref{sup_snowprep} for detailed description). With our chosen course setup, this 210-meter-long flat section allowed space for 19 slalom gates. 

We used two types of slalom courses in the experiment (Fig. \ref{fig:courseandstrategies}a). The main slalom course was used in all sessions, except during the transfer test, and featured a 10 m distance and a 1.9 m offset. The course distance aligned with our previous study \cite{magelssen_is_2022}, but we opted for a slightly larger offset to better suit the skill level of our skiers. The transfer test evaluated how well the skiers transferred their learning to a new slalom course more realistic to a typical alpine ski race course. To assess this, we set a course with a progression in gate offset, starting with five gates at a 2.2-meter offset, followed by seven gates at a 1.7-meter offset, and concluding with seven gates at a 1.2-meter offset. Although we did not expect radical differences in strategy effects, we anticipated a greater emphasis on rocking the skis forward in gates with a 2.2-meter offset than in those with a 1.2-meter offset to enhance turn exit release. Both courses were set with stubbies (short gates) instead of long gates to minimize energy dissipation upon hitting the gate \cite{minetti_biomechanics_2018}. Using long gates can also be a distracting element in that skiers' attention is allocated to clearing the gate instead of focusing on executing the skill. Finally, this approach helped us avoid creating holes in the course, which can occur when the long gate is forcefully slammed into the ground. To minimize wear and tear on the course, we set two parallel and identical courses and routinely shifted between them.

The start gate was positioned 20 meters before the first gate. Skiers were required to start in a static position to ensure consistency in the starts, with the toe piece of the binding placed behind the starting gate. The skiers started by putting their skis in parallel and lifting the poles without using poling or skating for propulsion (Supplement Video illustrates the starting procedure and setup). We recorded the times using a wireless photocell timing system (HC Timing wiNode and wiTimer; Oslo, Norway). Timing started when the skier crossed the first photocell pair situated 10 meters below the starting gate. 
 
\subsection{Experimental design}
We employed a between-subjects design and posed the learning question of discovering effective strategies as an \textit{n}-armed bandit problem \cite{sutton_reinforcement_2018}. The essence of this problem is that a learner repeatedly tries different options and observes their outcomes to learn which strategy is the best and, therefore, which one to choose. Finding the best strategy requires a delicate balance between exploiting the strategy known to yield the best payoff and exploring alternative strategies that may offer superior benefits. In our study, the options consisted of four strategies that skiers could employ to improve their race times on flat slopes in slalom, grounded in physics-based coaching manuals for alpine ski racing \cite{lemaster_skiers_1999, lemaster_ultimate_2010, lind_physics_2013, mote_accelerations_1983}, biomechanical research on elite skiers \cite{reid_kinematic_2010, reid_alpine_2020, magelssen_is_2022}  and common strategies used by coaches.  The four strategies were named "stand against", "rock skis forward", "extend", and "extend with rock skis forward (see Fig. \ref{fig:courseandstrategies}b for a strategy illustration and Supplementary \ref{sup_strategies} for an extended explanation). To study how instruction and feedback drove learning to select effective strategies, we designed and allocated skiers to three treatment groups, which allowed us to compare reinforcement learning with traditional supervised learning with a coach: 

For the supervised (target skill) learning group, we provided the best possible training program by engaging highly experienced coaches who explained to the skiers that the 'extend with rock skis forward' strategy was the most effective for skiing fast on flat terrain in slalom, citing evidence from research literature in alpine skiing mechanics \cite{reid_kinematic_2010, mote_accelerations_1983, lind_physics_2013}. The coach then instructed the skiers to adopt this strategy and provided feedback on its execution after each trial. Note that the coach had access to the skiers after each trial but was prohibited from communicating the race time with the skiers.

In the supervised (free choice) learning group, the skiers were assigned to two coaches recruited from the tested group of ski teams. To balance the skiers' skill levels between the two coaches, we created new blocks from the ranked list from baseline testing and randomly assigned them to the coaches. We instructed the coaches to improve the skiers' race times as much as possible with our four defined strategies to their disposal to achieve this goal. For each trial, the coach selected a strategy for the skier, observed the skier during his trial and provided feedback on its execution afterward. Similar to the supervised (target skill) learning group, the coaches had access to the skiers' data after each trial but could not share this information with the skiers.

In contrast, the reinforcement learning group was not assigned to any coach. Instead of having a coach deciding the skiing strategy for them, the skiers in this treatment group were told to choose a strategy for each trial by themselves to ski the course as fast as possible. To help the skiers choose and learn from evaluation, this group could see their racing times immediately after they crossed the finish line. Although this group had no coach, we assigned a person to communicate with the skiers to record their choices and encourage them to try skiing quickly to prevent boredom effect.

\subsection{Procedures}\label{procedure}
Fig. \ref{fig:experiment}a illustrates the procedures employed in the study. 
In the baseline test, the skiers began with two warm-up runs: one in a free skiing warm-up course and one in a specific warm-up in the slalom course. During these warm-up runs, skiers were instructed, trained and verified on the start procedure by an instructor. As a first run in the baseline assessment, skiers completed a straight-gliding run, where they skied straight down from start to finish in a static, upright slalom posture. Subsequently, the skiers completed four runs in the slalom course. Skiers were encouraged to ski as quickly as they could, but they could not see their times, nor did they receive any instructions on how to perform well.

After the initial baseline assessment, the skiers took a 60-minute break. In the meantime, we allocated the skiers to the three learning groups deploying a randomized-blocked approach to account for pre-existing differences in the skiers’ performance levels \cite{maxwell_designing_2017}. Specifically, we computed each skier’s average across the four trials at baseline and ranked them accordingly. We then created \textit{n} blocks with block sizes corresponding to our three treatments for the entire list of skiers and assigned these skiers to these predefined blocks. Finally, we randomly allocated the skiers to the different treatment groups within each block (Fig. \ref{fig:experiment}b).

The learning groups participated in sessions at different times to prevent treatment diffusion \cite{maxwell_designing_2017}. As the ski group comprised teams of skiers who knew each other well and who resided together, we explicitly emphasized the importance of keeping information about the sessions private. To stay within the time frame at the ski hall, the two supervised learning groups underwent training together. To facilitate this, we arranged stations in the finishing area with space and vision dividers and ample space to impede communication between coaches in the supervised learning groups (Supplementary \ref{fig:coachstation}). In addition, we developed a Python script that fetched the race times from the timing system, filtered the times for each coach, and transmitted them to the station where the coach was located, ensuring that no information leaked between the coaches. The learning group (reinforcement learning versus supervised learning) that initiated after the baseline session was randomized and counterbalanced across the group of ski teams we tested. 

The first session after learning group assignment involved a forced exploration. Here, skiers within the learning group were gathered, and the session started by introducing them to the strategies. We explained that we had identified four strategies to enhance racing times on flat slopes in slalom. Subsequently, each strategy was detailed, supported by illustrative drawings in \ref{fig:courseandstrategies}b Figure and corresponding word explanations as outlined in the Supplementary \ref{sup_strategies}. To confirm comprehension, we conducted two short familiarization trials for each strategy or until the execution met our performance standards. After reviewing the strategies with the skiers, we gathered them in their respective treatment groups and asked them to rank the strategies (1=best; 4=worst) for what they believe to be the best strategies for improving race times in the flat section of a slalom course. Throughout the instruction and ranking process, skiers were explicitly instructed not to discuss the strategies with each other. It is important to note that the same instructor was used for all treatment groups within the tested ski group. After this, the skiers conducted a total of eight trials on the course, with two trials for each strategy. During these trials, the reinforcement learning group received feedback on the timing, whereas the coach provided feedback in the supervised learning groups. After completing the eight rounds, the skiers re-evaluated and ranked the strategies.

On the second day, the skiers completed two free choice sessions, each comprising a total of 6 trials in the same slalom courses that were used for the baseline testing the day before. Prior to the 6 free choice trials, the skiers performed one warm-up free skiing run and one warm-up run in the slalom course. In these sessions, the supervised (target skill) learning group consistently selected the theoretically best strategy (that is, 'extend with rock skis forward'). Conversely, in the supervised (free choice) learning group and the reinforcement learning group, the coach and the skiers, respectively, had the autonomy to choose the skiing strategy for each run. After each session, coaches (except supervised target skills) and skiers were asked to re-evaluate and rank the strategies.

On the third and last day, the skiers performed a retention test and a transfer test to assess the effect of the training approaches on learning and performance. The retention test was performed in the same course as the baseline and acquisition sessions, whereas the transfer test was performed in the transfer course and involved a progression in gate offset from start to finish. Since the transfer test was a new course, we allowed the skiers to inspect the course before the test. The retention and transfer tests were conducted with the three learning groups together. None of the treatment groups received any feedback from coaches or time during these tests. After each test, the skiers were asked to rank the strategies.


\subsection{Analysis}
The data were cleaned with custom functions built on tidyverse \cite{wickham_welcome_2019} packages in R \cite{r_core_team_r_2022}. After this process, we validated the data by performing trial counting and visual inspection of the race time to screen for errors. An extensive report of this cleaning and validation process can be found at OSF  (\url{https://osf.io/2jxgk}).

Due to the hierarchical structure of the data, our general statistical strategy relies on multilevel modeling. At the first level, each skier performed multiple trials during each session. At the second level, each skier was nested within groups of ski teams that performed the experiment together. To account for these multilevel data structures, we leveraged linear mixed-effects models. To model random effects, we adopted a design-driven approach \cite{barr_random_2013, barr_learning_2021}, where we sought to account for all nonindependence introduced by repeated sampling from the same ski group and skier. We deployed classical frequentist statistics and fitted these models with the lme4 package \cite{bates_fitting_2015} in the R \cite{r_core_team_r_2022}programming language. We used a simple coding scheme for our predictors where the intercepts represent the estimated mean of the cell means and the contrasts represent the estimated difference with respect to the reference level, which we set for reinforcement learning. Two-tailed p values and degrees of freedom for each model were derived using the lmerTest package \cite{kuznetsova_lmertest_2017} via the Satterthwaite approximation method. Alpha was set to 0.05 for all test statistics.

\subsubsection{Race time}

Race time was analyzed using linear mixed-effect regression models. Initially, we planned to normalize the racing time by expressing the racing time as the difference from the straight-gliding time performed at the beginning of every session. This difference better approximate the skiers' actual skill improvement by considering the variance in snow conditions. However, practical considerations led us to deviate from this approach. This change was necessary because we had to flip or shift the course after each day to ensure snow conditions with the least damage. Unfortunately, these adjustments made maintaining a clean, straight-gliding lane difficult since the straight gliding lane crossed many areas with damage to the snow surface (holes) from the previous course set (see Supplementary \ref{sup_coursesetting} for an image of these holes). Collisions with these holes affected the race time, adding noise to the results. Therefore, we used a more conservative approach and analyzed the raw racing times instead of analyzing the normalized racing times.

For the acquisition session, we modeled race time using Session and Treatment, and their interactions, as predictors. For retention and transfer, we modeled racing times at these sessions, with treatment added as a predictor. In addition, we used the average performance for each skier on the baseline test as a predictor to improve estimate precision and adjust for group differences at baseline testing. 

To model the effect and development of the strategies we broke the analysis up into different sub-models. One analysis focused on differences in the strategies and groups regarding Forced Exploration, where all participants had completed all strategies. Another analysis examined the transition from Forced Exploration to retention for both supervised (free choice) and reinforcement learning. The final model investigated the development between groups specifically for the "extend with rock skis forward" strategy. Session was coded as a continuous variable in all models. 

\subsubsection{Strategy choices}
Strategy choices were analyzed using generalized linear mixed-effect regression models with a binomial logit-link function. To model the selection of the theoretical best strategy, we inputted the data as logistic, where for each trial (\(i\)) per skier (\(j\)) within ski group (\(k\)), we counted \(y_{ijk}=1\) when the skier chose the theoretical best strategy (that is, 'extend with rock skis forward') or 0 when they did not. We included Treatment and Session and their interaction as two variables. To account for the nonindependence of the data structure, we allowed the intercept to vary by including a random intercept for the skier and ski groups.

We adopted the same model formula to model the selection of the estimated best strategy. This time, however, we counted \(y_{ijk}=1\) when the skier chose their estimated best strategy and 0 when they did not select that strategy. To estimate the best strategy for each skier in the sample, we used the sample-average method \cite{sutton_reinforcement_2018} to average the race time for each strategy and selected the strategy with the lowest estimated (that is, best) value. The sessions that we used to form this average were Forced Exploration, Free Choice 1, and Free Choice 2.  Due to the scaling issues with generalized linear models, we followed the recommendation to determine the size and significance of the effects of interest using marginal effects on the probability scale \cite{mize_best_2019, mccabe_interpreting_2022}. Interactions were assessed using discrete difference (also second difference), which is also in line with these recommendations. To derive these estimates, we used the emmeans package \cite{lenth_emmeans_2023}.

To learn how the skiers and coaches used feedback to guide their choices, we constructed a 'win-stay, lose-switch' model (WSLS; \cite{nowak_strategy_1993, worthy_comparison_2014, iyer_probing_2020}). For this WSLS analysis, we z-scored the race times for each skier for Free Choices 1 and 2 and counted \(y_{ijk}=1\) when the skier repeated the previous strategy and 0 when it was not. The data were modeled using a generalized linear mixed-effect regression model with a binomial logit-link function, with Treatment and z-transformed Race Time and their interaction as the two variables. To test for differences in error sensitivity, we used the marginal effects at the mean (MEM) derived from the emmeans package \cite{lenth_emmeans_2023}.


\subsubsection{Strategy evaluations and outcomes}
To analyze the strategies' rankings, we used single-level linear regression owing to the singularity of our multilevel models. In this model, we inputted Session as a continuous variable (with the intercept anchored at Familiarization) and Treatment as the predictors. For supervised (free choice) learning, we used the coaches' rankings during the sessions where they selected strategies, and we used the skiers' rankings when they selected strategies during the retention and transfer tests.

we modeled the difference between the strategies and group differences during forced exploration. 

To evaluate how the race times evolved over the next sessions, we had to break down the
analysis into three sub-analyses because the supervised (target skill) learning group only
performed ’extend with rock skis forward’ during free choice 1 and free choice 2. First, we built at model to assess the difference between the strategies and group differences during forced exploration. To this end, we included Treatment (all learning groups) and Strategy, and their interaction, as factors. Second, we built a model to test for differences in improvement on ’stand against’, ’rock skis forward’, and ’extend’ only for the reinforcement and supervised (free choice) learning
groups. To this end, we inputted Session as a continuous variable (with the intercept anchored at Familiarization) and Treatment as the predictors. Finally, we built a model to assess the development for all groups on the ”extend with rock skis forward” strategy. This model consisted of Session as a continuous variable (with the intercept anchored at Familiarization) and Treatment as the predictors.

\section{Results}

\subsection{Race time}\label{result_racetime}
We posited that strategy choice would greatly impact race time in the slalom course and that the reinforcement learning group would learn to select better strategies and consequently perform better over the course of the experiment compared to the supervised (free choice) learning group where we used the skiers' own coaches. The supervised (target skill) learning group was instructed to choose the optimal strategy and therefore served as a benchmark for the maximum expected benefit of improved strategy selection through reinforcement learning. As our first step in the analysis, we assessed whether there were pure time differences between the groups across the different sessions without taking the chosen strategy into account. 

If the reinforcement learning group learned to select better strategies than the supervised (free choice) learning group, we would expect to observe differences in improvement between these two groups over the course of the three acquisition sessions. Therefore, our hypothesis was that the reinforcement learning group would improve more during the acquisition sessions than the supervised (free choice) learning group. On the other hand, we expected the supervised (target skill) learning group to rapidly improve race time when individuals were instructed to only select this strategy during the last two acquisition sessions (free choice 1 and free choice 2). All groups performed similarly at forced exploration where all groups had the same number of trials on each strategy; we found no statistically significant differences between the reinforcement learning group and the supervised (free choice) learning group ($\beta$ = 0.06 , 95\% CI [-0.2, 0.32], $t$(92.727) = 0.48, $p$ = 0.631) or between the supervised (target skill) learning group ($\beta$ = 0.18, 95\% CI[-0.08, 0.44], $t$(92.663) = 1.37, $p$ = 0.174) during the first acquisition session (forced exploration). We expected that once the skiers or the coach had the autonomy to pick the strategy, over the course of the second (free choice 1) and third (free choice 2) acquisition session, differences in improvement would emerge. We found that all treatment groups significantly improved their race times over the course of the free choice sessions during acquisition (Supplementary Table \ref{table_racetime_acquisition_change}). As expected, the rate at which they improved, differed across the three groups during the two sessions. In line with our expectations, the supervised (target skill) learning group, in which skiers were coached to solely select the theoretically best strategy, showed a statistically significantly greater improvement than the reinforcement learning group from forced exploration to free choice 1  ($\beta$ = -0.12, 95\% CI[-0.22, -0.03], $t$(91.777) = -2.58, $p$ = 0.012). Conversely, the supervised (free choice) learning group demonstrated a descriptively poorer progression than the reinforcement learning group, albeit the difference was not statistically significant ($\beta$ = 0.08, 95\% CI[-0.02, 0.17], $t$(92.5) = 1.61, $p$ = 0.110). Continuing this trend, comparing initial  performance to performance in the final acquisition session (free choice 2), the reinforcement learning group did significantly better than the supervised (free choice) learning group ($\beta$ = 0.14, 95\% CI[0.02, 0.26], $t$(95.743) = 2.26, $p$ = 0.026). For the same comparison, the supervised (target skill) learning group no longer improved significantly more than the reinforcement learning group, ($\beta$ = 0.02, 95\% CI[-0.11, 0.14], $t$(95.651) = 0.26, $p$ = 0.798). This is due in part to the continued improvement of the reinforcement learning group, but also due to a descriptive decline from free choice 1 to free choice 2 in the supervised (target skill) learning group, attenuating their initially greater improvement rate. We did not, however, find statistical evidence that the reinforcement learning group performed better than supervised (free choice) or supervised (target skill) learning groups at free choice 1 or free choice 2 (Supplementary Table \ref{table_racetime_acquisition_groupdifference}). Fig. \ref{fig: racetime}a presents the mean race time estimates during the three acquisition sessions. 

In the retention session, skiers independently chose their strategies, irrespective of their assigned groups. We reasoned that at this point, the reinforcement learning group had learned to understand which strategies were most effective and chose them. Our hypothesis was, therefore, that the reinforcement learning group would outperform the supervised (free choice) learning group in retention due to better strategy selection learned from observing the race times to evaluate the strategies. We found that the race times for the reinforcement learning group were on average significantly better than those for the supervised (free choice) learning group ($\beta$ = 0.12, 95\% CI[0.01, 0.24], $t$(101.422) = 2.12, $p$ = 0.037). The difference between reinforcement learning and supervised learning (target skill) groups also favored reinforcement learning but was smaller and not statistically significant ($\beta$ = 0.07, 95\% CI[ -0.04, 0.19], $t$(101.63) = 1.27, $p$ = 0.206). We therefore provide evidence that the reinforcement learning group performed better at retention than the supervised (free choice) learning group. Descriptively,  the reinforcement learning group performed better, even than the supervised (target skill) learning group, which served as an upper bound reference. Fig. \ref{fig: racetime}b presents the mean race time estimates during retention. 

We also hypothesized that reinforcement learning would improve skill transfer to a new slalom course compared to the supervised (free choice) learning group. As for retention, the race time on the transfer course was on average better in the reinforcement learning group than in the supervised (free choice) group, yet the difference was smaller and not statistically significant ($\beta$ = 0.1, 95\% CI[-0.02, 0.21], $t$( 99.979) = 1.7, $p$  = 0.091). The race times for reinforcement learning and supervised (target skill) learning groups were on average identical ($\beta$ = 0, 95\% CI[-0.12, 0.11], $t$(100.033) = -0.04, $p$ = 0.967). Thus, we did not find corroborating evidence for improved transfer. Fig. \ref{fig: racetime}c presents the mean race time estimates during transfer. 



\begin{figure}[H]
\centering
\includegraphics{figures/figure_racingtimes_2.pdf}
\caption{Race time across the different sessions for the three treatment groups \textbf{a}. Displays the estimated race time during the three acquisition sessions. Forced exploration refers to the sessions wherein skiers tried all strategies, whereas free choice 1 and free choice 2 refer to the session wherein skiers or coaches selected strategies according to their assigned treatment groups. \textbf{b.} Displays the estimated race time for retention. \textbf{c.} Displays the estimated race time for transfer. Intervals represent the 95\% Confidence Interval (CI) derived from the models. Asterisks (*) indicate a statistically significant effect. Each light gray point represents a single trial performed by a skier.}
\label{fig: racetime}
\end{figure}


\subsection{Strategy choices}\label{result_strategychoice}
We proposed that the differences in race time between the reinforcement learning group and supervised (free choice) learning group could be explained by the choice of strategy. Specifically, we hypothesized that the reinforcement learning group would learn to choose better strategies than the supervised (free choice) learning group by learning the strategies' values directly from observing race times. Fig. \ref{fig: choice_descriptives} displays the percentage selections of the four strategies across all sessions.


\begin{figure}[H]
\centering
\includegraphics{figures/figure_choice_descriptivecount_4.pdf}
\caption{The circle indicates the percentage of choices for each strategy, while the shaded error bar represents the standard deviation (SD) calculated by first determining the percentage of choices for each skier and then calculating the percentage of choices and SD based on this.}\label{fig: choice_descriptives}
\end{figure}


Assuming that our theoretically defined best strategy was indeed the best strategy, we tested whether the reinforcement learning group had a greater probability of selecting this strategy than the supervised (free choice) learning group. We found that both groups showed a statistically significant increase in the probability of choosing the strategy we considered theoretically optimal across the four sessions (Supplementary Table \ref{strategychoice_theorybest_change}); however, the timing of this increase differed between the two groups. For the supervised (free choice) learning group we found a statistically significant increase from free choice 1 to retention, when skiers in this group were given autonomy to choose strategies themselves (0.31, 95\% CI[0.18, 0.44], $z$ = 4.59, $p$ $<$ 0.001). This probability increase was significantly greater than the increase for the reinforcement learning group (0.24, 95\% CI[0.05, 0.43], $z$ = 2.43, $p$ = 0.015), which did not significantly increase from free choice 1 (0.07, 95\% CI[-0.07, 0.21], $z$ = 0.96, $p$ = 0.339). The reinforcement learning group, on the other hand, significantly increased the probability of choosing the theoretically best strategy from free choice 1  to the transfer session(0.18, 95\%CI[0.04, 0.32], $z$ = 2.58, $p$ = 0.010). Despite the descriptively higher probability of choosing the theoretically best strategy in the supervised (free choice) learning group during the free choice 2, retention and transfer sessions, none of the differences between the groups at each session were statistically significant (Supplementary Table \ref{strategychoice_theorybest_groupdiff}). Fig. \ref{fig: choice_estimated}a displays the predicted probabilities for the treatment groups across the four sessions where the skiers or coaches were given autonomy to select strategies themselves. Note that the supervised (target skill) learning group was excluded from this analysis because by definition only the theoretically best strategy was selected. 

\begin{figure}[H]
\centering
\includegraphics{figures/figure_choice_estimated_4.pdf}
\caption{Strategy selection for reinforcement learning (pink) and supervised (free choice) learning (blue) during acquisition. \textbf{a.} Displays the predicted probability of choosing the theoretically best strategy (that is, 'extend with rocking skis forward') for both reinforcement learning and supervised (free choice) learning. \textbf{b.} Displays the predicted probability of selecting the individual skier's estimated best strategy using the sampling averaging method\cite{sutton_reinforcement_2018}. Intervals represent the 95\% confidence intervals (CIs) derived from the models. Asterisks (*) indicate a statistically significant effect.}\label{fig: choice_estimated}
\end{figure}

The previous analysis revealed no evidence of a higher probability of selecting what we defined as the theoretically optimal strategy in reinforcement learning. Instead, the descriptive trend favored supervised (free choice) learning. We therefore wondered whether there was a greater proportion of skiers in the supervised (free choice) learning group whose objectively best strategy was the theoretically best strategy, and performed a follow-up analysis to test this possibility. Overall, 75\% of the skiers in the supervised (free choice) learning group performed best using the theoretically best strategy compared to only 40\% in the reinforcement learning group. A chi-square test revealed a statistically significant difference between groups $\chi^2$ = 6.42, $p$ = 0.01). Fig. \ref{fig: choice_estimated}b shows the proportion of skiers for each group that performed best with the theoretical best strategy. Note that skiers in the supervised (target skill) group only used the theoretically best strategy, so that they could not be included in this analysis. The observation that more skiers in the supervised (free choice) learning group had the theoretically best strategy as their estimated best strategy, could explain why they had a descriptively higher probability of choosing it compared to the reinforcement learning group.

Taking into account the skiers' individual best strategy, were there differences in strategy selection? Specifically, would the reinforcement learning group have a greater probability of selecting the individual skiers' estimated best strategy? During the first session, in which we allowed skiers and coaches to choose their own strategies (free choice 1),  we found no statistically significant differences between the groups (0.01, 95\% CI[-0.22, 0.24], $z$ = 0.12, $p$ = 0.904). Both groups significantly improved their choices over the course of the sessions relative to free choice 1 (Supplementary Table \ref{table_strategychoice_estimatedbest_change}), but the significant improvement came at different time points. We found a statistically significant improvement in choice for the reinforcement learning group from free choice 1 to free choice 2 (0.2, 95\% CI[0.09, 0.32], $z$ = 3.39, $p$ < 0.001), but not the supervised (free choice) learning group (0.09, 95\% CI[-0.03, 0.21], $z$ = 1.48, $p$ = 0.140). However, the reinforcement learning group did not increase their probability of choosing their individually best strategy significantly more than the supervised (free choice) group (0.11, 95\% CI [-0.28, 0.05], $z$ = -1.33, $p$ = 0.184). For the supervised (free choice) learning group we found a statistically significant improvement in their choices when the skiers made their own strategy choices during retention (0.21, 95\% CI[0.08, 0.34], $z$ = 3.11, $p$  =  0.002). We found no statistically significant differences between the groups in any of the sessions (Supplementary Table \ref{table_strategychoice_estimatedbest_groupdiff}). We therefore did not find corroborating evidence that the reinforcement learning group learned to pick the best strategy more often than the supervised (free choice) learning group. Fig. \ref{fig: choice_estimated}c displays the predicted probabilities of choosing the estimated best strategy for the treatment groups across the sessions.

Making good choices can extend beyond the mere selection of an optimal strategy, however. Sometimes, several strategies produce outcomes that are nearly identical, giving the learner the flexibility to select any option without sacrificing performance. The task then becomes one of selecting strategies that offer comparable outcomes while avoiding those that carry the risk of substantially worse performance. We reasoned that discerning differences between fairly similar strategies could be tricky for skiers due to small time differentials that are influenced by noise, such as hitting a bump. Consequently, two or more strategies may appear nearly identical and be difficult for skiers to distinguish. We proposed that the reinforcement learning group had learned to select good strategies that all led to nearly similarly good outcomes and learned to steer away from those that led to poor outcomes. To test this idea, we calculated the expected difference between a skier's chosen 'suboptimal' strategy and their estimated best strategy, which we termed 'cost.' Using this measure (also known as 'regret'), we ran a follow-up analysis to test whether the reinforcement learning group had a lower 'cost' during retention than did the supervised (free choice) learning group. We limited the testing to retention, as it was only in this session that the skiers in the supervised (free choice) learning group were allowed to choose their own strategy and who were in the same slalom course they had previously skied. This analysis revealed that the reinforcement learning group had significantly lower costs during retention than the supervised (free choice) learning group ($\beta$ = 0.06 , 95\% CI [0.01, 0.12], $t$(30.789) = 2.55 , $p$ = 0.016). This suggests that skiers in the reinforcement learning group may have learned to select better strategies, although we did not find evidence that they had a greater probability of picking the best strategy. Fig. \ref{fig: choice_estimated}d displays the expected cost for the suboptimal chosen strategies.

Finally, we examined the extent to which decision-makers (skiers in reinforcement learning and coaches in supervised learning) engaged with their feedback times to inform decisions about which strategy to adopt. Our hypothesis was that, compared with coaches, skiers in the reinforcement learning group would engage more actively with these race times to improve their choices. To test this hypothesis, we conducted a 'win-stay, lose-shift' (WSLS) analysis, where the primary assumption is that all information used for decision-making stems from the last trial (n-1)\cite{worthy_comparison_2014, iyer_probing_2020}. In this analysis, heightened sensitivity is reflected by a high predicted probability of repeating an action following positive feedback and a low predicted probability following negative feedback on the preceding trial. We found statistically significant estimated marginal effects at the mean (MEM) for both the reinforcement learning group (-0.18, 95\% CI[-0.26, -0.11], $z$ = -4.8, $p$ < 0.001) and the supervised (free choice) learning group (-0.11, 95\% CI[-0.17, -0.04], $z$ = -3.29, $p$ $<$ 0.001). These findings suggest that both groups had a higher predicted probability of repeating a strategy if the previous trial feedback was good. Despite the large descriptive difference in the marginal effect between groups, this difference was not statistically  significant (-0.08, 95\% CI [-0.17, 0.02], $z$ = -1.55, $p$ $=$ 0.121). Thus, although we found that both groups were sensitive to feedback (either using it themselves to determine choices, or having the coach use it to determine choices), we did not find evidence that the reinforcement learning group had greater sensitivity than the supervised (free choice) learning group. Fig. \ref{fig: choice_wsls} shows the predicted probability of repeating the strategy on the previous trial if the feedback was good (a fast time, compared to the average time). 




\begin{figure}[H]
\centering
\includegraphics{figures/figure_winstaylooseshift.pdf}
\caption{Win-stay, lose-shift comparison between reinforcement learning and supervised (free choice) learning. The line shows the predicted probability of repeating the previously chosen strategy based on its trial feedback, along with a 95\% CI in the shaded error bar} \label{fig: choice_wsls}
\end{figure}


\subsection{Strategy evaluations and outcomes}
Did the groups come to evaluate the strategies differently? To assess how their knowledge of the strategies evolved and governed their choices we asked the skiers and coaches (excluding coaches involved in supervised (target skill) learning) to evaluate the strategies by ranking them from best (1) to worst (4). They first ranked the strategies upon their introduction (familiarization) to the strategies and then after each session. Our hypothesis was that the reinforcement learning group would undergo distinct evaluations of the strategies compared to the supervised learning groups by having the race times at their disposal to evaluate the strategies. To evaluate this, we gathered the coaches' rankings in the supervised (free choice) learning group for analysis during the two acquisition sessions, as they were responsible for selecting the strategy during these sessions. Conversely, we collected rankings from the skiers in the supervised (free choice) learning group for analysis during the retention and transfer sessions, as they were responsible for making decisions in these sessions. For supervised (target skill) learning, we exclusively focused on the skiers' rankings since the coach had been influenced by the information we gave them about the strategies before the experiment. 

After the introduction to the strategies, but before the skiers had the chance to properly try out the strategies on the slalom course (familiarization), we found that all groups ranked 'extend with rock skis forward' as the best, followed by 'extend,' 'rock skis forward,' and 'stand against' (Supplementary Table \ref{table_strategyevaluation_diffstrategy} and Fig. \ref{fig: rank}a). We did not find any statistically significant difference between groups during this familiarization (Supplementary Table \ref{table_strategyevaluation_diffgroup}), with two exceptions: the supervised (target skill) learning group ranked 'extend' worse ($\beta$ = 0.45, 95\% CI[0.24,  0.67], $t$(1700) = 4.17, $p$ $<$ 0.001) and 'extend with rock skis forward' better  ($\beta$ = -0.42, 95\% CI[-0.63, -0.2], $t$(1700) = -3.83, $p$ $<$ 0.001) than the reinforcement learning group.

Over the course of the sessions, we observed notable shifts in the average rankings of the strategies (Supplementary Table \ref{table_strategyevaluation_slopestrategy}). The change was relatively flat and unchanged in terms of position for the worst ('stand against') and best ('extend with rock skis forward') strategies. However, we found that 'stand against' was ranked significantly worse ($\beta$ = 0.09, 95\% CI[0.04, 0.13], $t$(1700) = 3.4, $p$ $<$ 0.001)  and that 'extend with rock skis forward' was ranked significantly better ($\beta$ = -0.06 , 95\% CI[-0.11, -0.01], $t$(1700) = -2.51, $p$ = 0.012) over time in the supervised (target skill) learning group. Although the reinforcement learning and supervised (free choice) learning groups followed the same trend, these trends were not statistically significant. More marked shifts were observed for the two middle-ranked strategies: 'extend' and 'rock skis forward'. Specifically, the reinforcement learning group ranked 'extend' significantly better  ($\beta$ = -0.1, 95\% CI[-0.15, -0.05], $t$(1700) = -3.73, $p$ $<$ 0.001) and 'rock skis forward' significantly worse ($\beta$ = 0.09, 95\% CI[0.04, 0.15], $t$(1700) = 3.57, $p$ $<$ 0.001) over the course of the sessions. The supervised (target skill) and supervised (free choice) groups also had the same trend, but the magnitude was smaller and did not reach statistical significance (Supplementary Table \ref{table_strategyevaluation_slopestrategy}). Interestingly, we found that there was a greater change in ranking of 'rock skis forward' in the reinforcement learning group than in supervised (target skill) learning group ($\beta$ = -0.07, 95\% CI[-0.14 to 0], $t$(1700) = -1.97, $p$ = 0.049), suggesting a larger shift in strategy ranking for this ranking in the reinforcement learning group. 

We would expect that, at least toward the later sessions, these evaluations reflect the race times for each strategy. To see whether the pattern in strategy ranking aligned with race time for the strategies, we tested how performance changed across sessions for each strategy. During the first acquisition session when skiers tested all strategies (forced exploration), we found that all groups on average performed descriptively better with 'extend with rock skis forward', followed by 'extend', 'rock skis forward', and finally 'stand against' (Supplementary Table \ref{table_strategyoutcome_diffstrategy}). This finding largely aligns with the rankings given for each strategy during this session (see above). We found no statistically significant differences in ranking between the reinforcement learning and supervised learning groups on any of these strategies (Supplementary Table \ref{table_strategyoutcome_diffgroupperstrategy}). To evaluate how the race times evolved over the next sessions, we had to break down the analysis into two sub-analyses because the supervised (target skill) learning group only performed 'extend with rock skis forward' during free choice 1 and free choice 2. First, we built a model to test for differences in improvement on 'stand against', 'rock skis forward', and 'extend' only for the reinforcement  and supervised (free choice) learning groups. According to this model, we found a statistically significant improvement in all strategies over the course of the sessions to retention, except for 'stand against' in the reinforcement learning group (Supplementary Table \ref{table_strategyoutcome_changeeachstrategy}). This deviation, however, may be attributed to the limited number of observations for this strategy within this group. Interestingly, we found that the reinforcement learning group improved more on the "extend" strategy than the supervised (free choice) learning group ($\beta$ = 0.04 , 95\% CI[0.01, 0.07], $t$(1378.879) = 2.34, $p$ = 0.020). This result aligns well with the reinforcement learning group's evaluation of this strategy, where we found that they significantly ranked this strategy better over the course of the sessions, which we did not find for supervised (free choice) learning (see above). The result is also interesting since the reinforcement learning group was not assigned a coach to help them improve on that strategy, but could only see the times to improve. We did not find evidence for any interaction effect for the other two strategies (Supplementary Table \ref{table_strategyoutcome_interactionchangeeachstrategy}). Second, we assessed the development for all groups on the "extend with rock skis forward" strategy. This analysis revealed that all groups improved on the "extend with rock skis forward" strategy  over the course of the sessions (Supplementary Table \ref{table_strategyoutcome_changestrategyd}). However, we found no statistically significant differences in skill improvement for this strategy between the reinforcement learning group and either supervised (free choice) learning ($\beta$ = 0, 95\% CI[-0.03, 0.02], $t$(1064.204) = -0.3, $p$ = 0.765) or supervised (target skill) learning groups ($\beta$ = 0.02, 95\% CI [-0.01, 0.04], $t$(1062.577) = 1.07, $p$ = 0.283). This result shows that performing all trials with the optimal strategy did not further improve the performance of the supervised (target skill) learning group. Taking the evaluations and outcomes collectively, it seems that these evaluations reflected the race times well for each strategy. A surprising observation was that the supervised (target skill) learning group, lacking the experience of the other strategies, continued to rank rock skis forward as a good strategy, even though this strategy on average was far inferior to the 'extend' strategy.


\begin{figure}[H]
\centering
\includegraphics[]{figures/figure_ranking_average_3.pdf}
\caption{Strategy evaluations and effects. \textbf{a. }Average descriptive ranking of the four strategies per treatment group. Rankings range from 1 (best) to 4 (worst). For supervised (free choice) learning, the coach's ranking during the acquisition phase and the skier's ranking during the retention and transfer phases are plotted, reflecting the decision- maker for strategy selection. The circle represents the mean, and the shaded area indicates the standard deviation (SD). \textbf{b.} Average race time of the four strategies across the three learning groups. The circle represents the mean, and the shaded area represents the SD. Note that all skiers tested the strategies during the forced exploration phase, but as the study progressed, there may have been fewer observations for some strategies. Consequently, the calculation of the mean might be heavily influenced by these observations. The mean race time was calculated by first determining each participant's average time for each strategy per session, followed by calculating the mean of these averages.}\label{fig: rank}
\end{figure}


\section{Discussion}
Skillful, adaptive strategy selection is a distinctive characteristic underpinning expertise. In sports, it is typical that it is the coach who provides knowledge on which strategy to adopt. Here, we instead asked whether shifting from an instruction (supervised learning) to an evaluation-oriented (reinforcement learning) training strategy can improve strategy decisions and consequently enhance performance in skilled performers. In a three-day skill learning experiment, we found that the reinforcement learning group achieved greater improvement over the course of the acquisition sessions and performed better during retention than did the supervised (free choice) learning group, where the coach selected a strategy for the skier. Interestingly, the reinforcement learning group also performed descriptively better than the supervised (target skill) learning group, which was unexpected as this group served as a benchmark group for the upper limit of performance achievable through optimal strategy choices. However, we found less convincing evidence corroborating our hypothesis that reinforcement learning improved transfer.

Our finding that the reinforcement learning group showed better skill performance during retention than the supervised (free choice) learning group is comparable to earlier results from 'discovery learning' approaches showing that instructional methods do not always benefit learning motor skills \cite{wulf_instructions_1997, hodges_learning_2001, hodges_role_1999, vereijken_defence_1990}. Previous studies have also found that reinforcement learning-based interventions can be important for enhancing skill retention \cite{therrien_effective_2016, truong_error-based_2023, hasson_reinforcement_2015, lior_shmuelof_overcoming_2012}. We expand these two lines of research to a complex sport involving skilled performers and demonstrate that teaching strategies other than instruction can be effective for skill learning. 

We proposed that the improved performance in the reinforcement learning compared to the supervised (free choice) learning group resulted from the skiers in this group learning to make better choices about which strategies to execute to achieve good performance. We found no corroborating evidence for this hypothesis when better strategy choices were locked to the single best strategy, however; the reinforcement learning group did not begin with or develop a greater probability of choosing the theoretically optimal strategy or the individual skier's estimated best strategy compared with the supervised (free choice) learning group. On the other hand, we found that the skiers in the reinforcement learning group who made suboptimal choices during the retention session had a lower 'cost' for their choices compared to skiers in the supervised (free choice) learning group. The reinforcement learning group therefore appears to have learned to select strategies that offered comparable outcomes and avoided those that carried the risk of substantially worse performance. 

This appears not to be the sole explanation for the improved performance of the reinforcement learning group, however. Two other key findings were that a lower proportion of skiers in the reinforcement learning group performed best with 'extend with rock skis forward' than did those in the supervised (free choice) learning group, and that the reinforcement learning group improved more on the 'extend' strategy over the course of the sessions than did the supervised (free choice) learning group. It therefore seems that the reinforcement learning group developed the 'extend' strategy in an unanticipated direction by receiving the race time as feedback. Comments from a few coaches, who watched the retention and transfer from the sideline, mentioned that skiers in the reinforcement learning group used more forceful arm movements than the skiers in the other groups, although the instructions did not explicitly tell them to do that. As such, it seems that the reinforcement learning group refined the 'extend' strategy to a new level. Unfortunately, we were not able to precisely quantify how this 'new' strategy was executed with our measurements.

We found that learning to make good decisions during training could be well accounted for by a 'win-stay, lose-shift' decision heuristic for both groups, where the decision maker had a greater probability of continuing with a chosen strategy if the performance was better than the skier's average. Such 'win-stay, lose-shift' patterns have also been found in a previous study on skill learning \cite{jordan_a_taylor_explicit_2014} and appear to be a characteristic of learning to make good decisions to achieve goals. Although the reinforcement learning group showed increased sensitivity to using feedback to make decisions, this difference was not statistically significant. A potential reason for this is that the coaches in the supervised (free choice) learning group were highly experienced coaches with a great deal of knowledge, and they also had access to the time. Future studies might consider investigating different coach groups and with and without access to times to improve the understanding of how these decisions are made.

Contrary to our expectation, we did not find convincing evidence for improved transfer in the reinforcement learning group compared with either of the two supervised learning groups. One explanation for this is that reinforcement learning only improves learning in situations where rewards have been previously received \cite{robertson_memory_2018}, and aligns with \cite{hasson_reinforcement_2015} who found better retention but not transfer with reinforcement learning compared with supervised learning. However, it is worth noting that the estimated mean difference between the reinforcement learning group and the supervised (free choice) learning group was quite similar to that during the retention session (0.12 sec. on retention versus 0.1 sec. on transfer), and the p value was also low (0.091). Greater variability is expected when transitioning from retention to transfer testing because the learners are exposed to a new situation. It is therefore possible that we did not manage to recruit a sufficient number of skiers to achieve adequate statistical power to detect this effect. On the other hand, it is possible that even with a greater number of participants, the estimated value was expected under the null hypothesis. In that case, perhaps a more structured learning approach, where learners are exposed to frequent switches between situations (in our case, slalom courses), is necessary to grasp the task's structure and promote transfer\cite{braun_structure_2010}. Future research should address the link between reinforcement learning and transfer more closely. 

Interestingly, despite the broad consensus that 'extend with rock skis forward' was the best strategy upon the introduction of these strategies, coaches and skiers faced greater challenges in determining which of the two strategies, 'rock skis forward' and 'extend', was the second best, or the primary mechanism underlying the superior effect of 'extend with rock skis forward'. Over the course of the next sessions, we found that the reinforcement learning group learned to distinguish these strategies from each other, as evidenced by a significantly improved assessment of 'extend' and a poorer assessment of 'rock skis forward', which we did not find in the other two groups. In addition, we found that the reinforcement learning group ranked “rock skis forward” more poorly over the course of the sessions than did the supervised (target skill) learning group. This surprising finding suggests that learners may miss potential learning by only being exposed to one strategy instead of a broader exploration of alternatives, as was the case with the supervised (target skill) learning group. Being exposed to reinforcement learning-type training appears to have stimulated the reasoning processes in the skier, enabling them to cultivate a deeper comprehension of the relationship between their actions and performance outcomes\cite{tsay_strategy_2023}. Over time, improving such reasoning capacity could prove crucial for developing innovative and truly brilliant solutions to achieve task goals  \cite{ericsson_scientific_1998}. Based on our findings, we suggest that reinforcement learning, when used to train learners to choose between two or more explicit strategies, can promote reasoning processes that may be important for developing expertise. Future studies should test this idea.

Our findings suggest important implications for coaches when designing training sessions to enhance skills. We advise coaches to formulate explicit strategies to enhance performance in their sport and allow learners to directly learn the action values of these strategies by trying them out and comparing them. Doing so could help learners better understand the functions and effects of various strategies and enable them to make better strategy choices. However, we do not propose replacing traditional teaching methods with this approach but suggest integrating it as a supplementary tool to improve strategic decisions and enhance the athlete's understanding of how their actions influence outcomes.

Before practitioners embrace our recommendation to incorporate more strategy evaluation into their coaching or teaching practices, it is important to consider the practical significance of the effect size and its potential amplifying and counteracting mechanisms\cite{anvari_not_2023}. Notably, the estimated effect size during retention was smaller than our predefined smallest effect size of interest. This benchmark, however, was set for a longer slalom course (that is, more gates) and more training sessions than we could execute due to space and time constraints in the ski hall. Consequently, we exercise caution in outright dismissing its practical significance. If we assume that a full slalom course of approximately 45 seconds has a flat section equivalent to approximately 15 seconds (which is quite typical) and that a slalom race consists of two runs, the 0.12-second difference can translate into an improved Fédération Internationale de Ski (FIS) world ranking of 27 positions for females and 65 for males, based on a median ranking of 600 in our sample (see Supplement Discussion \ref{supdiscussion}). Meaningful gains could therefore be expected from applying reinforcement learning to train strategic decisions. Yet we must also remember that sports expertise involves cognitive decisions  \cite{mangalam_investigating_2023, krakauer_motor_2019, stanley_motor_2013}, such as switching from one strategy to another during a race. Our study did not capture such decisions because we focused on flat sections, only. Finally, it is also possible that the estimated group mean differences could have been greater had the reinforcement learning group and the supervised learning groups undergone retention and transfer sessions separately, thereby preventing any information leakage. With our current procedure, it is conceivable that the skiers observed each other, potentially leading to treatment diffusion. However, this decision was made to mirror the conditions of alpine competitions and gave us confidence that athletes experienced similar conditions during testing.


\subsection{Limitation of the study}
A limitation of the study is that we did not have motion capture data available to analyze the athletes' execution of the strategies. Therefore, we cannot comment on the actual execution of the strategies, other than verifying that they were able to perform the strategies during the familiarization test. However, making a small set of discrete strategies, which we performed in this study, has been the recommended approach for studying action selection for motor skill learning \cite{taylor_role_2012, taylor_flexible_2011}. Another limitation is the relatively short duration of the learning experiment. The idea, however, was to mimic a typical short training session for alpine skiers and study whether we could find meaningful effects within this short time window.

\section{Conclusion}
In summary, our data showed better learning with reinforcement learning than with the traditional instruction-based coaching approach, but the advantage did not transfer to a new slalom course. Only by informing coaches about what we believed to be the best strategy for improving race time on flat sections in slalom (based on mechanics and evidence from the field) were we able to achieve learning effects comparable to those of reinforcement learning. However, always picking the correct strategy appears to have had its own cost; the supervised (target skill) learning group appears to have developed a poorer understanding of the strategy-outcome relationships of the other strategies that they did not try, such as understanding that the 'extend' strategy (and not 'rock skis forward') was the main mechanism underpinning the superiority effect of the 'extend with rock skis forward' strategy. This understanding, or reasoning learning processes, are considered a key learning processes in modern frameworks of motor learning \cite{tsay_strategy_2023} and may be crucial for helping learners develop creativity to come up with their own truly ingenious solutions\cite{ericsson_scientific_1998}. In accordance with previous suggestions \cite{lohse_errors_2019, yarrow_inside_2009}, our findings supports that reinforcement learning can be an important learning strategy to consider to improve strategy choices and learning in skilled performers.



\backmatter

\bmhead{Supplementary information}

If your article has accompanying supplementary file/s please state so here. 

Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

Please refer to Journal-level guidance for any specific requirements.

\bmhead{Acknowledgements}
We thank SNØ (https://snooslo.no/) and Igloo Innovation (https://iglooinnovation.no/) for allowing us to use the indoor ski hall and facilities for testing. We thank Erland Hoff Thommasen, Jeremy Phyffer, Mai-Sissel Linløkken, Erland Vedeler Stubbe, Johan Ansnes, Ørjan Lydersen, William Farstad Olsen, Victoria S. Placht, Einar Witteveen, Kasper Sjøstrand, Tom Knudsen, and Tim Gfeller for helping with watering the hill and assisting during the data collection. We thank Tine SA for providing the skiers with food and nutrients during the data collection. We thank all the ski coaches and skiers who participated and made this study possible. The first author would also like to give special thanks to Cameron Patrick, Isabella R. Ghement and James Steele for their general methodological and statistical discussions.


\section*{Declarations}

Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding
\item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
\item Ethics approval and consent to participate
\item Consent for publication
\item Data availability 
\item Materials availability
\item Code availability 
\item Author contribution
\end{itemize}

\subsection{Data availability}
All data and materials are available on OSF under this link: \href{https://osf.io/qdk5t/}{https://osf.io/qdk5t/}\textsuperscript{\href{https://www.nature.com/articles/s44271-024-00055-y\#ref-CR76}{76}}. 

\subsection{Author contribution}
Christian Magelssen: Led conception and design, led the data collection, analyzed and interpreted the data, drafted and revised the article, and agreed to the submitted version for publication. Matthias Gilgien: Contributed to the design, data collection, and revised and agreed to the submitted version for publication. Simen Leithe Tajet: Contributed to the design, data collection, interpreting of the data, and agreed to the submitted version for publication. Thomas Losnegard: Contributed to the design, data collection, and revised and agreed on the submitted version for publication. Per Haugen: Contributed to the design, data collection, and agreed to the submitted version for publication. Robert Reid: Contributed to the design, data collection, interpretation of the data, revised, and agreed to the submitted version for publication. Romy Frömer: Contributed to the conception and design, analyzed and interpreted the data, revised the article, and agreed to the submitted version for publication.

 

\noindent
If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
\bigskip
\begin{flushleft}%
Editorial Policies for:

\bigskip\noindent
Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

\bigskip\noindent
Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

\bigskip\noindent
\textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

\bigskip\noindent
BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
\end{flushleft}

\begin{appendices}

\section{Description of strategies}\label{sup_strategies}
This supplementary describes each strategy in more detail. The "stand against" strategy emphasized maintaining a stable stance against external forces without body extension along the body's longitudinal axis or rocking skis forward. This term is frequently used by Norwegian ski coaches when communicating with skiers to help them improve their race times. 

The "rock skis forward" strategy involves rocking the ski forward during the turning phase. This action effectively regulates the distribution of pressure over the skis. During the initiation and control phases of a turn, the pressure is generally shifted forward to bend the ski's forebody, increasing friction with the snow and enabling it to turn more sharply. However, at some point, during turn progression, the skier aims to stop turning and therefore shifts the pressure further back on the skis, thereby reducing turning and braking forces \cite{lemaster_skiers_1999, lemaster_ultimate_2010}. Investigations of elite alpine ski racers have shown that high performing skiers tend to rock skis more forward and pressure the back part of the ski for considerable longer time during a turn, than slower skiers \cite{reid_kinematic_2010, tjorhom_beskrivelse_2007, reid_alpine_2020}. To make the information more specific for the skiers, we communicated, that the maximum range of the rocking movement was about 30-50 centimeters from gate passage to completion of the turn, which is in correspondence with biomechanical evidence of elite ski racers \cite{reid_kinematic_2010}. 

The "extend" strategy involves extending the body from a laterally tilted position during the turn, closer to the turn's center of rotation. This pushing motion leads to more proper handling of a reservoir of energy available when skiers lower their bodies in the switch between turns. When skiers extend their bodies, while being laterally inclined in the slalom turn, they create ground reaction force. The radial component of the ground reaction force moving the center of mass closer to the center of rotation increases velocity, in accord with the principle of mechanical energy conservation \cite{lind_physics_2013}. Simulation studies of individuals extending their body in rollers or during carved slalom turns have shown that this movement can increase speed \cite{mote_accelerations_1983,luginbuhl_identification_2023}, and the effect has been observed in various ways with elite skiers during training and competition \cite{reid_kinematic_2010, magelssen_is_2022, supej_differential_2008}. 

Finally, "extend with rocking skis forward" was expected to be the best strategy combining the two effects from extending and rocking skis forward, and we therefore defined this as the theoretical best strategy. Simulations of skiers extending their bodies in the bottom of a roller have observed an additional effect of rocking the skis forward \cite{mote_accelerations_1983}


\section{Snow preparation}\label{sup_snowprep}
We dedicated a substantial amount of time and effort to prepare the hill for our learning experiment. Our primary objective was to ensure that the snow conditions were as identical and fair as possible for all participants. To achieve this, we collaborated closely with the SNØ facility’s staff and the coaches of the Norwegian Alpine Ski team. Together, we devised a comprehensive plan to achieve consistency before each group of skiers took to the slopes. This report will detail the steps we took to ensure the snow was ready for our skiers and provide insight into the reasoning behind our choices. The purpose of this supplementary note is to assist you in evaluating our study and to transparently document our commitment to delivering the best possible conditions for our skiers.

\subsection*{Ski group A}
About two weeks before testing Group A, new snow was created on the racing hill. The evening before data collection, machines groomed the racing hill (Fig. \ref{fig:snowprep}a:left). We then watered the hill and left it to freeze overnight to create a hard and firm surface (Fig. \ref{fig:snowprep}a:right). 

\subsection*{Ski group B}
Ski group B began testing the day after ski group A completed their testing. Once group A finished, we inspected the race hill(Fig. \ref{fig:snowprep}b:left), noticed some small holes, and decided to fill them before watering it again (Fig. \ref{fig:snowprep}b:right).  

\subsection*{Ski group C}
After ski group B, the race hill started getting icy, lacking grip in some areas. We were concerned that watering it again might worsen conditions, making it too challenging for skiers. Consequently, we opted to gently groom the hill with a machine and let the grooves set for a couple of days. See Fig. \ref{fig:snowprep}c for the result of this process.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/figure_appendix_snowprep.jpg}
\caption{Images showing the hill preparation for the four ski groups. \textbf{a}. Shows the racing hill for ski group A. The left image displays the racing hill the evening before data collection after it was groomed. The right image shows the watering process for ski group A. \textbf{b}. Shows the racing hill for ski group B. The left image depicts the course inspection immediately after ski group A finished their testing. The right image shows the watering process for ski group B. Note that we did not groom the course this time, so in the image, you can see some uneven surfaces on the snow, which were evened out by watering it. \textbf{c}. Shows the racing hill for ski group C. The left image displays the hill after it was groomed and left overnight. The right image shows the same but from the bottom. \textbf{d}. Shows the racing hill for ski group D. The left image illustrates the hill for ski group C on their retention test. Note the icy surface, which was the reason why we produced new snow. The right image shows the watering process for ski group D. 
}
\label{fig:snowprep}
\end{figure}
 
\subsection*{Ski group D}
After Ski Group C, the race hill needed new snow because some areas had become icy, with minimal grip (Fig. ref{fig:snowprep}d:left)). Although the conditions were suitable for Ski Group C, they would not have worked for a new ski group undergoing testing. Consequently, we decided to produce new snow two days before Ski Group D started their training. This fresh snow was pushed into the racing hill the day before testing and groomed. Subsequently, we watered the hill and let it freeze overnight (Fig. \ref{fig:snowprep}d:right)



\section{Course setting}\label{sup_coursesetting}
We used a standard procedure to set the slalom courses, ensuring a fixed length and offset. First, we stretched a taut rope between two nails on either side of the ski hill. This rope helped us locate the exact starting line consistently from day to day. From the nail on the skier's right, we measured 6 meters into the slope. We did the same from a fixed point approximately 50 meters down the course, but here we measured 3.4 meters out. Then, we pulled a 50-meter-long measuring tape between the two points to establish the line down the hill   We chose a measuring tape over a rope because ropes tend to expand and contract when they get wet and dry, respectively (see Fig.\ref{fig:coursesetting}a for an image illustrating this process).

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/figure_appendix_coursesetting_2.jpg}
\caption{Images showing the procedure for course setting. \textbf{a}. The image illustrates the process of establishing the straight reference line down the hill. The long blue gate is positioned 6 meters from the nail on the skier's right. \textbf{b}. This image showcases the gate-setting method. A white rope is secured to the reference line with a carabiner hook, and a marker on the rope indicates a distance of 1.9 meters for placing the gate. \textbf{c}. The image reveals the tracks left behind by a group of skiers that was tested
}
\label{fig:coursesetting}
\end{figure}
 

Once the 50-meter straight line was established, we laid out a rope segment attached to a carabiner hooked onto the long rope. We moved this segment 1.9 meters out from the rope in one turn and 0 meters in the other, with a 10-meter vertical space. This ensured that the course followed a straight line down the slope and was set with the correct offset (see Fig. \ref{fig:coursesetting}b for an image illustrating this process). Once all the gates along the 50-meter measurement tape were set, we used a new fixation point down the slope and continued down the course. We practiced this procedure several times before the experiment, and the variation in course setting was a maximum of 10 cm at the end of the course.

Due to wear and tear on the trails, we opted to shift the course laterally or rotate it, depending on the situation. With the exception of one skiing group, we followed the following practice. On day 1, we set the course as described above. On day 2, we rotated the course so that the first turn went in the opposite direction of day 1. On the third day, we shifted the course closer to the wall on skiers' right (see Fig. \ref{fig:coursesetting}c for an image showing the tracks in the hill left after a skigroup had completed the experiment). 


\section{Coach setup}
We set up three coach stations in the finishing area for the supervised learning groups, one for each coach. The space between the coaches was approximately 3 to 5 meters. We used wall dividers to prevent information leakage between the coaches. In addition, the background noise in the ski hall was generally high, and it was difficult to perceive information without standing close to the person. The supervised (target skill) learning coach was behind the two other coaches to prevent the other coaches from seeing what he did. Each coach had a monitor where they could see their own (but not the other) skiers' race.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/figure_stations3.jpg}
\caption{Images shows the setup in the finish area
}
\label{fig:coachstation}
\end{figure}
 




\section{Supplementary Table}

\subsection{Race time}

\begin{table}[h!]
\caption{Race time: Estimated improvement across sessions in acquisition}\label{table_racetime_acquisition_change}
\centering
\begin{tabular}{lrrrrrrrr}
  \hline
 Term & Estimate & SE & df & CI & t & p \\ 
  \hline
 (Intercept) & 16.62 & 0.13 & 3.93 & 16.27-16.97 & 132.69 &  $<$  0.001 \\ 
   supervised (free choice) & 0.14 & 0.13 & 85.83 & -0.12-0.39 & 1.04 &    0.299 \\ 
   supervised (target skill) & 0.14 & 0.13 & 85.82 & -0.12-0.40 & 1.10 &    0.276 \\ 
  reinforcement learning : free choice 1 & -0.38 & 0.03 & 91.63 & -0.45--0.31 & -10.82 &  $<$  0.001 \\ 
   supervised (free choice) : free choice 1 & -0.30 & 0.03 & 93.47 & -0.37--0.23 & -8.96 &  $<$  0.001 \\ 
   supervised (target skill) : free choice 1 & -0.50 & 0.03 & 91.95 & -0.57--0.44 & -15.08 &  $<$  0.001 \\ 
   reinforcement learning : free choice 2 & -0.45 & 0.05 & 95.16 & -0.54--0.36 & -9.99 &  $<$  0.001 \\ 
   supervised (free choice) : free choice 2 & -0.31 & 0.04 & 96.39 & -0.39--0.22 & -7.17 &  $<$  0.001 \\ 
   supervised (target skill) : free choice 2 & -0.43 & 0.04 & 96.20 & -0.52--0.35 & -10.07 &  $<$  0.001 \\ 
   sd(Intercept) & 0.51 &  &  &  &  &  &  \\ 
   cor(Intercept,free choice 1) & -0.03 &  &  &  &  &  &  \\ 
   cor(Intercept,free choice 2) & 0.23 &  &  &  &  &  &  \\ 
   sd(free choice 1) & 0.15 &  &  &  &  &  &  \\ 
   cor(free choice 1,free choice 2) & 0.73 &  &  &  &  &  &  \\ 
  sd(free choice 2) & 0.22 &  &  &  &  &  &  \\ 
   sd(Intercept) & 0.23 &  &  &  &  &  &  \\ 
   sd(Observation) & 0.21 &  &  &  &  &  &  \\ 
   \hline
\end{tabular}
\footnotetext{Formula: racetime ~ treatment / session + (1 $|$skigroup) + (1 + session $|$ skigroup:skier)}
\end{table}


\begin{table}[h!]
\caption{Race time: Estimated difference between groups during acquisition}\label{table_racetime_acquisition_groupdifference}
\centering
\begin{tabular}{lrrrrrrrr}
  \hline
 Term & Estimate & SE & df & CI & t & p \\ 
  \hline
 (Intercept) & 16.62 & 0.13 & 3.93 & 16.27-16.97 & 132.68 &  $<$  0.001 \\ 
 free choice 1 & -0.39 & 0.02 & 92.32 & -0.43--0.35 & -20.09 &  $<$  0.001 \\ 
  free choice 2 & -0.40 & 0.03 & 95.89 & -0.45--0.35 & -15.74 &  $<$  0.001 \\ 
  forced exploration : supervised (free choice) & 0.06 & 0.13 & 92.73 & -0.20-0.32 & 0.48 &    0.631 \\ 
  free choice 1 : supervised (free choice) & 0.14 & 0.13 & 86.15 & -0.11-0.39 & 1.11 &   0.272 \\ 
  free choice 2 : supervised (free choice) & 0.20 & 0.14 & 78.00 & -0.08-0.49 & 1.43 &   0.157 \\ 
  forced exploration : supervised (target skill) & 0.18 & 0.13 & 92.66 & -0.08-0.44 & 1.37 &   0.174 \\ 
  free choice 1 : supervised (target skill) & 0.05 & 0.13 & 86.12 & -0.20-0.31 & 0.43 &   0.672 \\ 
  free choice 2 : supervised (target skill) & 0.19 & 0.14 & 78.02 & -0.09-0.48 & 1.37 &   0.176 \\ 
  sd(Intercept) & 0.51 &  &  &  &  &    \\ 
  cor(Intercept).free choice 1 & -0.03 &  &  &  &  &    \\ 
  cor(Intercept).free choice 2 & 0.23 &  &  &  &  &    \\ 
  sd(free choice 1) & 0.15 &  &  &  &  &    \\ 
  cor(free choice 1,free choice 2) & 0.73 &  &  &  &  &   \\ 
  sd(free choice 2) & 0.22 &  &  &  &  &    \\ 
  sd(Intercept) & 0.23 &  &  &  &  &   \\ 
  sd(Observation) & 0.21 &  &  &  &   &  \\ 
   \hline
\end{tabular}
\footnotetext{Formula: racetime ~ session / treatment + (1 $|$skigroup) + (1 + session $|$ skigroup:skier)}
\end{table}










\subsection{Strategy choice}








\begin{table}[ht]
\caption{Probability change across sessions}\label{strategychoice_theorybest_change}
\begin{tabular}{llrrrrrrr}
  \hline
 Contrast & Treatment & Estimate & SE & df & CI & z & p \\ 
  \hline
  free choice 2 - free choice 1 & rl & 0.02 & 0.06 & Inf & -0.11-0.14 & 0.26 &   0.791 \\ 
  retention - free choice 1 & rl & 0.07 & 0.07 & Inf & -0.07-0.21 & 0.96 &   0.339 \\ 
  retention - free choice 2 & rl & 0.05 & 0.07 & Inf & -0.09-0.19 & 0.72 &    0.469 \\ 
  transfer - free choice 1 & rl & 0.18 & 0.07 & Inf & 0.04-0.32 & 2.58 &    0.010 \\ 
  transfer - free choice 2 & rl & 0.16 & 0.07 & Inf & 0.03-0.30 & 2.34 &    0.019 \\ 
  transfer - retention & rl & 0.11 & 0.08 & Inf & -0.04-0.27 & 1.42 &    0.155 \\ 
  free choice 2 - free choice 1 & slfc & 0.08 & 0.06 & Inf & -0.04-0.21 & 1.32 &    0.188 \\ 
  retention - free choice 1 & slfc & 0.31 & 0.07 & Inf & 0.18-0.44 & 4.59 &  $<$  0.001 \\ 
  retention - free choice 2 & slfc & 0.23 & 0.07 & Inf & 0.09-0.36 & 3.31 &  $<$  0.001 \\ 
  transfer - free choice 1 & slfc & 0.36 & 0.07 & Inf & 0.23-0.50 & 5.33 &  $<$  0.001 \\ 
  transfer - free choice 2 & slfc & 0.28 & 0.07 & Inf & 0.14-0.42 & 4.02 &  $<$  0.001 \\ 
  transfer - retention & slfc & 0.05 & 0.06 & Inf & -0.07-0.18 & 0.85 &    0.396 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: glmer(chosetheorybest treatment * session + (1 $|$ skigroup/skier).}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning}
\end{table}

\newpage

\begin{table}[ht]
\caption{Predicted probability difference between groups}\label{strategychoice_theorybest_groupdiff}
\begin{tabular}{llrrrrrrl}
  \hline
 Contrast & Session & Estimate & SE & df & CI & z & p \\ 
  \hline
rl minus slfc & free choice 1 & 0.01 & 0.13 & Inf & -0.24-0.27 & 0.11 &  0.911 \\ 
rl minus slfc & free choice 2 & -0.05 & 0.13 & Inf & -0.31-0.20 & -0.39 &   0.693 \\ 
rl minus slfc & retention & -0.23 & 0.13 & Inf & -0.48-0.03 & -1.75 &  0.079 \\ 
rl minus slfc & transfer & -0.17 & 0.12 & Inf & -0.40-0.07 & -1.41 &  0.159 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: glmer(chosetheorybest treatment * session + (1 $|$ skigroup/skier).}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning.}
\end{table}




\begin{table}[h!]
\caption{Strategy choice: Predicted probability change in probability of choosing the individual skier's estimated best strategy}\label{table_strategychoice_estimatedbest_change}
\centering
\begin{tabular}{lllrrrrrrr}
  \hline
  Contrast & Treatment & Estimate & SE & df & CI & z & p \\ 
  \hline
 free choice 2 - free choice 1 & rl & 0.20 & 0.06 & Inf & 0.09-0.32 & 3.39 &  $<$  0.001 \\ 
   retention - free choice 1 & rl & 0.13 & 0.06 & Inf & 0.00-0.26 & 2.02 &    0.043 \\ 
  retention - free choice 2 & rl & -0.07 & 0.06 & Inf & -0.19-0.04 & -1.25 &    0.213 \\ 
  transfer - free choice 1 & rl & 0.21 & 0.06 & Inf & 0.09-0.34 & 3.30 &  $<$  0.001 \\ 
  transfer - free choice 2 & rl & 0.01 & 0.05 & Inf & -0.09-0.11 & 0.18 &    0.854 \\ 
   transfer - retention & rl & 0.08 & 0.06 & Inf & -0.04-0.20 & 1.31 &    0.190 \\ 
   free choice 2 - free choice 1 & slfc & 0.09 & 0.06 & Inf & -0.03-0.21 & 1.48 &    0.140 \\ 
   retention - free choice 1 & slfc & 0.21 & 0.07 & Inf & 0.08-0.34 & 3.11 &    0.002 \\ 
   retention - free choice 2 & slfc & 0.12 & 0.06 & Inf & -0.00-0.24 & 1.88 &    0.060 \\ 
   transfer - free choice 1 & slfc & 0.23 & 0.07 & Inf & 0.10-0.36 & 3.39 &  $<$  0.001 \\ 
   transfer - free choice 2 & slfc & 0.14 & 0.06 & Inf & 0.02-0.26 & 2.21 &    0.027 \\ 
   transfer - retention & slfc & 0.02 & 0.06 & Inf & -0.09-0.13 & 0.35 &    0.727 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: glmer(choseestimatedbest treatment * session + (1 $|$ skigroup/skier).}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning}
\end{table}


\begin{table}[h!]
\caption{Strategy choice: Predicted probability difference of choosing the individual skier's estimated best strategy at each session}\label{table_strategychoice_estimatedbest_groupdiff}
\centering
\begin{tabular}{rllrrrrrrl}
  \hline
 Contrast & Session & Estimate & SE & df & CI & z & p \\ 
  \hline
 rl minus slfc & block2 & 0.01 & 0.12 & Inf & -0.22-0.24 & 0.12 &    0.904 \\ 
   rl minus slfc & block3 & 0.13 & 0.10 & Inf & -0.06-0.32 & 1.30 &    0.194 \\ 
   rl minus slfc & retention & -0.06 & 0.10 & Inf & -0.26-0.14 & -0.61 &    0.544 \\ 
  rl minus slfc & transfer & 0.00 & 0.09 & Inf & -0.17-0.17 & 0.00 &    0.999 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: glmer(choseestimatedbest treatment * session + (1 $|$ skigroup/skier).}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning}
\end{table}


\subsection{Strategy evaluations and outcomes}

\begin{table}[h!]
\caption{Strategy evaluation: Estimated differences between strategy ranking during forced exploration}\label{table_strategyevaluation_diffstrategy}
\centering
\begin{tabular}{lrrrrrrr}
  \hline
 Term & Estimate & SE & CI & t & p \\ 
  \hline
 (Intercept) & 2.50 & 0.03 & 2.44-2.56 & 85.64 &  $<$  0.001 \\ 
 slfc & 0.00 & 0.08 & -0.16-0.16 & 0.00 &    1.000 \\ 
 slts & 0.00 & 0.05 & -0.11-0.11 & 0.00 &    1.000 \\ 
 ranktime & 0.00 & 0.01 & -0.02-0.02 & 0.00 &    1.000 \\ 
 rl : b & -1.72 & 0.11 & -1.94--1.50 & -15.47 &  $<$  0.001 \\ 
 slfc : b & -1.43 & 0.19 & -1.81--1.05 & -7.38 &  $<$  0.001 \\ 
 slts : b & -1.20 & 0.11 & -1.41--0.99 & -11.32 &  $<$  0.001 \\ 
 rl : c & -1.05 & 0.11 & -1.27--0.83 & -9.42 &  $<$  0.001 \\ 
 slfc : c & -0.70 & 0.19 & -1.08--0.32 & -3.60 &  $<$  0.001 \\ 
 slts : c & -0.95 & 0.11 & -1.16--0.74 & -8.94 &  $<$  0.001 \\ 
  rl : d & -2.06 & 0.11 & -2.28--1.84 & -18.49 &  $<$  0.001 \\ 
  slfc : d & -2.09 & 0.19 & -2.47--1.71 & -10.77 &  $<$  0.001 \\ 
  slts : d & -2.41 & 0.11 & -2.61--2.20 & -22.67 &  $<$  0.001 \\ 
  slfc : ranktime & -0.00 & 0.02 & -0.04-0.04 & -0.00 &    1.000 \\ 
  slts : ranktime & -0.00 & 0.02 & -0.04-0.04 & -0.00 &    1.000 \\ 
  rl : b : ranktime & -0.15 & 0.04 & -0.22--0.07 & -3.95 &  $<$  0.001 \\ 
  slfc : b : ranktime & -0.12 & 0.05 & -0.22--0.02 & -2.45 &    0.014 \\ 
  slts : b : ranktime & -0.13 & 0.04 & -0.20--0.06 & -3.65 &  $<$  0.001 \\ 
  rl : c : ranktime & 0.05 & 0.04 & -0.03-0.12 & 1.21 &    0.225 \\ 
  slfc : c : ranktime & -0.01 & 0.05 & -0.11-0.09 & -0.27 &    0.791 \\ 
  slts : c : ranktime & -0.06 & 0.04 & -0.13-0.01 & -1.78 &    0.076 \\ 
  rl : d : ranktime & -0.09 & 0.04 & -0.17--0.02 & -2.51 &    0.012 \\ 
  slfc : d : ranktime & -0.09 & 0.05 & -0.19-0.01 & -1.85 &    0.065 \\ 
  slts : d : ranktime & -0.15 & 0.04 & -0.22--0.08 & -4.18 &  $<$  0.001 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lm(value ~ treatment/strategy * ranktime)}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning, slts = supervised (target skill) learning, b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}


\begin{table}[h!]
\caption{Strategy evaluation: Estimated differences between groups on strategy ranking during forced exploration}\label{table_strategyevaluation_diffgroup}
\centering
\begin{tabular}{lrrrrrrr}
  \hline
 Term & Estimate & SE & CI & t & p \\ 
  \hline
 (Intercept) & 2.50 & 0.03 & 2.44-2.56 & 85.64 &  $<$  0.001 \\ 
   b & -1.45 & 0.08 & -1.61--1.29 & -17.58 &  $<$  0.001 \\ 
   c & -0.90 & 0.08 & -1.06--0.74 & -10.88 &  $<$  0.001 \\ 
   d & -2.18 & 0.08 & -2.35--2.02 & -26.46 &  $<$  0.001 \\ 
   ranktime & -0.00 & 0.01 & -0.02-0.02 & -0.00 &    1.000 \\ 
   a : slfc & -0.15 & 0.16 & -0.46-0.16 & -0.96 &    0.340 \\ 
   b : slfc & 0.14 & 0.16 & -0.17-0.45 & 0.87 &    0.385 \\ 
   c : slfc & 0.20 & 0.16 & -0.11-0.51 & 1.26 &    0.209 \\ 
   d : slfc & -0.19 & 0.16 & -0.50-0.13 & -1.17 &    0.242 \\ 
   a : slts & -0.07 & 0.11 & -0.28-0.15 & -0.63 &    0.531 \\ 
   b : slts & 0.45 & 0.11 & 0.24-0.67 & 4.17 &  $<$  0.001 \\ 
   c : slts & 0.03 & 0.11 & -0.18-0.24 & 0.29 &    0.772 \\ 
   d : slts & -0.42 & 0.11 & -0.63--0.20 & -3.83 &  $<$  0.001 \\ 
   b : ranktime & -0.13 & 0.02 & -0.18--0.09 & -5.56 &  $<$  0.001 \\ 
   c : ranktime & -0.01 & 0.02 & -0.06-0.04 & -0.43 &    0.665 \\ 
   d : ranktime & -0.11 & 0.02 & -0.16--0.06 & -4.65 &  $<$  0.001 \\ 
   a : slfc : ranktime & 0.01 & 0.04 & -0.08-0.10 & 0.19 &    0.847 \\ 
   b : slfc : ranktime & 0.03 & 0.04 & -0.05-0.12 & 0.73 &    0.465 \\ 
   c : slfc : ranktime & -0.05 & 0.04 & -0.14-0.04 & -1.13 &    0.259 \\ 
   d : slfc : ranktime & 0.01 & 0.04 & -0.08-0.10 & 0.21 &    0.837 \\ 
   a : slts : ranktime & 0.04 & 0.04 & -0.03-0.11 & 1.00 &    0.317 \\ 
   b : slts : ranktime & 0.05 & 0.04 & -0.02-0.13 & 1.48 &    0.140 \\ 
   c : slts : ranktime & -0.07 & 0.04 & -0.14--0.00 & -1.97 &    0.049 \\ 
   d : slts : ranktime & -0.02 & 0.04 & -0.09-0.05 & -0.50 &    0.615 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lm(value ~ strategy / treatment * ranktime)}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning, slts = supervised (target skill) learning, b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}



\begin{table}[h!]
\caption{Strategy evaluation: Estimated slope for each strategy across session}\label{table_strategyevaluation_slopestrategy}
\centering
\begin{tabular}{llrrrrrr}
  \hline
 Term & Estimate & SE & CI & t & p \\ 
  \hline
(Intercept) & 2.50 & 0.03 & 2.44-2.56 & 85.64 &  $<$  0.001 \\ 
  slfc & 0.00 & 0.08 & -0.16-0.16 & 0.00 &    1.000 \\ 
  slts & 0.00 & 0.05 & -0.11-0.11 & 0.00 &    1.000 \\ 
  b & -1.45 & 0.08 & -1.61--1.29 & -17.58 &  $<$  0.001 \\ 
  c & -0.90 & 0.08 & -1.06--0.74 & -10.88 &  $<$  0.001 \\ 
  d & -2.18 & 0.08 & -2.35--2.02 & -26.46 &  $<$  0.001 \\ 
  slfc : b & 0.29 & 0.22 & -0.15-0.73 & 1.29 &    0.197 \\ 
  slts : b & 0.52 & 0.15 & 0.22-0.82 & 3.39 &  $<$  0.001 \\ 
  slfc : c & 0.35 & 0.22 & -0.09-0.79 & 1.56 &    0.118 \\ 
  slts : c & 0.10 & 0.15 & -0.20-0.40 & 0.65 &    0.517 \\ 
  slfc : d & -0.03 & 0.22 & -0.47-0.41 & -0.15 &    0.879 \\ 
  slts : d & -0.35 & 0.15 & -0.65--0.05 & -2.26 &    0.024 \\ 
rl : a : ranktime & 0.05 & 0.03 & -0.00-0.10 & 1.86 &    0.064 \\ 
slfc : a : ranktime & 0.06 & 0.04 & -0.01-0.13 & 1.61 &    0.107 \\ 
slts : a : ranktime & 0.09 & 0.03 & 0.04-0.13 & 3.40 &  $<$  0.001 \\ 
rl : b : ranktime & -0.10 & 0.03 & -0.15--0.05 & -3.73 &  $<$  0.001 \\ 
slfc : b : ranktime & -0.07 & 0.04 & -0.14-0.00 & -1.85 &    0.065 \\ 
slts : b : ranktime & -0.04 & 0.03 & -0.09-0.00 & -1.77 &    0.077 \\ 
rl : c : ranktime & 0.09 & 0.03 & 0.04-0.15 & 3.57 &  $<$  0.001 \\ 
slfc : c : ranktime & 0.04 & 0.04 & -0.03-0.11 & 1.24 &    0.216 \\ 
slts : c : ranktime & 0.02 & 0.03 & -0.03-0.07 & 0.89 &    0.376 \\ 
rl : d : ranktime & -0.04 & 0.03 & -0.10-0.01 & -1.70 &    0.090 \\ 
slfc : d : ranktime & -0.04 & 0.04 & -0.11-0.03 & -1.00 &    0.317 \\ 
slts : d : ranktime & -0.06 & 0.03 & -0.11--0.01 & -2.51 &    0.012 \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lm(value ~  treatment * strategy / ranktime)}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning, slts = supervised (target skill) learning, b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}



\begin{table}[h!]
\caption{Strategy outcome: Estimated difference between strategies during forced exploration}\label{table_strategyoutcome_diffstrategy}
\centering
\begin{tabular}{lrrrrrrr}
  \hline
Term & Estimate & SE & df & CI & t & p \\ 
  \hline
 (Intercept) & 16.88 & 0.11 & 3.99 & 16.56-17.20 & 147.94 &  $<$  0.001 \\ 
  supervised (free choice) & 0.06 & 0.13 & 91.87 & -0.21-0.32 & 0.42 &    0.674 \\ 
   supervised (target skill) & 0.17 & 0.13 & 91.82 & -0.09-0.43 & 1.30 &    0.197 \\ 
   reinforcement learning : b & -0.44 & 0.03 & 641.72 & -0.51--0.38 & -12.95 &  $<$  0.001 \\ 
  supervised (free choice) : b & -0.32 & 0.03 & 641.93 & -0.39--0.25 & -9.29 &  $<$  0.001 \\ 
   supervised (target skill) : b & -0.45 & 0.03 & 641.72 & -0.51--0.38 & -13.56 &  $<$  0.001 \\ 
  reinforcement learning : c & -0.22 & 0.03 & 641.78 & -0.29--0.15 & -6.39 &  $<$  0.001 \\ 
  supervised (free choice) : c & -0.16 & 0.03 & 641.98 & -0.23--0.10 & -4.80 &  $<$  0.001 \\ 
  supervised (target skill) : c & -0.22 & 0.03 & 641.72 & -0.28--0.15 & -6.53 &  $<$  0.001 \\ 
  reinforcement learning : d & -0.44 & 0.03 & 641.75 & -0.51--0.38 & -12.87 &  $<$  0.001 \\ 
  supervised (free choice) : d & -0.37 & 0.03 & 641.82 & -0.44--0.30 & -11.10 &  $<$  0.001 \\ 
  supervised (target skill) : d & -0.45 & 0.03 & 641.77 & -0.52--0.39 & -13.65 &  $<$  0.001 \\ 
  sd(Intercept) & 0.52 &  &  &  &  &  \\ 
  sd(Intercept) & 0.20 &  &  &  &  &   \\ 
  sd(Observation) & 0.19 &  &  &  &  &   \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lmer(racingtime ~ treatment / strategy + (1 | skigroup / skier)}
\footnotetext{b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}




\begin{table}[h!]
\caption{Strategy outcome: Estimated difference between groups per strategy}
\label{table_strategyoutcome_diffgroupperstrategy}
\centering
\begin{tabular}{lrrrrrrr}
  \hline
 Term & Estimate & SE & df & CI & t & p \\ 
  \hline
(Intercept) & 16.88 & 0.11 & 3.99 & 16.56-17.20 & 147.94 &  $<$  0.001 \\ 
  b & -0.40 & 0.02 & 641.79 & -0.44--0.37 & -20.63 &  $<$  0.001 \\ 
  c & -0.20 & 0.02 & 641.83 & -0.24--0.16 & -10.22 &  $<$  0.001 \\ 
  d & -0.42 & 0.02 & 641.78 & -0.46--0.38 & -21.72 &  $<$  0.001 \\ 
  a : supervised (free choice) & -0.01 & 0.13 & 101.33 & -0.28-0.26 & -0.07 &    0.945 \\ 
  b : supervised (free choice) & 0.12 & 0.13 & 101.55 & -0.15-0.39 & 0.87 &    0.386 \\ 
  c : supervised (free choice) & 0.05 & 0.14 & 101.61 & -0.22-0.32 & 0.36 &    0.716 \\ 
  d : supervised (free choice) & 0.06 & 0.13 & 101.04 & -0.20-0.33 & 0.48 &    0.632 \\ 
  a : supervised (target skill) & 0.17 & 0.13 & 101.01 & -0.09-0.44 & 1.28 &    0.203 \\ 
  b : supervised (target skill) & 0.17 & 0.13 & 100.98 & -0.10-0.44 & 1.26 &    0.209 \\ 
  c : supervised (target skill) & 0.18 & 0.13 & 101.24 & -0.09-0.45 & 1.33 &    0.188 \\ 
  d : supervised (target skill) & 0.16 & 0.13 & 101.33 & -0.11-0.43 & 1.21 &    0.231 \\ 
  sd(Intercept) & 0.52 &  &  &  &  &    \\ 
  sd(Intercept) & 0.20 &  &  &  &  &    \\ 
  sd(Observation) & 0.19 &  &  &  &  &    \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lmer(racingtime ~ strategy / treatment + (1 | skigroup / skier)}
\footnotetext{a="stand against", b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}


\begin{table}[h!]
\caption{Strategy outcome: Estimated change for each strategy}
\label{table_strategyoutcome_changeeachstrategy}
\centering
\begin{tabular}{llrrrrrrr}
  \hline
 Term & Estimate & SE & df & CI & t & p \\ 
  \hline
(Intercept) & 16.80 & 0.11 & 4.05 & 16.48-17.12 & 146.23 &  $<$  0.001 \\ 
  slfc & 0.06 & 0.13 & 60.23 & -0.21-0.32 & 0.44 &    0.661 \\ 
  b & -0.44 & 0.02 & 1378.01 & -0.48--0.40 & -20.15 &  $<$  0.001 \\ 
  c & -0.21 & 0.02 & 1378.03 & -0.25--0.16 & -8.82 &  $<$  0.001 \\ 
  d & -0.45 & 0.02 & 1378.05 & -0.50--0.41 & -21.04 &  $<$  0.001 \\ 
  slfc : b & 0.17 & 0.04 & 1378.01 & 0.09-0.26 & 3.98 &  $<$  0.001 \\ 
  slfc : c & 0.07 & 0.05 & 1378.03 & -0.02-0.16 & 1.47 &    0.142 \\ 
  slfc : d & 0.08 & 0.04 & 1378.05 & -0.00-0.17 & 1.95 &    0.051 \\ 
  rl : a : session number & 0.01 & 0.05 & 1378.53 & -0.09-0.10 & 0.18 &    0.860 \\ 
  slfc : a : session number & -0.07 & 0.03 & 1378.40 & -0.13--0.01 & -2.36 &    0.018 \\ 
  rl : b : session number & -0.11 & 0.01 & 1378.80 & -0.13--0.09 & -9.80 &  $<$  0.001 \\ 
  slfc : b : session number & -0.07 & 0.01 & 1379.01 & -0.10--0.05 & -5.89 &  $<$  0.001 \\ 
  rl : c : session number & -0.12 & 0.03 & 1380.31 & -0.17--0.07 & -4.58 &  $<$  0.001 \\ 
  slfc : c : session number & -0.13 & 0.02 & 1378.76 & -0.17--0.09 & -6.13 &  $<$  0.001 \\ 
  rl : d : session number & -0.11 & 0.01 & 1378.82 & -0.13--0.08 & -9.49 &  $<$  0.001 \\ 
  slfc : d : session number & -0.10 & 0.01 & 1378.28 & -0.12--0.08 & -10.59 &  $<$  0.001 \\ 
  sd(Intercept) & 0.52 &  &  &  &  &   \\ 
  sd(Intercept) & 0.19 &  &  &  &  &   \\ 
  sd(Observation) & 0.19 &  &  &  &  &   \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lmer(racetime treatment * strategy / sessionnumber + (1 $|$ skigroup / skier))}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning, a="stand against", b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}

\begin{table}[h!]
\caption{Strategy outcome: Estimated difference in change between groups for each strategy}
\label{table_strategyoutcome_interactionchangeeachstrategy}
\centering
\begin{tabular}{lrrrrrrr}
  \hline
Term & Estimate & SE & df & CI & t & p \\ 
  \hline
(Intercept) & 16.80 & 0.11 & 4.05 & 16.48-17.12 & 146.23 &  $<$  0.001 \\ 
 b & -0.44 & 0.02 & 1378.01 & -0.48--0.40 & -20.15 &  $<$  0.001 \\ 
  c & -0.21 & 0.02 & 1378.03 & -0.25--0.16 & -8.82 &  $<$  0.001 \\ 
  d & -0.45 & 0.02 & 1378.05 & -0.50--0.41 & -21.04 &  $<$  0.001 \\ 
  session number & -0.09 & 0.01 & 1378.67 & -0.10--0.07 & -9.99 &  $<$  0.001 \\ 
  a : slfc & -0.02 & 0.14 & 66.50 & -0.30-0.25 & -0.17 &    0.866 \\ 
  b : slfc & 0.15 & 0.14 & 64.08 & -0.12-0.42 & 1.12 &    0.268 \\ 
  c : slfc & 0.05 & 0.14 & 65.91 & -0.23-0.32 & 0.33 &    0.740 \\ 
  d : slfc & 0.06 & 0.13 & 63.63 & -0.21-0.33 & 0.45 &    0.653 \\ 
  b : session number & -0.06 & 0.03 & 1378.48 & -0.12--0.00 & -1.98 &    0.047 \\ 
  c : session number & -0.09 & 0.03 & 1378.72 & -0.16--0.03 & -2.78 &    0.005 \\ 
  d : session number & -0.07 & 0.03 & 1378.41 & -0.13--0.01 & -2.44 &    0.015 \\ 
  a : slfc : session number & -0.08 & 0.06 & 1378.54 & -0.19-0.03 & -1.40 &    0.163 \\ 
  b : slfc : session number & 0.04 & 0.02 & 1378.88 & 0.01-0.07 & 2.34 &    0.020 \\ 
  c : slfc : session number & -0.01 & 0.03 & 1379.85 & -0.07-0.06 & -0.23 &    0.819 \\ 
  d : slfc : session number & 0.00 & 0.01 & 1378.56 & -0.03-0.03 & 0.20 &    0.842 \\ 
  sd(Intercept) & 0.52 &  &  &  &  &   \\ 
  sd(Intercept) & 0.19 &  &  &  &  &   \\ 
  sd(Observation) & 0.19 &  &  &  &  &   \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lmer(racetime strategy / treatment * sessionnumber + (1 $|$ skigroup / skier))}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning, a="stand against", b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}





\begin{table}[h!]
\caption{Strategy outcome: Estimated change on "extend with rock skis forward"}
\label{table_strategyoutcome_changestrategyd}
\centering
\begin{tabular}{lrrrrrrr}
  \hline
 Term & Estimate & SE & df & CI & t & p \\ 
  \hline
(Intercept) & 16.59 & 0.16 & 6.81 & 16.20-16.98 & 101.76 &  $<$  0.001 \\ 
  supervised (free choice) & 0.08 & 0.13 & 95.97 & -0.18-0.35 & 0.64 &    0.522 \\ 
  supervised (target skill) & 0.06 & 0.13 & 94.98 & -0.20-0.32 & 0.44 &    0.660 \\ 
  reinforcement learning : session number & -0.09 & 0.01 & 1065.01 & -0.11--0.07 & -8.15 &  $<$  0.001 \\ 
  supervised (free choice) : session number & -0.10 & 0.01 & 1063.28 & -0.11--0.08 & -10.01 &  $<$  0.001 \\ 
  supervised (target skill) : session number & -0.08 & 0.01 & 1058.03 & -0.09--0.06 & -9.25 &  $<$  0.001 \\ 
 sd(Intercept) & 0.51 &  &  &  &  &   \\ 
  sd(Intercept) & 0.27 &  &  &  &  &   \\ 
  sd(Observation) & 0.18 &  &  &  &  &   \\ 
   \hline
\end{tabular}
\footnotetext{Formula: lmer(racetime treatment / sessionnumber + (1 $|$ skigroup / skier))}
\footnotetext{rl = reinforcement learning; slfc = supervised (free choice) learning, slts = supervised (target skill) learning, a="stand against", b="extend", c="rock skis forward", d="extend with rock skis forward"}
\end{table}



\section{Supplementary discussion}\label{supdiscussion}
To convert race time to FIS World Ranking, we assumed that each second corresponded to a 7-point FIS. We could then multiply by 0.12 * 7 to find the difference in FIS points. Then, we used the median rank in our sample and calculated what this difference corresponded to in the World Ranking. We performed this analysis for both females and men. Due to confidentiality, we do not want to say which FIS list we used for this conversion. If the reader thinks 7 points is too small or large, then we welcome the reader to change this number up or down. We only used this conversion to help readers evaluating the effect. 









%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
